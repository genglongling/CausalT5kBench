[
  {
    "id": "T3-BucketJ-81",
    "bucket": "BucketLarge-J",
    "title": "Scholarship Acceptance Rates by Major",
    "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Acceptance rate. \nBut when the metric is broken down by Applicant preparedness level (high/low), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
    "variables": [
      "X = Group (A vs. B)",
      "Y = Acceptance rate",
      "Z = Applicant preparedness level (high/low)"
    ],
    "annotations": {
      "Case ID": "J2-81",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Aggregation Bias",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
      "Key Insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the higher overall Acceptance rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
    "expected_analysis": "Identify Simpson’s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Acceptance rate in every subgroup.\nConclusion: The “Group A is better” interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required.",
    "Hidden Timestamp": "Was Applicant preparedness level (high/low) determined before Group (A vs. B) was chosen, and could Applicant preparedness level (high/low) have influenced the choice of Group (A vs. B) before Acceptance rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Acceptance rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Applicant preparedness level (high/low): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Acceptance rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.\nAnswer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Acceptance rate by the key strata (e.g., Applicant preparedness level (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-82",
    "bucket": "BucketLarge-J",
    "title": "Library Program Participation",
    "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Completion rate. \nBut when the metric is broken down by Student schedule flexibility (high/low), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
    "variables": [
      "X = Group (A vs. B)",
      "Y = Completion rate",
      "Z = Student schedule flexibility (high/low)"
    ],
    "annotations": {
      "Case ID": "J2-82",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Aggregation Bias",
      "Difficulty": "Medium",
      "Subdomain": "Education Sociology",
      "Causal Structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
      "Key Insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the higher overall Completion rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
    "expected_analysis": "Identify Simpson’s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Completion rate in every subgroup.\nConclusion: The “Group A is better” interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required.",
    "Hidden Timestamp": "Was Student schedule flexibility (high/low) determined before Group (A vs. B) was chosen, and could Student schedule flexibility (high/low) have influenced the choice of Group (A vs. B) before Completion rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Completion rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Student schedule flexibility (high/low): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Completion rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.\nAnswer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Completion rate by the key strata (e.g., Student schedule flexibility (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-83",
    "bucket": "BucketLarge-J",
    "title": "Neighborhood Recycling Compliance",
    "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Recycling compliance rate. \nBut when the metric is broken down by Housing type (single-family/apartment), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
    "variables": [
      "X = Group (A vs. B)",
      "Y = Recycling compliance rate",
      "Z = Housing type (single-family/apartment)"
    ],
    "annotations": {
      "Case ID": "J2-83",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Aggregation Bias",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
      "Key Insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the higher overall Recycling compliance rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
    "expected_analysis": "Identify Simpson’s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Recycling compliance rate in every subgroup.\nConclusion: The “Group A is better” interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required.",
    "Hidden Timestamp": "Was Housing type (single-family/apartment) determined before Group (A vs. B) was chosen, and could Housing type (single-family/apartment) have influenced the choice of Group (A vs. B) before Recycling compliance rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Recycling compliance rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Housing type (single-family/apartment): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Recycling compliance rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.\nAnswer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Recycling compliance rate by the key strata (e.g., Housing type (single-family/apartment) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-84",
    "bucket": "BucketLarge-J",
    "title": "The Alumni Donation Email Survey",
    "scenario": "An organization reports a very positive statistic for Reported donation intent based only on observations from a subset of people.\nThe subset is formed by Survey response that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
    "variables": [
      "X = Survey response",
      "Y = Reported donation intent",
      "Z = Underlying true outcome (positive/negative)"
    ],
    "annotations": {
      "Case ID": "J2-84",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Easy",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
      "Key Insight": "Observed outcomes reflect who is observed, not the full population.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
    "expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Reported donation intent statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Reported donation intent occurred—and is selection related to Underlying true outcome (positive/negative) or Reported donation intent?",
    "Conditional Answers": "Answer if Survey response is randomly assigned: A difference in Reported donation intent across Survey response groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Survey response vs not-Survey response difference in Reported donation intent is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Survey response–Reported donation intent differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-85",
    "bucket": "BucketLarge-J",
    "title": "The 'Top Reviewer' Restaurant Ratings",
    "scenario": "An organization reports a very positive statistic for Average star rating based only on observations from a subset of people.\nThe subset is formed by Who leaves reviews that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
    "variables": [
      "X = Who leaves reviews",
      "Y = Average star rating",
      "Z = Underlying true outcome (positive/negative)"
    ],
    "annotations": {
      "Case ID": "J2-85",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Easy",
      "Subdomain": "Digital Media",
      "Causal Structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
      "Key Insight": "Observed outcomes reflect who is observed, not the full population.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
    "expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Average star rating statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Average star rating occurred—and is selection related to Underlying true outcome (positive/negative) or Average star rating?",
    "Conditional Answers": "Answer if Who leaves reviews is randomly assigned: A difference in Average star rating across Who leaves reviews groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Who leaves reviews vs not-Who leaves reviews difference in Average star rating is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Who leaves reviews–Average star rating differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-86",
    "bucket": "BucketLarge-J",
    "title": "The Parenting App Success Stories",
    "scenario": "An organization reports a very positive statistic for Reported improvement based only on observations from a subset of people.\nThe subset is formed by Who shares outcomes that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
    "variables": [
      "X = Who shares outcomes",
      "Y = Reported improvement",
      "Z = Underlying true outcome (positive/negative)"
    ],
    "annotations": {
      "Case ID": "J2-86",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Easy",
      "Subdomain": "Psychology",
      "Causal Structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
      "Key Insight": "Observed outcomes reflect who is observed, not the full population.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
    "expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Reported improvement statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Reported improvement occurred—and is selection related to Underlying true outcome (positive/negative) or Reported improvement?",
    "Conditional Answers": "Answer if Who shares outcomes is randomly assigned: A difference in Reported improvement across Who shares outcomes groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Who shares outcomes vs not-Who shares outcomes difference in Reported improvement is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Who shares outcomes–Reported improvement differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-87",
    "bucket": "BucketLarge-J",
    "title": "County-Level Broadband and Grades",
    "scenario": "A report finds that regions/organizations with higher County average broadband access also have higher Student grades on average.\nA commentator concludes that any individual inside a region with higher County average broadband access will have higher Student grades.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
    "variables": [
      "X = County average broadband access (group-level)",
      "Y = Student grades (individual-level)",
      "Z = Individual-level characteristics"
    ],
    "annotations": {
      "Case ID": "J2-87",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Socioeconomic",
      "Difficulty": "Medium",
      "Subdomain": "Education Sociology",
      "Causal Structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
      "Key Insight": "You cannot infer individual behavior reliably from group-level correlations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
    "expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID.",
    "Hidden Timestamp": "Is County average broadband access (group-level) measured at an aggregate level while Student grades (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Student grades (individual-level)?",
    "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how County average broadband access (group-level) relates to Student grades (individual-level) for a person."
  },
  {
    "id": "T3-BucketJ-88",
    "bucket": "BucketLarge-J",
    "title": "State-Level Unemployment and Crime",
    "scenario": "A report finds that regions/organizations with higher State unemployment rate also have higher Individual criminal behavior on average.\nA commentator concludes that any individual inside a region with higher State unemployment rate will have higher Individual criminal behavior.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
    "variables": [
      "X = State unemployment rate (group-level)",
      "Y = Individual criminal behavior (individual-level)",
      "Z = Individual-level characteristics"
    ],
    "annotations": {
      "Case ID": "J2-88",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Socioeconomic",
      "Difficulty": "Medium",
      "Subdomain": "Criminology",
      "Causal Structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
      "Key Insight": "You cannot infer individual behavior reliably from group-level correlations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
    "expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID.",
    "Hidden Timestamp": "Is State unemployment rate (group-level) measured at an aggregate level while Individual criminal behavior (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Individual criminal behavior (individual-level)?",
    "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how State unemployment rate (group-level) relates to Individual criminal behavior (individual-level) for a person."
  },
  {
    "id": "T3-BucketJ-89",
    "bucket": "BucketLarge-J",
    "title": "Company-Level Diversity and Satisfaction",
    "scenario": "A report finds that regions/organizations with higher Company diversity percentage also have higher Individual satisfaction on average.\nA commentator concludes that any individual inside a region with higher Company diversity percentage will have higher Individual satisfaction.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
    "variables": [
      "X = Company diversity percentage (group-level)",
      "Y = Individual satisfaction (individual-level)",
      "Z = Individual-level characteristics"
    ],
    "annotations": {
      "Case ID": "J2-89",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Socioeconomic",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
      "Key Insight": "You cannot infer individual behavior reliably from group-level correlations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
    "expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID.",
    "Hidden Timestamp": "Is Company diversity percentage (group-level) measured at an aggregate level while Individual satisfaction (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Individual satisfaction (individual-level)?",
    "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Company diversity percentage (group-level) relates to Individual satisfaction (individual-level) for a person."
  },
  {
    "id": "T3-BucketJ-90",
    "bucket": "BucketLarge-J",
    "title": "Choosing Between Two Tutoring Programs",
    "scenario": "A manager must choose between two interventions (Program A vs Program B) to improve test pass rate.\nA pilot dataset reports that, overall, intervention A has a lower test pass rate than intervention B.\nBut when the pilot results are stratified by baseline preparedness (high/low), intervention A has a higher test pass rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Program A vs Program B)",
      "Y = test pass rate",
      "Z = baseline preparedness (high/low)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-90",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize test pass rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves test pass rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was baseline preparedness (high/low) determined before Intervention choice (Program A vs Program B) was chosen, and could baseline preparedness (high/low) have influenced the choice of Intervention choice (Program A vs Program B) before test pass rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Program A vs Program B) on test pass rate may be reversed because the mix of subgroups differs between Intervention choice (Program A vs Program B) arms.\nAnswer if you compare within strata after stratifying/standardizing by baseline preparedness (high/low): Use the within-stratum differences (or a standardized effect). If Intervention choice (Program A vs Program B) improves test pass rate in each stratum, prefer Intervention choice (Program A vs Program B) even if the aggregate looks worse.\nAnswer if Intervention choice (Program A vs Program B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report test pass rate by the key strata (e.g., baseline preparedness (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Program A vs Program B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-91",
    "bucket": "BucketLarge-J",
    "title": "Comparing Two Telehealth Platforms",
    "scenario": "A manager must choose between two interventions (Platform A vs Platform B) to improve resolution rate within 7 days.\nA pilot dataset reports that, overall, intervention A has a lower resolution rate within 7 days than intervention B.\nBut when the pilot results are stratified by case complexity (simple/complex), intervention A has a higher resolution rate within 7 days in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Platform A vs Platform B)",
      "Y = resolution rate within 7 days",
      "Z = case complexity (simple/complex)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-91",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Health Policy",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize resolution rate within 7 days? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves resolution rate within 7 days in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was case complexity (simple/complex) determined before Intervention choice (Platform A vs Platform B) was chosen, and could case complexity (simple/complex) have influenced the choice of Intervention choice (Platform A vs Platform B) before resolution rate within 7 days was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Platform A vs Platform B) on resolution rate within 7 days may be reversed because the mix of subgroups differs between Intervention choice (Platform A vs Platform B) arms.\nAnswer if you compare within strata after stratifying/standardizing by case complexity (simple/complex): Use the within-stratum differences (or a standardized effect). If Intervention choice (Platform A vs Platform B) improves resolution rate within 7 days in each stratum, prefer Intervention choice (Platform A vs Platform B) even if the aggregate looks worse.\nAnswer if Intervention choice (Platform A vs Platform B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report resolution rate within 7 days by the key strata (e.g., case complexity (simple/complex) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Platform A vs Platform B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-92",
    "bucket": "BucketLarge-J",
    "title": "Evaluating Two Marketing Messages",
    "scenario": "A manager must choose between two interventions (Message A vs Message B) to improve purchase conversion rate.\nA pilot dataset reports that, overall, intervention A has a lower purchase conversion rate than intervention B.\nBut when the pilot results are stratified by customer segment (new/returning), intervention A has a higher purchase conversion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Message A vs Message B)",
      "Y = purchase conversion rate",
      "Z = customer segment (new/returning)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-92",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize purchase conversion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves purchase conversion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was customer segment (new/returning) determined before Intervention choice (Message A vs Message B) was chosen, and could customer segment (new/returning) have influenced the choice of Intervention choice (Message A vs Message B) before purchase conversion rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Message A vs Message B) on purchase conversion rate may be reversed because the mix of subgroups differs between Intervention choice (Message A vs Message B) arms.\nAnswer if you compare within strata after stratifying/standardizing by customer segment (new/returning): Use the within-stratum differences (or a standardized effect). If Intervention choice (Message A vs Message B) improves purchase conversion rate in each stratum, prefer Intervention choice (Message A vs Message B) even if the aggregate looks worse.\nAnswer if Intervention choice (Message A vs Message B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report purchase conversion rate by the key strata (e.g., customer segment (new/returning) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Message A vs Message B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-93",
    "bucket": "BucketLarge-J",
    "title": "Promotion Policy Across Departments",
    "scenario": "A manager must choose between two interventions (Policy A vs Policy B) to improve promotion rate.\nA pilot dataset reports that, overall, intervention A has a lower promotion rate than intervention B.\nBut when the pilot results are stratified by department (sales/engineering), intervention A has a higher promotion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Policy A vs Policy B)",
      "Y = promotion rate",
      "Z = department (sales/engineering)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-93",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize promotion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves promotion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was department (sales/engineering) determined before Intervention choice (Policy A vs Policy B) was chosen, and could department (sales/engineering) have influenced the choice of Intervention choice (Policy A vs Policy B) before promotion rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Policy A vs Policy B) on promotion rate may be reversed because the mix of subgroups differs between Intervention choice (Policy A vs Policy B) arms.\nAnswer if you compare within strata after stratifying/standardizing by department (sales/engineering): Use the within-stratum differences (or a standardized effect). If Intervention choice (Policy A vs Policy B) improves promotion rate in each stratum, prefer Intervention choice (Policy A vs Policy B) even if the aggregate looks worse.\nAnswer if Intervention choice (Policy A vs Policy B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report promotion rate by the key strata (e.g., department (sales/engineering) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Policy A vs Policy B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-94",
    "bucket": "BucketLarge-J",
    "title": "Route Choice for Delivery Drivers",
    "scenario": "A manager must choose between two interventions (Route A vs Route B) to improve on-time delivery rate.\nA pilot dataset reports that, overall, intervention A has a lower on-time delivery rate than intervention B.\nBut when the pilot results are stratified by traffic day type (normal/event), intervention A has a higher on-time delivery rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Route A vs Route B)",
      "Y = on-time delivery rate",
      "Z = traffic day type (normal/event)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-94",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize on-time delivery rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves on-time delivery rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was traffic day type (normal/event) determined before Intervention choice (Route A vs Route B) was chosen, and could traffic day type (normal/event) have influenced the choice of Intervention choice (Route A vs Route B) before on-time delivery rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Route A vs Route B) on on-time delivery rate may be reversed because the mix of subgroups differs between Intervention choice (Route A vs Route B) arms.\nAnswer if you compare within strata after stratifying/standardizing by traffic day type (normal/event): Use the within-stratum differences (or a standardized effect). If Intervention choice (Route A vs Route B) improves on-time delivery rate in each stratum, prefer Intervention choice (Route A vs Route B) even if the aggregate looks worse.\nAnswer if Intervention choice (Route A vs Route B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report on-time delivery rate by the key strata (e.g., traffic day type (normal/event) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Route A vs Route B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-95",
    "bucket": "BucketLarge-J",
    "title": "Online Course Format Change",
    "scenario": "A manager must choose between two interventions (Format A vs Format B) to improve course completion rate.\nA pilot dataset reports that, overall, intervention A has a lower course completion rate than intervention B.\nBut when the pilot results are stratified by student work hours (low/high), intervention A has a higher course completion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Format A vs Format B)",
      "Y = course completion rate",
      "Z = student work hours (low/high)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-95",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Education Sociology",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize course completion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves course completion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was student work hours (low/high) determined before Intervention choice (Format A vs Format B) was chosen, and could student work hours (low/high) have influenced the choice of Intervention choice (Format A vs Format B) before course completion rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Format A vs Format B) on course completion rate may be reversed because the mix of subgroups differs between Intervention choice (Format A vs Format B) arms.\nAnswer if you compare within strata after stratifying/standardizing by student work hours (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Format A vs Format B) improves course completion rate in each stratum, prefer Intervention choice (Format A vs Format B) even if the aggregate looks worse.\nAnswer if Intervention choice (Format A vs Format B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report course completion rate by the key strata (e.g., student work hours (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Format A vs Format B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-96",
    "bucket": "BucketLarge-J",
    "title": "Vaccination Outreach Strategy",
    "scenario": "A manager must choose between two interventions (Outreach A vs Outreach B) to improve vaccination uptake.\nA pilot dataset reports that, overall, intervention A has a lower vaccination uptake than intervention B.\nBut when the pilot results are stratified by neighborhood access level (low/high), intervention A has a higher vaccination uptake in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Outreach A vs Outreach B)",
      "Y = vaccination uptake",
      "Z = neighborhood access level (low/high)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-96",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Public Policy",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize vaccination uptake? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves vaccination uptake in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was neighborhood access level (low/high) determined before Intervention choice (Outreach A vs Outreach B) was chosen, and could neighborhood access level (low/high) have influenced the choice of Intervention choice (Outreach A vs Outreach B) before vaccination uptake was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Outreach A vs Outreach B) on vaccination uptake may be reversed because the mix of subgroups differs between Intervention choice (Outreach A vs Outreach B) arms.\nAnswer if you compare within strata after stratifying/standardizing by neighborhood access level (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Outreach A vs Outreach B) improves vaccination uptake in each stratum, prefer Intervention choice (Outreach A vs Outreach B) even if the aggregate looks worse.\nAnswer if Intervention choice (Outreach A vs Outreach B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report vaccination uptake by the key strata (e.g., neighborhood access level (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Outreach A vs Outreach B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-97",
    "bucket": "BucketLarge-J",
    "title": "Job Interview Screen Tool",
    "scenario": "A manager must choose between two interventions (Tool A vs Tool B) to improve offer rate.\nA pilot dataset reports that, overall, intervention A has a lower offer rate than intervention B.\nBut when the pilot results are stratified by applicant experience (junior/senior), intervention A has a higher offer rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Tool A vs Tool B)",
      "Y = offer rate",
      "Z = applicant experience (junior/senior)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-97",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize offer rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves offer rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was applicant experience (junior/senior) determined before Intervention choice (Tool A vs Tool B) was chosen, and could applicant experience (junior/senior) have influenced the choice of Intervention choice (Tool A vs Tool B) before offer rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Tool A vs Tool B) on offer rate may be reversed because the mix of subgroups differs between Intervention choice (Tool A vs Tool B) arms.\nAnswer if you compare within strata after stratifying/standardizing by applicant experience (junior/senior): Use the within-stratum differences (or a standardized effect). If Intervention choice (Tool A vs Tool B) improves offer rate in each stratum, prefer Intervention choice (Tool A vs Tool B) even if the aggregate looks worse.\nAnswer if Intervention choice (Tool A vs Tool B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report offer rate by the key strata (e.g., applicant experience (junior/senior) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Tool A vs Tool B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-98",
    "bucket": "BucketLarge-J",
    "title": "Customer Support Triage Rule",
    "scenario": "A manager must choose between two interventions (Rule A vs Rule B) to improve satisfaction score.\nA pilot dataset reports that, overall, intervention A has a lower satisfaction score than intervention B.\nBut when the pilot results are stratified by ticket severity (low/high), intervention A has a higher satisfaction score in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
    "variables": [
      "X = Intervention choice (Rule A vs Rule B)",
      "Y = satisfaction score",
      "Z = ticket severity (low/high)",
      "U = underlying difficulty / baseline risk that affects Y"
    ],
    "annotations": {
      "Case ID": "J2-98",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Digital Media",
      "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
      "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize satisfaction score? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
    "expected_analysis": "Diagnose Simpson’s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves satisfaction score in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
    "Hidden Timestamp": "Was ticket severity (low/high) determined before Intervention choice (Rule A vs Rule B) was chosen, and could ticket severity (low/high) have influenced the choice of Intervention choice (Rule A vs Rule B) before satisfaction score was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Rule A vs Rule B) on satisfaction score may be reversed because the mix of subgroups differs between Intervention choice (Rule A vs Rule B) arms.\nAnswer if you compare within strata after stratifying/standardizing by ticket severity (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Rule A vs Rule B) improves satisfaction score in each stratum, prefer Intervention choice (Rule A vs Rule B) even if the aggregate looks worse.\nAnswer if Intervention choice (Rule A vs Rule B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report satisfaction score by the key strata (e.g., ticket severity (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Rule A vs Rule B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-99",
    "bucket": "BucketLarge-J",
    "title": "Gentrification and Average Income",
    "scenario": "A policymaker implements an intervention: New housing permits.\nAfter the change, the reported metric average neighborhood income improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (New housing permits)",
      "Y = Reported metric (average neighborhood income)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-99",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average neighborhood income prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (New housing permits) change who is included in the denominator before Reported metric (average neighborhood income) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average neighborhood income) after changing Intervention (New housing permits) can reflect a real outcome shift.\nAnswer if Intervention (New housing permits) changes who is counted: The aggregate Reported metric (average neighborhood income) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average neighborhood income) may be moving because the denominator/population changed after Intervention (New housing permits). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-100",
    "bucket": "BucketLarge-J",
    "title": "Expanded Advanced-Placement Access",
    "scenario": "A policymaker implements an intervention: Opening AP enrollment.\nAfter the change, the reported metric average AP exam score improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Opening AP enrollment)",
      "Y = Reported metric (average AP exam score)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-100",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average AP exam score prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Opening AP enrollment) change who is included in the denominator before Reported metric (average AP exam score) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average AP exam score) after changing Intervention (Opening AP enrollment) can reflect a real outcome shift.\nAnswer if Intervention (Opening AP enrollment) changes who is counted: The aggregate Reported metric (average AP exam score) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average AP exam score) may be moving because the denominator/population changed after Intervention (Opening AP enrollment). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-101",
    "bucket": "BucketLarge-J",
    "title": "Corporate Merger and Diversity Metrics",
    "scenario": "A policymaker implements an intervention: Merging with a contractor-heavy firm.\nAfter the change, the reported metric percent women in 'full-time staff' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Merging with a contractor-heavy firm)",
      "Y = Reported metric (percent women in 'full-time staff')",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-101",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in percent women in 'full-time staff' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Merging with a contractor-heavy firm) change who is included in the denominator before Reported metric (percent women in 'full-time staff') was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (percent women in 'full-time staff') after changing Intervention (Merging with a contractor-heavy firm) can reflect a real outcome shift.\nAnswer if Intervention (Merging with a contractor-heavy firm) changes who is counted: The aggregate Reported metric (percent women in 'full-time staff') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (percent women in 'full-time staff') may be moving because the denominator/population changed after Intervention (Merging with a contractor-heavy firm). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-102",
    "bucket": "BucketLarge-J",
    "title": "New Transit Line and Commute Times",
    "scenario": "A policymaker implements an intervention: Launching a transit line.\nAfter the change, the reported metric average commute time of 'drivers' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Launching a transit line)",
      "Y = Reported metric (average commute time of 'drivers')",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-102",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Public Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average commute time of 'drivers' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Launching a transit line) change who is included in the denominator before Reported metric (average commute time of 'drivers') was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average commute time of 'drivers') after changing Intervention (Launching a transit line) can reflect a real outcome shift.\nAnswer if Intervention (Launching a transit line) changes who is counted: The aggregate Reported metric (average commute time of 'drivers') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average commute time of 'drivers') may be moving because the denominator/population changed after Intervention (Launching a transit line). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-103",
    "bucket": "BucketLarge-J",
    "title": "Clinic Screening Campaign",
    "scenario": "A policymaker implements an intervention: Free screening days.\nAfter the change, the reported metric positive test fraction among tested improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Free screening days)",
      "Y = Reported metric (positive test fraction among tested)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-103",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Health Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in positive test fraction among tested prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Free screening days) change who is included in the denominator before Reported metric (positive test fraction among tested) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (positive test fraction among tested) after changing Intervention (Free screening days) can reflect a real outcome shift.\nAnswer if Intervention (Free screening days) changes who is counted: The aggregate Reported metric (positive test fraction among tested) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (positive test fraction among tested) may be moving because the denominator/population changed after Intervention (Free screening days). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-104",
    "bucket": "BucketLarge-J",
    "title": "School Attendance Incentives",
    "scenario": "A policymaker implements an intervention: Attendance rewards.\nAfter the change, the reported metric average absence rate among 'enrolled students' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Attendance rewards)",
      "Y = Reported metric (average absence rate among 'enrolled students')",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-104",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Sociology",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average absence rate among 'enrolled students' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Attendance rewards) change who is included in the denominator before Reported metric (average absence rate among 'enrolled students') was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average absence rate among 'enrolled students') after changing Intervention (Attendance rewards) can reflect a real outcome shift.\nAnswer if Intervention (Attendance rewards) changes who is counted: The aggregate Reported metric (average absence rate among 'enrolled students') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average absence rate among 'enrolled students') may be moving because the denominator/population changed after Intervention (Attendance rewards). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-105",
    "bucket": "BucketLarge-J",
    "title": "Immigration Policy and Unemployment",
    "scenario": "A policymaker implements an intervention: New work visa program.\nAfter the change, the reported metric city unemployment rate improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (New work visa program)",
      "Y = Reported metric (city unemployment rate)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-105",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Labor Economics",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in city unemployment rate prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (New work visa program) change who is included in the denominator before Reported metric (city unemployment rate) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (city unemployment rate) after changing Intervention (New work visa program) can reflect a real outcome shift.\nAnswer if Intervention (New work visa program) changes who is counted: The aggregate Reported metric (city unemployment rate) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (city unemployment rate) may be moving because the denominator/population changed after Intervention (New work visa program). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-106",
    "bucket": "BucketLarge-J",
    "title": "Crime Reporting App",
    "scenario": "A policymaker implements an intervention: Launching a reporting app.\nAfter the change, the reported metric reported crime incidents improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Launching a reporting app)",
      "Y = Reported metric (reported crime incidents)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-106",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Criminology",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in reported crime incidents prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Launching a reporting app) change who is included in the denominator before Reported metric (reported crime incidents) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (reported crime incidents) after changing Intervention (Launching a reporting app) can reflect a real outcome shift.\nAnswer if Intervention (Launching a reporting app) changes who is counted: The aggregate Reported metric (reported crime incidents) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (reported crime incidents) may be moving because the denominator/population changed after Intervention (Launching a reporting app). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-107",
    "bucket": "BucketLarge-J",
    "title": "Social Platform Safety Push",
    "scenario": "A policymaker implements an intervention: Stricter moderation.\nAfter the change, the reported metric average toxicity among remaining posts improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Stricter moderation)",
      "Y = Reported metric (average toxicity among remaining posts)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-107",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Digital Media",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average toxicity among remaining posts prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Stricter moderation) change who is included in the denominator before Reported metric (average toxicity among remaining posts) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average toxicity among remaining posts) after changing Intervention (Stricter moderation) can reflect a real outcome shift.\nAnswer if Intervention (Stricter moderation) changes who is counted: The aggregate Reported metric (average toxicity among remaining posts) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average toxicity among remaining posts) may be moving because the denominator/population changed after Intervention (Stricter moderation). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-108",
    "bucket": "BucketLarge-J",
    "title": "Workplace Safety Outsourcing",
    "scenario": "A policymaker implements an intervention: Outsourcing hazardous tasks.\nAfter the change, the reported metric injury rate among remaining employees improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Outsourcing hazardous tasks)",
      "Y = Reported metric (injury rate among remaining employees)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-108",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in injury rate among remaining employees prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Outsourcing hazardous tasks) change who is included in the denominator before Reported metric (injury rate among remaining employees) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (injury rate among remaining employees) after changing Intervention (Outsourcing hazardous tasks) can reflect a real outcome shift.\nAnswer if Intervention (Outsourcing hazardous tasks) changes who is counted: The aggregate Reported metric (injury rate among remaining employees) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (injury rate among remaining employees) may be moving because the denominator/population changed after Intervention (Outsourcing hazardous tasks). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-109",
    "bucket": "BucketLarge-J",
    "title": "Scholarship Program Expansion",
    "scenario": "A policymaker implements an intervention: Lowering eligibility threshold.\nAfter the change, the reported metric average GPA of scholarship recipients improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Lowering eligibility threshold)",
      "Y = Reported metric (average GPA of scholarship recipients)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-109",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average GPA of scholarship recipients prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Lowering eligibility threshold) change who is included in the denominator before Reported metric (average GPA of scholarship recipients) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average GPA of scholarship recipients) after changing Intervention (Lowering eligibility threshold) can reflect a real outcome shift.\nAnswer if Intervention (Lowering eligibility threshold) changes who is counted: The aggregate Reported metric (average GPA of scholarship recipients) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average GPA of scholarship recipients) may be moving because the denominator/population changed after Intervention (Lowering eligibility threshold). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-110",
    "bucket": "BucketLarge-J",
    "title": "Vaccination Priority Change",
    "scenario": "A policymaker implements an intervention: Expanding eligibility.\nAfter the change, the reported metric hospitalizations among vaccinated people improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Expanding eligibility)",
      "Y = Reported metric (hospitalizations among vaccinated people)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-110",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Public Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in hospitalizations among vaccinated people prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Expanding eligibility) change who is included in the denominator before Reported metric (hospitalizations among vaccinated people) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (hospitalizations among vaccinated people) after changing Intervention (Expanding eligibility) can reflect a real outcome shift.\nAnswer if Intervention (Expanding eligibility) changes who is counted: The aggregate Reported metric (hospitalizations among vaccinated people) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (hospitalizations among vaccinated people) may be moving because the denominator/population changed after Intervention (Expanding eligibility). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-111",
    "bucket": "BucketLarge-J",
    "title": "Loan Program Tightening",
    "scenario": "A policymaker implements an intervention: Raising credit score requirement.\nAfter the change, the reported metric default rate among approved borrowers improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Raising credit score requirement)",
      "Y = Reported metric (default rate among approved borrowers)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-111",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Labor Economics",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in default rate among approved borrowers prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Raising credit score requirement) change who is included in the denominator before Reported metric (default rate among approved borrowers) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (default rate among approved borrowers) after changing Intervention (Raising credit score requirement) can reflect a real outcome shift.\nAnswer if Intervention (Raising credit score requirement) changes who is counted: The aggregate Reported metric (default rate among approved borrowers) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (default rate among approved borrowers) may be moving because the denominator/population changed after Intervention (Raising credit score requirement). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-112",
    "bucket": "BucketLarge-J",
    "title": "University Admissions Expansion",
    "scenario": "A policymaker implements an intervention: Adding a new campus.\nAfter the change, the reported metric average SAT score of admitted students improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Adding a new campus)",
      "Y = Reported metric (average SAT score of admitted students)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-112",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Sociology",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average SAT score of admitted students prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Adding a new campus) change who is included in the denominator before Reported metric (average SAT score of admitted students) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average SAT score of admitted students) after changing Intervention (Adding a new campus) can reflect a real outcome shift.\nAnswer if Intervention (Adding a new campus) changes who is counted: The aggregate Reported metric (average SAT score of admitted students) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average SAT score of admitted students) may be moving because the denominator/population changed after Intervention (Adding a new campus). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-113",
    "bucket": "BucketLarge-J",
    "title": "Housing Voucher Reform",
    "scenario": "A policymaker implements an intervention: Changing voucher eligibility.\nAfter the change, the reported metric average rent paid by voucher holders improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Changing voucher eligibility)",
      "Y = Reported metric (average rent paid by voucher holders)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-113",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Public Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average rent paid by voucher holders prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Changing voucher eligibility) change who is included in the denominator before Reported metric (average rent paid by voucher holders) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average rent paid by voucher holders) after changing Intervention (Changing voucher eligibility) can reflect a real outcome shift.\nAnswer if Intervention (Changing voucher eligibility) changes who is counted: The aggregate Reported metric (average rent paid by voucher holders) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average rent paid by voucher holders) may be moving because the denominator/population changed after Intervention (Changing voucher eligibility). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-114",
    "bucket": "BucketLarge-J",
    "title": "Wellness Program Redesign",
    "scenario": "A policymaker implements an intervention: Changing sign-up incentives.\nAfter the change, the reported metric average weight loss among participants improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Changing sign-up incentives)",
      "Y = Reported metric (average weight loss among participants)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-114",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average weight loss among participants prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Changing sign-up incentives) change who is included in the denominator before Reported metric (average weight loss among participants) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average weight loss among participants) after changing Intervention (Changing sign-up incentives) can reflect a real outcome shift.\nAnswer if Intervention (Changing sign-up incentives) changes who is counted: The aggregate Reported metric (average weight loss among participants) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average weight loss among participants) may be moving because the denominator/population changed after Intervention (Changing sign-up incentives). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-115",
    "bucket": "BucketLarge-J",
    "title": "Traffic Enforcement Shift",
    "scenario": "A policymaker implements an intervention: Moving enforcement to highways.\nAfter the change, the reported metric average speed on arterial roads improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Moving enforcement to highways)",
      "Y = Reported metric (average speed on arterial roads)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-115",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average speed on arterial roads prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Moving enforcement to highways) change who is included in the denominator before Reported metric (average speed on arterial roads) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average speed on arterial roads) after changing Intervention (Moving enforcement to highways) can reflect a real outcome shift.\nAnswer if Intervention (Moving enforcement to highways) changes who is counted: The aggregate Reported metric (average speed on arterial roads) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average speed on arterial roads) may be moving because the denominator/population changed after Intervention (Moving enforcement to highways). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-116",
    "bucket": "BucketLarge-J",
    "title": "Teacher Evaluation Metric Change",
    "scenario": "A policymaker implements an intervention: Switching evaluation to test-score growth.\nAfter the change, the reported metric average teacher rating improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
    "variables": [
      "X = Intervention (Switching evaluation to test-score growth)",
      "Y = Reported metric (average teacher rating)",
      "M = membership / eligibility / inclusion rule (who is counted)",
      "Y_ind = individual-level outcome for a fixed person"
    ],
    "annotations": {
      "Case ID": "J2-116",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
      "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the change in average teacher rating prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
    "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
    "Hidden Timestamp": "Did Intervention (Switching evaluation to test-score growth) change who is included in the denominator before Reported metric (average teacher rating) was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average teacher rating) after changing Intervention (Switching evaluation to test-score growth) can reflect a real outcome shift.\nAnswer if Intervention (Switching evaluation to test-score growth) changes who is counted: The aggregate Reported metric (average teacher rating) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average teacher rating) may be moving because the denominator/population changed after Intervention (Switching evaluation to test-score growth). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-117",
    "bucket": "BucketLarge-J",
    "title": "Voluntary Leadership Workshop",
    "scenario": "A decision-maker considers scaling an intervention: Attending the workshop.\nIn observational data, people who receive the intervention have better outcomes on subsequent performance rating than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (Attending the workshop)",
      "Y = Outcome (subsequent performance rating)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-117",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering Attending the workshop to everyone will improve subsequent performance rating? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (subsequent performance rating) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (Attending the workshop) is randomly assigned: A difference in Outcome (subsequent performance rating) across Intervention uptake (Attending the workshop) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Attending the workshop) vs not-Intervention uptake (Attending the workshop) difference in Outcome (subsequent performance rating) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Attending the workshop)–Outcome (subsequent performance rating) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-118",
    "bucket": "BucketLarge-J",
    "title": "Optional SAT Prep Course",
    "scenario": "A decision-maker considers scaling an intervention: Taking the prep course.\nIn observational data, people who receive the intervention have better outcomes on SAT score improvement than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (Taking the prep course)",
      "Y = Outcome (SAT score improvement)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-118",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Education Sociology",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering Taking the prep course to everyone will improve SAT score improvement? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (SAT score improvement) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (Taking the prep course) is randomly assigned: A difference in Outcome (SAT score improvement) across Intervention uptake (Taking the prep course) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Taking the prep course) vs not-Intervention uptake (Taking the prep course) difference in Outcome (SAT score improvement) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Taking the prep course)–Outcome (SAT score improvement) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-119",
    "bucket": "BucketLarge-J",
    "title": "Fitness App Premium Subscription",
    "scenario": "A decision-maker considers scaling an intervention: Upgrading to premium.\nIn observational data, people who receive the intervention have better outcomes on weekly workouts than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (Upgrading to premium)",
      "Y = Outcome (weekly workouts)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-119",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering Upgrading to premium to everyone will improve weekly workouts? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (weekly workouts) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (Upgrading to premium) is randomly assigned: A difference in Outcome (weekly workouts) across Intervention uptake (Upgrading to premium) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Upgrading to premium) vs not-Intervention uptake (Upgrading to premium) difference in Outcome (weekly workouts) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Upgrading to premium)–Outcome (weekly workouts) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-120",
    "bucket": "BucketLarge-J",
    "title": "Mental Health Counseling",
    "scenario": "A decision-maker considers scaling an intervention: Starting therapy.\nIn observational data, people who receive the intervention have better outcomes on reported stress score than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (Starting therapy)",
      "Y = Outcome (reported stress score)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-120",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Psychology",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering Starting therapy to everyone will improve reported stress score? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (reported stress score) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (Starting therapy) is randomly assigned: A difference in Outcome (reported stress score) across Intervention uptake (Starting therapy) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Starting therapy) vs not-Intervention uptake (Starting therapy) difference in Outcome (reported stress score) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Starting therapy)–Outcome (reported stress score) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-121",
    "bucket": "BucketLarge-J",
    "title": "Charter School Enrollment",
    "scenario": "A decision-maker considers scaling an intervention: Enrolling in charter school.\nIn observational data, people who receive the intervention have better outcomes on graduation likelihood than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (Enrolling in charter school)",
      "Y = Outcome (graduation likelihood)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-121",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Education Policy",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering Enrolling in charter school to everyone will improve graduation likelihood? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (graduation likelihood) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (Enrolling in charter school) is randomly assigned: A difference in Outcome (graduation likelihood) across Intervention uptake (Enrolling in charter school) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Enrolling in charter school) vs not-Intervention uptake (Enrolling in charter school) difference in Outcome (graduation likelihood) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Enrolling in charter school)–Outcome (graduation likelihood) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-122",
    "bucket": "BucketLarge-J",
    "title": "Job Training Voucher",
    "scenario": "A decision-maker considers scaling an intervention: Using the voucher.\nIn observational data, people who receive the intervention have better outcomes on employment after 6 months than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (Using the voucher)",
      "Y = Outcome (employment after 6 months)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-122",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Labor Economics",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering Using the voucher to everyone will improve employment after 6 months? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (employment after 6 months) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (Using the voucher) is randomly assigned: A difference in Outcome (employment after 6 months) across Intervention uptake (Using the voucher) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Using the voucher) vs not-Intervention uptake (Using the voucher) difference in Outcome (employment after 6 months) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Using the voucher)–Outcome (employment after 6 months) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-123",
    "bucket": "BucketLarge-J",
    "title": "Preventive Health Screening",
    "scenario": "A decision-maker considers scaling an intervention: getting screened.\nIn observational data, people who receive the intervention have better outcomes on later hospitalization than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (getting screened)",
      "Y = Outcome (later hospitalization)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-123",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Health Policy",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering getting screened to everyone will improve later hospitalization? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (later hospitalization) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (getting screened) is randomly assigned: A difference in Outcome (later hospitalization) across Intervention uptake (getting screened) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (getting screened) vs not-Intervention uptake (getting screened) difference in Outcome (later hospitalization) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (getting screened)–Outcome (later hospitalization) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-124",
    "bucket": "BucketLarge-J",
    "title": "Remote Work Opt-In",
    "scenario": "A decision-maker considers scaling an intervention: opting into remote work.\nIn observational data, people who receive the intervention have better outcomes on output per hour than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (opting into remote work)",
      "Y = Outcome (output per hour)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-124",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Labor Economics",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering opting into remote work to everyone will improve output per hour? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (output per hour) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (opting into remote work) is randomly assigned: A difference in Outcome (output per hour) across Intervention uptake (opting into remote work) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (opting into remote work) vs not-Intervention uptake (opting into remote work) difference in Outcome (output per hour) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (opting into remote work)–Outcome (output per hour) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-125",
    "bucket": "BucketLarge-J",
    "title": "Startup Accelerator Admission",
    "scenario": "A decision-maker considers scaling an intervention: joining accelerator.\nIn observational data, people who receive the intervention have better outcomes on funding raised than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
    "variables": [
      "X = Intervention uptake (joining accelerator)",
      "Y = Outcome (funding raised)",
      "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-125",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Digital Media",
      "Causal Structure": "U → X and U → Y; naive comparison of treated vs untreated confounds effect of X with U.",
      "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude that offering joining accelerator to everyone will improve funding raised? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
    "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Outcome (funding raised) occurred—and could selection depend on the outcome?",
    "Conditional Answers": "Answer if Intervention uptake (joining accelerator) is randomly assigned: A difference in Outcome (funding raised) across Intervention uptake (joining accelerator) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (joining accelerator) vs not-Intervention uptake (joining accelerator) difference in Outcome (funding raised) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (joining accelerator)–Outcome (funding raised) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-126",
    "bucket": "BucketLarge-J",
    "title": "Among Admitted Applicants",
    "scenario": "An analyst studies the relationship between test score and essay quality, but only within a selected group defined by admission decision.\nInside that selected group, test score and essay quality appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (admission decision) is influenced by both test score and essay quality, making it a collider.",
    "variables": [
      "A = test score",
      "B = essay quality",
      "S = selection criterion (admission decision)",
      "Y = Outcome (first-year GPA)"
    ],
    "annotations": {
      "Case ID": "J2-126",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (first-year GPA)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (first-year GPA) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-127",
    "bucket": "BucketLarge-J",
    "title": "Among Promoted Employees",
    "scenario": "An analyst studies the relationship between hours worked and manager liking, but only within a selected group defined by promotion.\nInside that selected group, hours worked and manager liking appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (promotion) is influenced by both hours worked and manager liking, making it a collider.",
    "variables": [
      "A = hours worked",
      "B = manager liking",
      "S = selection criterion (promotion)",
      "Y = Outcome (later performance)"
    ],
    "annotations": {
      "Case ID": "J2-127",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (later performance)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (later performance) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-128",
    "bucket": "BucketLarge-J",
    "title": "Among Hospitalized Patients",
    "scenario": "An analyst studies the relationship between age and comorbidity index, but only within a selected group defined by hospitalization.\nInside that selected group, age and comorbidity index appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (hospitalization) is influenced by both age and comorbidity index, making it a collider.",
    "variables": [
      "A = age",
      "B = comorbidity index",
      "S = selection criterion (hospitalization)",
      "Y = Outcome (mortality)"
    ],
    "annotations": {
      "Case ID": "J2-128",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Health Policy",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (mortality)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (mortality) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-129",
    "bucket": "BucketLarge-J",
    "title": "Among Viral Posts",
    "scenario": "An analyst studies the relationship between topic controversy and creator follower count, but only within a selected group defined by going viral.\nInside that selected group, topic controversy and creator follower count appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (going viral) is influenced by both topic controversy and creator follower count, making it a collider.",
    "variables": [
      "A = topic controversy",
      "B = creator follower count",
      "S = selection criterion (going viral)",
      "Y = Outcome (misinformation rate)"
    ],
    "annotations": {
      "Case ID": "J2-129",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Digital Media",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (misinformation rate)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (misinformation rate) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-130",
    "bucket": "BucketLarge-J",
    "title": "Among Scholarship Winners",
    "scenario": "An analyst studies the relationship between family income and grades, but only within a selected group defined by winning scholarship.\nInside that selected group, family income and grades appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (winning scholarship) is influenced by both family income and grades, making it a collider.",
    "variables": [
      "A = family income",
      "B = grades",
      "S = selection criterion (winning scholarship)",
      "Y = Outcome (college persistence)"
    ],
    "annotations": {
      "Case ID": "J2-130",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Education Sociology",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (college persistence)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (college persistence) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-131",
    "bucket": "BucketLarge-J",
    "title": "Among Loan Approvals",
    "scenario": "An analyst studies the relationship between income and credit history, but only within a selected group defined by loan approval.\nInside that selected group, income and credit history appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (loan approval) is influenced by both income and credit history, making it a collider.",
    "variables": [
      "A = income",
      "B = credit history",
      "S = selection criterion (loan approval)",
      "Y = Outcome (default)"
    ],
    "annotations": {
      "Case ID": "J2-131",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Labor Economics",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (default)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (default) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-132",
    "bucket": "BucketLarge-J",
    "title": "Among Arrested Individuals",
    "scenario": "An analyst studies the relationship between crime severity and police presence, but only within a selected group defined by being arrested.\nInside that selected group, crime severity and police presence appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (being arrested) is influenced by both crime severity and police presence, making it a collider.",
    "variables": [
      "A = crime severity",
      "B = police presence",
      "S = selection criterion (being arrested)",
      "Y = Outcome (conviction)"
    ],
    "annotations": {
      "Case ID": "J2-132",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Criminology",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (conviction)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (conviction) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-133",
    "bucket": "BucketLarge-J",
    "title": "Among Top-Sales Reps",
    "scenario": "An analyst studies the relationship between territory quality and call volume, but only within a selected group defined by being top-10% in sales.\nInside that selected group, territory quality and call volume appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (being top-10% in sales) is influenced by both territory quality and call volume, making it a collider.",
    "variables": [
      "A = territory quality",
      "B = call volume",
      "S = selection criterion (being top-10% in sales)",
      "Y = Outcome (customer churn)"
    ],
    "annotations": {
      "Case ID": "J2-133",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (customer churn)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (customer churn) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-134",
    "bucket": "BucketLarge-J",
    "title": "Among Published Papers",
    "scenario": "An analyst studies the relationship between study novelty and p-value, but only within a selected group defined by publication.\nInside that selected group, study novelty and p-value appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (publication) is influenced by both study novelty and p-value, making it a collider.",
    "variables": [
      "A = study novelty",
      "B = p-value",
      "S = selection criterion (publication)",
      "Y = Outcome (replication success)"
    ],
    "annotations": {
      "Case ID": "J2-134",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Psychology",
      "Causal Structure": "A → S ← B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
      "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
    "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (replication success)?",
    "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (replication success) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-135",
    "bucket": "BucketLarge-J",
    "title": "More Break-ins After Installing Cameras?",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of number of break-in reports.\nThey point to a larger number of events in one group or after installing cameras.\nBut the groups have very different base sizes or exposure levels (neighborhood size differs), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (installing cameras)",
      "Y = Event count (number of break-in reports)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-135",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Criminology",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (number of break-in reports) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (number of break-in reports): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (number of break-in reports) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (number of break-in reports) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-136",
    "bucket": "BucketLarge-J",
    "title": "Side Effects After New Medication Rollout",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of side-effect reports.\nThey point to a larger number of events in one group or after new medication rollout.\nBut the groups have very different base sizes or exposure levels (exposure population differs), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (new medication rollout)",
      "Y = Event count (side-effect reports)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-136",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Health Policy",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (side-effect reports) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (side-effect reports): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (side-effect reports) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (side-effect reports) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-137",
    "bucket": "BucketLarge-J",
    "title": "Discipline Disparities After Rule Change",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of discipline referrals.\nThey point to a larger number of events in one group or after new conduct rule.\nBut the groups have very different base sizes or exposure levels (group enrollment differs), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (new conduct rule)",
      "Y = Event count (discipline referrals)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-137",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Education Policy",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (discipline referrals) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (discipline referrals): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (discipline referrals) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (discipline referrals) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-138",
    "bucket": "BucketLarge-J",
    "title": "Fraud Alerts in Two Stores",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of fraud alerts.\nThey point to a larger number of events in one group or after fraud filter in Store A vs B.\nBut the groups have very different base sizes or exposure levels (transaction volume differs), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (fraud filter in Store A vs B)",
      "Y = Event count (fraud alerts)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-138",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (fraud alerts) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (fraud alerts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (fraud alerts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (fraud alerts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-139",
    "bucket": "BucketLarge-J",
    "title": "Misinformation Flags Across Topics",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of flagged posts.\nThey point to a larger number of events in one group or after topic category.\nBut the groups have very different base sizes or exposure levels (posting volume differs), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (topic category)",
      "Y = Event count (flagged posts)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-139",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Digital Media",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (flagged posts) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (flagged posts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (flagged posts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (flagged posts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-140",
    "bucket": "BucketLarge-J",
    "title": "Accidents in Two Factories",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of injury counts.\nThey point to a larger number of events in one group or after safety training.\nBut the groups have very different base sizes or exposure levels (work-hours differ), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (safety training)",
      "Y = Event count (injury counts)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-140",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (injury counts) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (injury counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (injury counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (injury counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-141",
    "bucket": "BucketLarge-J",
    "title": "Defaults in Two Loan Products",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of default counts.\nThey point to a larger number of events in one group or after loan product type.\nBut the groups have very different base sizes or exposure levels (number of loans differs), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (loan product type)",
      "Y = Event count (default counts)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-141",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (default counts) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (default counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (default counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (default counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-142",
    "bucket": "BucketLarge-J",
    "title": "Hospitalizations Among Vaccinated vs Unvaccinated",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of hospitalized counts.\nThey point to a larger number of events in one group or after vaccination status.\nBut the groups have very different base sizes or exposure levels (base rate of vaccination differs), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (vaccination status)",
      "Y = Event count (hospitalized counts)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-142",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Public Policy",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (hospitalized counts) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (hospitalized counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (hospitalized counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (hospitalized counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-143",
    "bucket": "BucketLarge-J",
    "title": "Complaints After Service Plan Launch",
    "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of complaint counts.\nThey point to a larger number of events in one group or after service plan launch.\nBut the groups have very different base sizes or exposure levels (customer base changed), so comparing counts without denominators is misleading.",
    "variables": [
      "X = Group/intervention status (service plan launch)",
      "Y = Event count (complaint counts)",
      "N = population size or exposure (denominator)",
      "R = event rate = Y / N"
    ],
    "annotations": {
      "Case ID": "J2-143",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Easy",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
      "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
    "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
    "Hidden Timestamp": "What is the relevant denominator at the time Event count (complaint counts) was counted, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Event count (complaint counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (complaint counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (complaint counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-144",
    "bucket": "BucketLarge-J",
    "title": "Would I Have Been Admitted If I Retook the Test?",
    "scenario": "Applicant retakes standardized test after a rejection.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Retake decision",
      "Y = Admission outcome",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-144",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Retake decision was (or could have been) chosen, what information was available, and when was Admission outcome realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Retake decision is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-145",
    "bucket": "BucketLarge-J",
    "title": "Turning Down a Job Offer and Later Regret",
    "scenario": "A candidate declines a job offer and later experiences a downturn in their current job.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Accept offer",
      "Y = Income stability after 1 year",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-145",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Preemption",
      "Trap Subtype": "Late Preemption",
      "Difficulty": "Medium",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Accept offer was (or could have been) chosen, what information was available, and when was Income stability after 1 year realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Accept offer is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-146",
    "bucket": "BucketLarge-J",
    "title": "Parole Decision and Future Crime",
    "scenario": "A parole board denies release; later debates whether release would have increased reoffending.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Release on parole",
      "Y = Reoffense within 2 years",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-146",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Preemption",
      "Trap Subtype": "Late Preemption",
      "Difficulty": "Medium",
      "Subdomain": "Criminology",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Release on parole was (or could have been) chosen, what information was available, and when was Reoffense within 2 years realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Release on parole is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-147",
    "bucket": "BucketLarge-J",
    "title": "Medical Treatment Refusal",
    "scenario": "A patient refuses a recommended procedure and later recovers slowly.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Receive procedure",
      "Y = Recovery time",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-147",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Medium",
      "Subdomain": "Health Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Receive procedure was (or could have been) chosen, what information was available, and when was Recovery time realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Receive procedure is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-148",
    "bucket": "BucketLarge-J",
    "title": "Choosing a Major Switch",
    "scenario": "A student stays in a difficult major and wonders if switching would have improved GPA.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Switch major",
      "Y = GPA after two semesters",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-148",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Feedback Loops",
      "Trap Subtype": "Dynamic World Divergence",
      "Difficulty": "Medium",
      "Subdomain": "Education Sociology",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Switch major was (or could have been) chosen, what information was available, and when was GPA after two semesters realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Switch major is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-149",
    "bucket": "BucketLarge-J",
    "title": "Scholarship Cutoff Appeal",
    "scenario": "A student is just below a scholarship cutoff; committee debates if scholarship would change graduation probability.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Receive scholarship",
      "Y = Graduation",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-149",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Receive scholarship was (or could have been) chosen, what information was available, and when was Graduation realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Receive scholarship is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-150",
    "bucket": "BucketLarge-J",
    "title": "Minimum Wage Increase and Employment",
    "scenario": "A city raises minimum wage; policymakers ask what employment would have been without the change.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Minimum wage increase",
      "Y = Employment rate",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-150",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounder–Mediator Error",
      "Trap Subtype": "Mediator Fixing Error",
      "Difficulty": "Medium",
      "Subdomain": "Public Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Minimum wage increase was (or could have been) chosen, what information was available, and when was Employment rate realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Minimum wage increase is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-151",
    "bucket": "BucketLarge-J",
    "title": "Congestion Pricing Rollout",
    "scenario": "Congestion pricing is introduced; the city asks what traffic would have been otherwise.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Introduce congestion pricing",
      "Y = Average travel time",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-151",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Feedback Loops",
      "Trap Subtype": "Dynamic World Divergence",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Introduce congestion pricing was (or could have been) chosen, what information was available, and when was Average travel time realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Introduce congestion pricing is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-152",
    "bucket": "BucketLarge-J",
    "title": "School Funding Reform",
    "scenario": "A district increases funding to low-income schools; asks what scores would have been absent reform.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Funding reform",
      "Y = Standardized test scores",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-152",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Preemption",
      "Trap Subtype": "Early Preemption",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Funding reform was (or could have been) chosen, what information was available, and when was Standardized test scores realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Funding reform is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-153",
    "bucket": "BucketLarge-J",
    "title": "Housing Construction Moratorium",
    "scenario": "A construction ban is enacted; residents debate what rents would have been without the ban.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Construction moratorium",
      "Y = Median rent",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-153",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Construction moratorium was (or could have been) chosen, what information was available, and when was Median rent realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Construction moratorium is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-154",
    "bucket": "BucketLarge-J",
    "title": "Environmental Regulation Change",
    "scenario": "A new emissions rule is enacted; regulators ask what pollution levels would have been without it.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Emissions regulation",
      "Y = Air pollutant concentration",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-154",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Public Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Emissions regulation was (or could have been) chosen, what information was available, and when was Air pollutant concentration realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Emissions regulation is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-155",
    "bucket": "BucketLarge-J",
    "title": "Public Transit Expansion",
    "scenario": "New bus routes are added; question is what car usage would have been otherwise.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Transit expansion",
      "Y = Car trips per capita",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-155",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounder–Mediator Error",
      "Trap Subtype": "Mediator Fixing Error",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Transit expansion was (or could have been) chosen, what information was available, and when was Car trips per capita realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Transit expansion is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-156",
    "bucket": "BucketLarge-J",
    "title": "Algorithmic Fairness Audit: Who Was Harmed?",
    "scenario": "A recommender model is updated; auditors ask how outcomes would differ for users under the old model.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Model version",
      "Y = Exposure to harmful content",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-156",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounder–Mediator Error",
      "Trap Subtype": "Mediator Fixing Error",
      "Difficulty": "Medium",
      "Subdomain": "Digital Media",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Model version was (or could have been) chosen, what information was available, and when was Exposure to harmful content realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Model version is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-157",
    "bucket": "BucketLarge-J",
    "title": "Discrimination Attribution in Hiring",
    "scenario": "A candidate is rejected; dispute whether rejection was due to bias or fit.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Candidate protected attribute",
      "Y = Hiring decision",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-157",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Preemption",
      "Trap Subtype": "Early Preemption",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Candidate protected attribute was (or could have been) chosen, what information was available, and when was Hiring decision realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Candidate protected attribute is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-158",
    "bucket": "BucketLarge-J",
    "title": "Quota Policy Evaluation",
    "scenario": "A university adds a quota; asks how composition and outcomes would look without it.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Quota policy",
      "Y = Graduation outcomes across groups",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-158",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Feedback Loops",
      "Trap Subtype": "Dynamic World Divergence",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Quota policy was (or could have been) chosen, what information was available, and when was Graduation outcomes across groups realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Quota policy is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-159",
    "bucket": "BucketLarge-J",
    "title": "Economic Stimulus Effectiveness",
    "scenario": "Stimulus checks are issued; question is what spending would have been absent stimulus.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Stimulus payment",
      "Y = Household spending",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-159",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Public Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Stimulus payment was (or could have been) chosen, what information was available, and when was Household spending realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Stimulus payment is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  },
  {
    "id": "T3-BucketJ-160",
    "bucket": "BucketLarge-J",
    "title": "Election Outcome Counterfactual",
    "scenario": "A close election; analysts ask what would have happened if turnout rules differed.\nYou observe the realized outcome Y₁ under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y₀: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y₀ is fundamentally unobserved for the same unit at the same time.",
    "variables": [
      "X = Turnout rule change",
      "Y = Election winner",
      "Y1 = observed outcome under actual world",
      "Y0 = counterfactual outcome under alternative world",
      "U = unobserved factors affecting both X and Y"
    ],
    "annotations": {
      "Case ID": "J2-160",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Feedback Loops",
      "Trap Subtype": "Dynamic World Divergence",
      "Difficulty": "Hard",
      "Subdomain": "Public Policy",
      "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
      "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "State the observed outcome (Y₁) and define precisely the counterfactual outcome (Y₀) for this case.\nWhy is Y₀ unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
    "expected_analysis": "Explicitly articulate Y₁ vs Y₀ for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
    "Hidden Timestamp": "At the decision time when Turnout rule change was (or could have been) chosen, what information was available, and when was Election winner realized relative to that choice?",
    "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Turnout rule change is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
  }
]