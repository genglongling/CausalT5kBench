[
  {
    "id": "T3-BucketJ-161",
    "bucket": "BucketLarge-J",
    "title": "Scholarship Acceptance Rates by Prep Program",
    "scenario": "A nonprofit reports that applicants from Prep Program A are accepted into selective colleges at a higher overall rate than applicants from Prep Program B. Donors conclude Program A is more effective and recommend expanding it.\n\nA counselor points out that when acceptance rates are broken down by family income bracket (low-income vs higher-income), Program B has a higher acceptance rate in both brackets. Program A has more higher-income participants overall, while Program B serves mostly low-income participants.",
    "variables": [
      "X = Prep program (A vs. B)",
      "Y = College acceptance rate",
      "Z = Family income bracket (low vs. higher)"
    ],
    "annotations": {
      "Case ID": "J2-161",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Aggregation Bias",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z affects Y and differs in distribution across X; aggregating across Z reverses the subgroup acceptance-rate comparison.",
      "Key Insight": "Aggregate success rates can reverse within-group trends when group composition differs.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the higher overall acceptance rate for Program A imply it is more effective than Program B?\nWhat happens to the comparison if you condition on family income bracket?",
    "expected_analysis": "Identify Simpson’s Paradox: the aggregate difference is driven by different income composition across programs.\nIncome bracket (Z) predicts acceptance (Y) and differs across programs (X), so the overall rate is not a fair comparison.\nWithin each income bracket, Program B performs better, suggesting Program A’s advantage is compositional.\nConclusion: The claim “Program A is more effective” is INVALID based on the aggregate statistic alone.",
    "Hidden Timestamp": "Was family income bracket known before students enrolled in Prep Program A vs. B, making it a true pre-enrollment stratifier?",
    "Conditional Answers": "Answer if you only use overall acceptance rates: You may wrongly prefer Program A because it serves more higher-income students.\nAnswer if you stratify or standardize by income bracket: Compare within strata (or compute standardized rates) to evaluate program performance.\nAnswer if assignment to programs can be randomized within income strata: Then within-stratum differences can support a causal claim about program effectiveness.",
    "Wise Refusal": "I can’t determine which program is causally better without baseline comparability (prior grades, support) and a credible assignment mechanism."
  },
  {
    "id": "T3-BucketJ-162",
    "bucket": "BucketLarge-J",
    "title": "Two Banks and the Default Rate",
    "scenario": "A city report states that Bank X has a lower overall loan default rate than Bank Y, and officials argue Bank X has better underwriting practices.\n\nWhen analysts stratify borrowers by credit-risk tier (prime vs subprime), Bank Y has a lower default rate in both tiers. The reversal occurs because Bank X approves a larger share of prime borrowers, while Bank Y serves more subprime borrowers overall.",
    "variables": [
      "X = Bank (X vs. Y)",
      "Y = Loan default rate",
      "Z = Credit-risk tier (prime vs. subprime)"
    ],
    "annotations": {
      "Case ID": "J2-162",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Imbalanced Group Composition",
      "Difficulty": "Medium",
      "Subdomain": "Finance",
      "Causal Structure": "Z strongly affects Y and the distribution of Z differs by X; aggregate defaults reverse the within-tier comparison.",
      "Key Insight": "Comparing institutions using aggregate outcomes is misleading when they serve different mixes of clients.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does Bank X’s lower overall default rate prove it underwrites better than Bank Y?\nWhy can the conclusion reverse after stratifying by credit-risk tier?",
    "expected_analysis": "This is Simpson’s Paradox caused by imbalanced risk composition.\nRisk tier (Z) drives default (Y). Banks differ in borrower mix, so overall default rates conflate underwriting with portfolio composition.\nWithin both tiers, Bank Y performs better.\nConclusion: The claim “Bank X has better underwriting” is INVALID from the aggregate comparison alone.",
    "Hidden Timestamp": "Is credit-risk tier assessed before borrowers choose a bank, making Z pre-treatment with respect to X?",
    "Conditional Answers": "Answer if you compare only overall defaults: You may incorrectly attribute Bank X’s lower defaults to better underwriting.\nAnswer if you compare within risk tiers or compute standardized defaults: Prefer the bank with lower within-tier defaults (and report a standardized overall rate).\nAnswer if approval decisions themselves change the portfolio mix: Separate “mix effects” from “within-tier performance.”",
    "Wise Refusal": "I can’t judge underwriting quality without consistent risk-tier measurement and a standardized comparison across the same risk distribution."
  },
  {
    "id": "T3-BucketJ-163",
    "bucket": "BucketLarge-J",
    "title": "Clinic Readmissions by Age Mix",
    "scenario": "A hospital network claims Clinic A provides better follow-up care because its overall 30-day readmission rate is lower than Clinic B’s.\n\nWhen readmissions are broken down by patient age group (under 65 vs 65+), Clinic B has lower readmission rates in both groups. Clinic A treats a much larger share of younger patients, while Clinic B treats more older patients overall.",
    "variables": [
      "X = Clinic (A vs. B)",
      "Y = 30-day readmission rate",
      "Z = Age group (under 65 vs. 65+)"
    ],
    "annotations": {
      "Case ID": "J2-163",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Aggregation Bias",
      "Difficulty": "Medium",
      "Subdomain": "Healthcare Administration",
      "Causal Structure": "Z affects Y and differs by clinic; aggregating across Z reverses the subgroup comparison.",
      "Key Insight": "Overall metrics can hide within-group differences driven by demographic mix.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does Clinic A’s lower overall readmission rate imply better follow-up care?\nWhat does the comparison show within each age group?",
    "expected_analysis": "Simpson’s Paradox: age group (Z) strongly affects readmission (Y) and is uneven across clinics (X).\nClinic A’s overall advantage can be explained by treating more younger patients.\nWithin both age groups, Clinic B performs better.\nConclusion: The claim “Clinic A has better follow-up care” is INVALID from the overall rate alone.",
    "Hidden Timestamp": "Was age determined before clinic selection (a true pre-treatment stratifier), or does clinic choice affect which ages are seen?",
    "Conditional Answers": "Answer if you only look at overall readmissions: You might reward Clinic A incorrectly.\nAnswer if you age-standardize or compare within age strata: The within-stratum comparison is more informative for care quality.\nAnswer if clinics differ in referral patterns: Adjust for case mix and referral selection before drawing conclusions.",
    "Wise Refusal": "I can’t attribute readmission differences to care quality without case-mix adjustment and information on referral/triage processes."
  },
  {
    "id": "T3-BucketJ-164",
    "bucket": "BucketLarge-J",
    "title": "The Employee Satisfaction Survey",
    "scenario": "A company emails an anonymous satisfaction survey and reports that “85% of employees are satisfied.” Leadership concludes morale is high.\n\nOnly 30% of employees responded. Dissatisfied employees may be less likely to respond because they think nothing will change or fear being identified.",
    "variables": [
      "X = Survey response (respond vs. not)",
      "Y = Reported satisfaction",
      "Z = Dissatisfaction / fear of retaliation"
    ],
    "annotations": {
      "Case ID": "J2-164",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Easy",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Z affects response and satisfaction; conditioning on responders yields biased satisfaction estimate.",
      "Key Insight": "Survey results can be unrepresentative when response is selective.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does 85% satisfaction among respondents represent satisfaction among all employees?\nWhat selection mechanism could bias the observed satisfaction rate?",
    "expected_analysis": "Selection bias via sampling-on-the-outcome: only responders are observed.\nIf dissatisfied employees respond less, the observed satisfaction rate overestimates true satisfaction.\nConclusion: The claim “morale is high company-wide” is INVALID from respondent-only data.",
    "Hidden Timestamp": "Did dissatisfaction exist before employees chose whether to respond, or did the survey context change willingness to respond?",
    "Conditional Answers": "Answer if response is random: Then the respondent rate estimates overall morale.\nAnswer if dissatisfied employees avoid responding: Then results are biased upward; improve sampling/response incentives.\nAnswer if response differs by team or manager: Use stratified follow-ups to assess representation.",
    "Wise Refusal": "I can’t infer company-wide morale without response-rate patterns and information about non-respondents."
  },
  {
    "id": "T3-BucketJ-165",
    "bucket": "BucketLarge-J",
    "title": "Studying Only Award-Winning Films",
    "scenario": "A blog analyzes “what makes a film profitable” by looking only at movies that won major awards. The author finds that award-winning films have high budgets and concludes that increasing budgets causes higher profits.\n\nThe dataset excludes many high-budget films that were not nominated or that failed commercially. Award status is a filter for inclusion.",
    "variables": [
      "X = Production budget",
      "Y = Profit",
      "Z = Award nomination/win status"
    ],
    "annotations": {
      "Case ID": "J2-165",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Medium",
      "Subdomain": "Media Economics",
      "Causal Structure": "Conditioning on Z (award inclusion) selects a non-representative subset; X–Y relationship differs from full population.",
      "Key Insight": "Analyzing only visible successes distorts relationships between inputs and outcomes.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the budget–profit pattern among award winners imply bigger budgets cause profits?\nWhy does restricting to award winners bias the analysis?",
    "expected_analysis": "Selection bias: conditioning on award status creates a selected sample.\nAwards correlate with many factors and exclude failures, so the remaining sample is not representative.\nConclusion: The claim “bigger budgets cause higher profits” is INVALID from award-winner-only data.",
    "Hidden Timestamp": "Is award status determined before profits are realized, or is the dataset effectively filtered after outcomes are known?",
    "Conditional Answers": "Answer if awards are unrelated to profit (unlikely): Restriction would be less problematic.\nAnswer if awards filter on quality/visibility correlated with profit: Include all films to avoid selection bias.\nAnswer if you can model nomination probability: Any correction remains assumption-dependent.",
    "Wise Refusal": "I can’t draw a causal conclusion without data on non-award films and a clear sampling frame that includes both successes and failures."
  },
  {
    "id": "T3-BucketJ-166",
    "bucket": "BucketLarge-J",
    "title": "Dropout Bias in a Workforce Program",
    "scenario": "A city runs a six-month workforce training program and reports that participants’ average wages increased by 20%. Officials conclude the program boosted earnings.\n\nHowever, 40% of participants dropped out and are missing from the final wage measurement. The report analyzes only participants who completed the program.",
    "variables": [
      "X = Training program enrollment",
      "Y = Post-program wage",
      "Z = Program completion (complete vs. dropout)"
    ],
    "annotations": {
      "Case ID": "J2-166",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Attrition Bias",
      "Difficulty": "Medium",
      "Subdomain": "Labor Policy",
      "Causal Structure": "Completion is selected post-enrollment; completers differ from dropouts in factors affecting Y.",
      "Key Insight": "Attrition can make observed improvements unrepresentative of the full enrolled group.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the 20% wage increase among completers apply to all enrollees?\nHow can attrition bias the reported outcome?",
    "expected_analysis": "Attrition bias: outcomes are observed only for completers.\nIf dropouts would have had lower gains, analyzing only completers overstates average improvement.\nConclusion: The claim “participants gained 20%” is INVALID without accounting for missing outcomes.",
    "Hidden Timestamp": "Did dropout occur after participants could have benefited (or been harmed) by the program, making completion a post-treatment selection?",
    "Conditional Answers": "Answer if dropout is random: Completer outcomes may approximate all enrollees.\nAnswer if dropout is related to constraints or low baseline readiness: Use intent-to-treat or obtain administrative wage data for dropouts.\nAnswer if dropouts can be followed up: Measuring outcomes for all reduces attrition bias.",
    "Wise Refusal": "I can’t estimate overall impact without outcomes (or credible imputations) for dropouts and evidence about why participants left."
  },
  {
    "id": "T3-BucketJ-167",
    "bucket": "BucketLarge-J",
    "title": "Tutoring and Test Scores in Two Neighborhoods",
    "scenario": "A school district observes that students who attend private tutoring have higher math scores than students who do not. Administrators conclude tutoring causes higher scores.\n\nStudents who get tutoring are more likely to come from higher-income families with more educational resources at home.",
    "variables": [
      "X = Tutoring attendance",
      "Y = Math test score",
      "Z = Socioeconomic resources"
    ],
    "annotations": {
      "Case ID": "J2-167",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Socioeconomic",
      "Difficulty": "Easy",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z influences both X and Y, creating a spurious X–Y association.",
      "Key Insight": "A common cause can make correlation look like causation.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the association between tutoring and scores prove tutoring causes gains?\nWhat socioeconomic confounders could explain both tutoring and higher scores?",
    "expected_analysis": "Socioeconomic confounding: resources (Z) affect tutoring access (X) and test performance (Y).\nWithout controlling for Z or using a causal design, the tutoring–score gap is not a causal effect.\nConclusion: The claim “tutoring causes higher scores” is INVALID from the observational association alone.",
    "Hidden Timestamp": "Were family resources present before tutoring began, making them a baseline confounder?",
    "Conditional Answers": "Answer if tutoring were randomly assigned: Score differences could be attributed to tutoring.\nAnswer if tutoring is purchased mostly by higher-resource families: Adjust for baseline resources and prior scores; residual confounding may remain.\nAnswer if comparing within the same school and income bracket: Confounding is reduced but not eliminated.",
    "Wise Refusal": "I can’t make a causal claim without baseline scores, resource measures, and a credible strategy to compare similar students."
  },
  {
    "id": "T3-BucketJ-168",
    "bucket": "BucketLarge-J",
    "title": "Ice Cream Sales and Park Injuries",
    "scenario": "A city analyst finds that days with higher ice cream sales in parks also have more reported minor injuries. They argue ice cream vendors create unsafe conditions.\n\nA parks manager suggests that hot weather increases both park attendance (creating more injury opportunities) and ice cream sales.",
    "variables": [
      "X = Ice cream sales",
      "Y = Park injury reports",
      "Z = Temperature / attendance"
    ],
    "annotations": {
      "Case ID": "J2-168",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Omitted Variable",
      "Difficulty": "Easy",
      "Subdomain": "Urban Policy",
      "Causal Structure": "Z increases both X and Y; omitting Z yields misleading correlation.",
      "Key Insight": "An omitted common cause can create a misleading association.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the correlation between ice cream sales and injuries imply vendors cause injuries?\nWhat omitted variable could drive both ice cream sales and injuries?",
    "expected_analysis": "Omitted-variable confounding: temperature/attendance (Z) increases both sales (X) and injuries (Y).\nCorrelation can appear even if vendors do not affect injuries.\nConclusion: The claim “ice cream sales cause injuries” is INVALID from the raw correlation.",
    "Hidden Timestamp": "Did temperature rise before both ice cream sales and injuries, suggesting a shared upstream cause?",
    "Conditional Answers": "Answer if you control for temperature and attendance: The sales–injury correlation may disappear.\nAnswer if injuries rise on high-attendance days regardless of vendors: Vendor restriction is not justified by this correlation.\nAnswer if vendor placement is randomized across similar days: Comparisons become more informative.",
    "Wise Refusal": "I can’t conclude causality without controlling for weather/attendance and knowing how vendor permits are assigned."
  },
  {
    "id": "T3-BucketJ-169",
    "bucket": "BucketLarge-J",
    "title": "Stronger Painkillers and Mortality",
    "scenario": "A hospital audit reports that patients prescribed a stronger painkiller have higher 30-day mortality than patients prescribed a milder painkiller. A supervisor argues the stronger drug is dangerous.\n\nClinicians respond that the stronger painkiller is typically prescribed to patients with more severe conditions, who are already at higher risk of death.",
    "variables": [
      "X = Painkiller prescribed (strong vs. mild)",
      "Y = 30-day mortality",
      "Z = Underlying illness severity"
    ],
    "annotations": {
      "Case ID": "J2-169",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Confounding by Indication",
      "Difficulty": "Medium",
      "Subdomain": "Healthcare Administration",
      "Causal Structure": "Severity (Z) influences both treatment choice (X) and mortality (Y).",
      "Key Insight": "Sicker patients are more likely to receive stronger treatments, confounding naive comparisons.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does higher mortality among patients receiving the stronger painkiller imply the drug causes deaths?\nHow does confounding by indication distort the comparison?",
    "expected_analysis": "Confounding by indication: severity (Z) drives prescribing (X) and mortality risk (Y).\nThe observed association may reflect baseline severity rather than a harmful drug effect.\nConclusion: The claim “the stronger drug increases mortality” is INVALID from this observational comparison alone.",
    "Hidden Timestamp": "Was severity assessed before the prescription decision, and is it measured well enough to adjust for it?",
    "Conditional Answers": "Answer if patients of similar severity are compared: The estimate is less confounded.\nAnswer if severity is poorly measured: Residual confounding remains even after adjustment.\nAnswer if prescribing follows a protocol unrelated to individual severity: Comparisons are closer to causal.",
    "Wise Refusal": "I can’t infer a causal drug effect without detailed severity/comorbidity measures and timing of prescription relative to deterioration."
  },
  {
    "id": "T3-BucketJ-170",
    "bucket": "BucketLarge-J",
    "title": "Telemedicine and Missed Follow-ups",
    "scenario": "A health system introduced telemedicine visits in some clinics while others stayed in-person only. A summary report claims telemedicine reduced follow-up adherence because telemedicine clinics show a lower overall rate of patients completing a recommended follow-up within 30 days.\n\nThe evaluation compares 30-day follow-up completion rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline chronic-disease burden of the clinic’s patient panel (high vs. low), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = 30-day follow-up completion rate",
      "Z = baseline chronic-disease burden of the clinic’s patient panel (high vs. low)"
    ],
    "annotations": {
      "Case ID": "J2-170",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will 30-day follow-up completion rate improve?\nWhy does conditioning on baseline chronic-disease burden of the clinic’s patient panel (high vs. low) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was baseline chronic-disease burden of the clinic’s patient panel (high vs. low) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline chronic-disease burden of the clinic’s patient panel (high vs. low) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline chronic-disease burden of the clinic’s patient panel (high vs. low) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-171",
    "bucket": "BucketLarge-J",
    "title": "After-School Tutoring App Results",
    "scenario": "A district piloted an after-school tutoring app in some middle schools. The district claims the app lowered math performance because app schools have lower average end-of-year math scores than non-app schools.\n\nThe evaluation compares average end-of-year math score in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline student achievement level (higher vs. lower prior-year scores), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = average end-of-year math score",
      "Z = baseline student achievement level (higher vs. lower prior-year scores)"
    ],
    "annotations": {
      "Case ID": "J2-171",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will average end-of-year math score improve?\nWhy does conditioning on baseline student achievement level (higher vs. lower prior-year scores) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was baseline student achievement level (higher vs. lower prior-year scores) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline student achievement level (higher vs. lower prior-year scores) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline student achievement level (higher vs. lower prior-year scores) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-172",
    "bucket": "BucketLarge-J",
    "title": "Body Cameras and Citizen Complaints",
    "scenario": "A police department deployed body cameras in some precincts first. City leaders claim body cameras increased misconduct because camera precincts show higher overall citizen complaint rates than non-camera precincts during the evaluation period.\n\nThe evaluation compares citizen complaint rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline complaint environment of the precinct (historically high vs. low complaint rate), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = citizen complaint rate",
      "Z = baseline complaint environment of the precinct (historically high vs. low complaint rate)"
    ],
    "annotations": {
      "Case ID": "J2-172",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will citizen complaint rate improve?\nWhy does conditioning on baseline complaint environment of the precinct (historically high vs. low complaint rate) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was baseline complaint environment of the precinct (historically high vs. low complaint rate) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline complaint environment of the precinct (historically high vs. low complaint rate) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline complaint environment of the precinct (historically high vs. low complaint rate) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-173",
    "bucket": "BucketLarge-J",
    "title": "Transit Pass Subsidy and Commute Times",
    "scenario": "A city offered subsidized monthly transit passes at certain large worksites. A memo claims the subsidy increased commute times because subsidized sites show longer average commutes than non-subsidized sites.\n\nThe evaluation compares average door-to-door commute time in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by home-to-work distance category (short vs. long), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = average door-to-door commute time",
      "Z = home-to-work distance category (short vs. long)"
    ],
    "annotations": {
      "Case ID": "J2-173",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Urban Planning",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will average door-to-door commute time improve?\nWhy does conditioning on home-to-work distance category (short vs. long) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was home-to-work distance category (short vs. long) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each home-to-work distance category (short vs. long) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in home-to-work distance category (short vs. long) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-174",
    "bucket": "BucketLarge-J",
    "title": "Chatbot Support and Customer Satisfaction",
    "scenario": "A company added an automated chatbot to handle customer support for some product lines. Executives claim the chatbot reduced satisfaction because chatbot product lines have lower overall satisfaction scores than product lines without the chatbot.\n\nThe evaluation compares customer satisfaction score in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by issue complexity level (simple vs. complex tickets), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = customer satisfaction score",
      "Z = issue complexity level (simple vs. complex tickets)"
    ],
    "annotations": {
      "Case ID": "J2-174",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Business Operations",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will customer satisfaction score improve?\nWhy does conditioning on issue complexity level (simple vs. complex tickets) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was issue complexity level (simple vs. complex tickets) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each issue complexity level (simple vs. complex tickets) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in issue complexity level (simple vs. complex tickets) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-175",
    "bucket": "BucketLarge-J",
    "title": "Remote Work Policy and Output",
    "scenario": "A firm allowed remote work in certain teams first. Leadership claims remote work reduced productivity because remote-eligible teams show lower overall weekly output than teams that remained on-site.\n\nThe evaluation compares weekly output per employee in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by role type (individual contributor vs. people manager), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = weekly output per employee",
      "Z = role type (individual contributor vs. people manager)"
    ],
    "annotations": {
      "Case ID": "J2-175",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will weekly output per employee improve?\nWhy does conditioning on role type (individual contributor vs. people manager) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was role type (individual contributor vs. people manager) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each role type (individual contributor vs. people manager) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in role type (individual contributor vs. people manager) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-176",
    "bucket": "BucketLarge-J",
    "title": "Carbon Fee Pilot and Emissions",
    "scenario": "A region piloted a carbon fee in some municipalities. A headline article claims the carbon fee increased emissions because fee municipalities show higher overall per-capita emissions than non-fee municipalities after the pilot begins.\n\nThe evaluation compares per-capita CO₂ emissions in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by industrial intensity of the municipality (high vs. low share of heavy industry), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = per-capita CO₂ emissions",
      "Z = industrial intensity of the municipality (high vs. low share of heavy industry)"
    ],
    "annotations": {
      "Case ID": "J2-176",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Environmental Policy",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will per-capita CO₂ emissions improve?\nWhy does conditioning on industrial intensity of the municipality (high vs. low share of heavy industry) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was industrial intensity of the municipality (high vs. low share of heavy industry) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each industrial intensity of the municipality (high vs. low share of heavy industry) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in industrial intensity of the municipality (high vs. low share of heavy industry) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-177",
    "bucket": "BucketLarge-J",
    "title": "Pass/Fail Option and Course Completion",
    "scenario": "A university introduced an optional pass/fail grading policy in some gateway courses. An internal report claims pass/fail reduced completion because pass/fail courses have a lower overall completion rate than comparable graded courses.\n\nThe evaluation compares course completion rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by course difficulty tier (hard vs. moderate), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = course completion rate",
      "Z = course difficulty tier (hard vs. moderate)"
    ],
    "annotations": {
      "Case ID": "J2-177",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Higher Education",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will course completion rate improve?\nWhy does conditioning on course difficulty tier (hard vs. moderate) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was course difficulty tier (hard vs. moderate) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each course difficulty tier (hard vs. moderate) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in course difficulty tier (hard vs. moderate) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-178",
    "bucket": "BucketLarge-J",
    "title": "Microgrants and Small Business Survival",
    "scenario": "A city offered emergency microgrants to small businesses in selected corridors. A press release claims the grants did not help because grant-recipient corridors show lower overall one-year business survival than corridors without grants.\n\nThe evaluation compares one-year business survival rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline business fragility (low vs. high pre-grant revenue volatility), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
    "variables": [
      "X = Intervention adoption (yes vs. no)",
      "Y = one-year business survival rate",
      "Z = baseline business fragility (low vs. high pre-grant revenue volatility)"
    ],
    "annotations": {
      "Case ID": "J2-178",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Local Economic Development",
      "Causal Structure": "Z → Y and rollout implies P(Z|X=1) ≠ P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
      "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we expand the intervention, will one-year business survival rate improve?\nWhy does conditioning on baseline business fragility (low vs. high pre-grant revenue volatility) reverse the aggregate conclusion?",
    "expected_analysis": "This is Simpson’s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
    "Hidden Timestamp": "Was baseline business fragility (low vs. high pre-grant revenue volatility) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
    "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline business fragility (low vs. high pre-grant revenue volatility) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
    "Wise Refusal": "I can’t estimate the intervention’s effect without knowing the rollout rule and verifying overlap in baseline business fragility (low vs. high pre-grant revenue volatility) between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-179",
    "bucket": "BucketLarge-J",
    "title": "Minimum Wage Policy and Restaurant Closures",
    "scenario": "A city raises the minimum wage and later observes a higher rate of restaurant closures than in neighboring cities that did not raise wages. Commentators claim the wage increase caused closures.\n\nThe city that raised wages was already experiencing rapidly rising commercial rents and a decline in foot traffic due to major construction, both of which affect closure risk and also influenced the political push for wage reform.",
    "variables": [
      "X = minimum wage increase (raised vs. not)",
      "Y = restaurant closure rate",
      "Z = commercial rent pressure and foot traffic trends"
    ],
    "annotations": {
      "Case ID": "J2-179",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Hard",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Z → X and Z → Y; without blocking Z, X–Y comparison is confounded.",
      "Key Insight": "Policy adoption can be correlated with underlying economic pressures that also affect outcomes.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement minimum wage increase (raised vs. not), will restaurant closure rate change?\nWhy might units receiving minimum wage increase (raised vs. not) differ from units not receiving it due to commercial rent pressure and foot traffic trends?",
    "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was commercial rent pressure and foot traffic trends measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of minimum wage increase (raised vs. not) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if minimum wage increase (raised vs. not) is targeted to units with different baseline commercial rent pressure and foot traffic trends: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for minimum wage increase (raised vs. not) and credible measurement of commercial rent pressure and foot traffic trends (and other confounders)."
  },
  {
    "id": "T3-BucketJ-180",
    "bucket": "BucketLarge-J",
    "title": "New Reading Curriculum and Test Scores",
    "scenario": "A district adopts a new reading curriculum in schools flagged as “at risk.” After one year, adopting schools have lower reading scores than non-adopting schools, and critics claim the curriculum harmed learning.\n\nAdoption was prioritized for schools with declining prior scores and higher poverty rates—factors that also predict future scores regardless of curriculum.",
    "variables": [
      "X = new reading curriculum adoption (yes vs. no)",
      "Y = reading test scores",
      "Z = baseline performance trend and student poverty rate"
    ],
    "annotations": {
      "Case ID": "J2-180",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z influences both adoption X and outcomes Y; treated schools start on different trajectories.",
      "Key Insight": "Targeted interventions create treated groups that differ systematically from controls.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement new reading curriculum adoption (yes vs. no), will reading test scores change?\nWhy might units receiving new reading curriculum adoption (yes vs. no) differ from units not receiving it due to baseline performance trend and student poverty rate?",
    "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was baseline performance trend and student poverty rate measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of new reading curriculum adoption (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if new reading curriculum adoption (yes vs. no) is targeted to units with different baseline baseline performance trend and student poverty rate: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for new reading curriculum adoption (yes vs. no) and credible measurement of baseline performance trend and student poverty rate (and other confounders)."
  },
  {
    "id": "T3-BucketJ-181",
    "bucket": "BucketLarge-J",
    "title": "Community Policing Grants and Crime",
    "scenario": "A federal grant funds community policing in selected neighborhoods. A year later, funded neighborhoods show higher reported crime than unfunded neighborhoods, leading to claims that the grants increased crime.\n\nGrant selection prioritized neighborhoods with historically high crime and recent upward trends, which also predict future crime.",
    "variables": [
      "X = community policing grant funding (yes vs. no)",
      "Y = reported crime rate",
      "Z = baseline crime level and pre-grant trend"
    ],
    "annotations": {
      "Case ID": "J2-181",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Z → X and Z → Y; selecting on need confounds causal interpretation.",
      "Key Insight": "Comparing funded vs. unfunded sites confounds intervention with baseline risk.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement community policing grant funding (yes vs. no), will reported crime rate change?\nWhy might units receiving community policing grant funding (yes vs. no) differ from units not receiving it due to baseline crime level and pre-grant trend?",
    "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was baseline crime level and pre-grant trend measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of community policing grant funding (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if community policing grant funding (yes vs. no) is targeted to units with different baseline baseline crime level and pre-grant trend: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for community policing grant funding (yes vs. no) and credible measurement of baseline crime level and pre-grant trend (and other confounders)."
  },
  {
    "id": "T3-BucketJ-182",
    "bucket": "BucketLarge-J",
    "title": "Nutrition Labels and Snack Purchases",
    "scenario": "A grocery chain introduces front-of-package nutrition labels in some stores. Purchases of sugary snacks are higher in labeled stores, and an executive claims labels backfired.\n\nThe chain piloted labels first in dense urban stores with distinct customer baskets and higher baseline snack purchases.",
    "variables": [
      "X = nutrition label rollout (store labeled vs. not)",
      "Y = purchases of sugary snacks",
      "Z = store neighborhood type and baseline basket patterns"
    ],
    "annotations": {
      "Case ID": "J2-182",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Medium",
      "Subdomain": "Behavioral Economics",
      "Causal Structure": "Z affects store selection and Y; naive comparison attributes Z effects to labeling.",
      "Key Insight": "Rollout choices can confound estimated behavioral impacts.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement nutrition label rollout (store labeled vs. not), will purchases of sugary snacks change?\nWhy might units receiving nutrition label rollout (store labeled vs. not) differ from units not receiving it due to store neighborhood type and baseline basket patterns?",
    "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was store neighborhood type and baseline basket patterns measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of nutrition label rollout (store labeled vs. not) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if nutrition label rollout (store labeled vs. not) is targeted to units with different baseline store neighborhood type and baseline basket patterns: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for nutrition label rollout (store labeled vs. not) and credible measurement of store neighborhood type and baseline basket patterns (and other confounders)."
  },
  {
    "id": "T3-BucketJ-183",
    "bucket": "BucketLarge-J",
    "title": "Hybrid Work Stipend and Turnover",
    "scenario": "A company offers a hybrid-work stipend to certain teams and later observes higher turnover in stipend teams. Management claims hybrid work drives attrition.\n\nThe stipend was offered first to teams undergoing reorganization and leadership turnover, which also increases attrition risk.",
    "variables": [
      "X = hybrid-work stipend offered (yes vs. no)",
      "Y = employee turnover rate",
      "Z = team reorganization/leadership instability"
    ],
    "annotations": {
      "Case ID": "J2-183",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Z → X and Z → Y; treated teams face other shocks that raise turnover.",
      "Key Insight": "Interventions often coincide with organizational changes that also affect outcomes.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement hybrid-work stipend offered (yes vs. no), will employee turnover rate change?\nWhy might units receiving hybrid-work stipend offered (yes vs. no) differ from units not receiving it due to team reorganization/leadership instability?",
    "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was team reorganization/leadership instability measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of hybrid-work stipend offered (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if hybrid-work stipend offered (yes vs. no) is targeted to units with different baseline team reorganization/leadership instability: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for hybrid-work stipend offered (yes vs. no) and credible measurement of team reorganization/leadership instability (and other confounders)."
  },
  {
    "id": "T3-BucketJ-184",
    "bucket": "BucketLarge-J",
    "title": "Bike Lane Expansion and Retail Sales",
    "scenario": "A city expands protected bike lanes on selected commercial corridors and later sees lower retail sales on those corridors than on others. Critics claim bike lanes hurt businesses.\n\nBike lanes were prioritized for corridors already facing construction disruption and declining sales trends, which also predict future sales.",
    "variables": [
      "X = protected bike lane expansion (yes vs. no)",
      "Y = retail sales",
      "Z = baseline construction disruption and pre-policy sales trend"
    ],
    "annotations": {
      "Case ID": "J2-184",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Hard",
      "Subdomain": "Urban Policy",
      "Causal Structure": "Z influences both which corridors get X and future Y.",
      "Key Insight": "Targeting infrastructure projects to struggling corridors confounds causal claims about sales impacts.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement protected bike lane expansion (yes vs. no), will retail sales change?\nWhy might units receiving protected bike lane expansion (yes vs. no) differ from units not receiving it due to baseline construction disruption and pre-policy sales trend?",
    "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was baseline construction disruption and pre-policy sales trend measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of protected bike lane expansion (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if protected bike lane expansion (yes vs. no) is targeted to units with different baseline baseline construction disruption and pre-policy sales trend: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for protected bike lane expansion (yes vs. no) and credible measurement of baseline construction disruption and pre-policy sales trend (and other confounders)."
  },
  {
    "id": "T3-BucketJ-185",
    "bucket": "BucketLarge-J",
    "title": "Adaptive Tutoring Hours and Learning Gains",
    "scenario": "A district uses an adaptive policy: students who score poorly on weekly quizzes are assigned additional tutoring hours the following week. Analysts find that students with more tutoring hours have smaller end-of-term learning gains and argue tutoring is ineffective.\n\nWeekly quiz scores (past outcomes) influence future tutoring assignment and also predict end-of-term gains.",
    "variables": [
      "X = assigned tutoring hours (time-varying)",
      "Y = end-of-term learning gains",
      "Z = weekly quiz score history (past performance)"
    ],
    "annotations": {
      "Case ID": "J2-185",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Time-varying Confounding",
      "Difficulty": "Hard",
      "Subdomain": "Education Policy",
      "Causal Structure": "Past Y → future X and past Y → future Y; time-varying assignment creates confounding.",
      "Key Insight": "When treatment responds to outcomes, naive correlations can reverse the true effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement assigned tutoring hours (time-varying), will end-of-term learning gains change?\nWhy might units receiving assigned tutoring hours (time-varying) differ from units not receiving it due to weekly quiz score history (past performance)?",
    "expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was weekly quiz score history (past performance) measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of assigned tutoring hours (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if assigned tutoring hours (time-varying) is targeted to units with different baseline weekly quiz score history (past performance): The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for assigned tutoring hours (time-varying) and credible measurement of weekly quiz score history (past performance) (and other confounders)."
  },
  {
    "id": "T3-BucketJ-186",
    "bucket": "BucketLarge-J",
    "title": "Outreach Calls and Vaccination Uptake",
    "scenario": "A clinic increases outreach call frequency for patients who have not scheduled vaccinations. Later, patients who received more calls have lower vaccination rates, and a manager claims calls deter people.\n\nCall frequency rises when patients remain unvaccinated over time (past outcome), and those patients are harder to reach and less likely to vaccinate.",
    "variables": [
      "X = outreach call intensity (time-varying)",
      "Y = vaccination uptake",
      "Z = prior vaccination status / past non-response"
    ],
    "annotations": {
      "Case ID": "J2-186",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Time-varying Confounding",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "Past Y drives future X; past Y also predicts future Y.",
      "Key Insight": "Reactive intensification of an intervention creates time-varying confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement outreach call intensity (time-varying), will vaccination uptake change?\nWhy might units receiving outreach call intensity (time-varying) differ from units not receiving it due to prior vaccination status / past non-response?",
    "expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was prior vaccination status / past non-response measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of outreach call intensity (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if outreach call intensity (time-varying) is targeted to units with different baseline prior vaccination status / past non-response: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for outreach call intensity (time-varying) and credible measurement of prior vaccination status / past non-response (and other confounders)."
  },
  {
    "id": "T3-BucketJ-187",
    "bucket": "BucketLarge-J",
    "title": "Surge Warnings and Ride Cancellations",
    "scenario": "A ride-share platform displays “high-demand pricing” warnings more often on routes where cancellations are rising. Analysts observe that rides with warnings have higher cancellation rates and claim warnings cause cancellations.\n\nWarnings are triggered in response to congestion and earlier cancellation surges, which also predict future cancellations.",
    "variables": [
      "X = displaying a surge-pricing warning (time-varying)",
      "Y = ride cancellation rate",
      "Z = recent congestion and cancellation history"
    ],
    "annotations": {
      "Case ID": "J2-187",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Time-varying Confounding",
      "Difficulty": "Hard",
      "Subdomain": "Platform Policy",
      "Causal Structure": "Past demand conditions influence X and Y; time-varying confounding biases naive estimates.",
      "Key Insight": "When interventions respond to worsening conditions, effects can be misattributed.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we implement displaying a surge-pricing warning (time-varying), will ride cancellation rate change?\nWhy might units receiving displaying a surge-pricing warning (time-varying) differ from units not receiving it due to recent congestion and cancellation history?",
    "expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
    "Hidden Timestamp": "Was recent congestion and cancellation history measured before the intervention decision, and did it influence both adoption and outcomes?",
    "Conditional Answers": "Answer if assignment of displaying a surge-pricing warning (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if displaying a surge-pricing warning (time-varying) is targeted to units with different baseline recent congestion and cancellation history: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
    "Wise Refusal": "I can’t infer the causal effect without a clear assignment rule for displaying a surge-pricing warning (time-varying) and credible measurement of recent congestion and cancellation history (and other confounders)."
  },
  {
    "id": "T3-BucketJ-188",
    "bucket": "BucketLarge-J",
    "title": "Wellness Program and Blood Pressure",
    "scenario": "A company offers a voluntary wellness program with weekly coaching. The HR report compares blood pressure changes only among employees who attended at least 8 of 10 sessions and concludes the program substantially lowers blood pressure.\n\nEmployees who miss sessions are excluded from the analysis, and attendance is affected by workload, baseline health, and motivation.",
    "variables": [
      "X = wellness program enrollment",
      "Y = blood pressure change",
      "S = high attendance (8+ sessions) (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-188",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Workplace Health",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does wellness program enrollment cause a change in blood pressure change?\nHow can restricting the analysis to high attendance (8+ sessions) bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into high attendance (8+ sessions) occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into high attendance (8+ sessions) is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by high attendance (8+ sessions) and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-189",
    "bucket": "BucketLarge-J",
    "title": "Scholarship Mentoring and Graduation",
    "scenario": "A university pairs scholarship recipients with mentors. The evaluation reports that mentored students graduate at higher rates, but it includes only students who met with their mentor at least once per month.\n\nStudents who miss meetings—often due to jobs or family obligations—are excluded from the graduation analysis.",
    "variables": [
      "X = mentorship program participation",
      "Y = graduation rate",
      "S = regular mentor-meeting compliance (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-189",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Higher Education",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does mentorship program participation cause a change in graduation rate?\nHow can restricting the analysis to regular mentor-meeting compliance bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into regular mentor-meeting compliance occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into regular mentor-meeting compliance is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by regular mentor-meeting compliance and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-190",
    "bucket": "BucketLarge-J",
    "title": "Job Placement Workshops and Employment",
    "scenario": "A workforce agency offers job placement workshops. The agency reports strong employment gains by comparing employment rates only among participants who completed the full workshop series.\n\nParticipants who drop out early are excluded, even though their employment outcomes may differ.",
    "variables": [
      "X = workshop enrollment",
      "Y = employment status after 3 months",
      "S = workshop completion (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-190",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Labor Policy",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does workshop enrollment cause a change in employment status after 3 months?\nHow can restricting the analysis to workshop completion bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into workshop completion occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into workshop completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by workshop completion and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-191",
    "bucket": "BucketLarge-J",
    "title": "Nutrition Benefit Cards and Food Security",
    "scenario": "A city issues nutrition benefit cards. The city reports improved food security by surveying only households that used the card at least once per week.\n\nHouseholds that rarely used the card (due to access barriers or stigma) are excluded from the reported outcomes.",
    "variables": [
      "X = receiving a nutrition benefit card",
      "Y = food security score",
      "S = frequent card usage (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-191",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does receiving a nutrition benefit card cause a change in food security score?\nHow can restricting the analysis to frequent card usage bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into frequent card usage occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into frequent card usage is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by frequent card usage and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-192",
    "bucket": "BucketLarge-J",
    "title": "New Onboarding Modules and Retention",
    "scenario": "A company introduces new onboarding modules. Managers claim the new onboarding reduces early attrition because employees who completed all modules had high 90-day retention.\n\nEmployees who did not complete modules are excluded from the retention calculation.",
    "variables": [
      "X = new onboarding process",
      "Y = 90-day retention",
      "S = completion of onboarding modules (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-192",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does new onboarding process cause a change in 90-day retention?\nHow can restricting the analysis to completion of onboarding modules bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into completion of onboarding modules occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into completion of onboarding modules is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by completion of onboarding modules and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-193",
    "bucket": "BucketLarge-J",
    "title": "Community Garden Events and Cohesion",
    "scenario": "A city funds community gardens. A report claims gardens increase neighborhood cohesion because survey results are positive among residents who attended at least one garden event.\n\nResidents who never attended events are excluded from the survey analysis.",
    "variables": [
      "X = community garden program",
      "Y = neighborhood cohesion index",
      "S = event attendance (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-193",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Urban Policy",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does community garden program cause a change in neighborhood cohesion index?\nHow can restricting the analysis to event attendance bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into event attendance occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into event attendance is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by event attendance and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-194",
    "bucket": "BucketLarge-J",
    "title": "Financial Literacy App and Savings",
    "scenario": "A bank rolls out a financial literacy app. The bank claims the app increases savings because users who completed all lessons increased their savings balances.\n\nThe analysis excludes users who installed the app but did not finish lessons.",
    "variables": [
      "X = financial literacy app rollout",
      "Y = savings balance change",
      "S = lesson completion (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-194",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Behavioral Economics",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does financial literacy app rollout cause a change in savings balance change?\nHow can restricting the analysis to lesson completion bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into lesson completion occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into lesson completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by lesson completion and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-195",
    "bucket": "BucketLarge-J",
    "title": "Bike Helmet Giveaway and Injuries",
    "scenario": "A city gives away free bike helmets at transit hubs. The city reports lower cyclist injuries among those who registered their helmet pickup online, concluding the giveaway reduces injuries.\n\nCyclists who took helmets but did not register are excluded from injury tracking.",
    "variables": [
      "X = helmet giveaway program",
      "Y = cyclist injury rate",
      "S = online registration of helmet pickup (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-195",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Transportation Safety",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does helmet giveaway program cause a change in cyclist injury rate?\nHow can restricting the analysis to online registration of helmet pickup bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into online registration of helmet pickup occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into online registration of helmet pickup is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by online registration of helmet pickup and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-196",
    "bucket": "BucketLarge-J",
    "title": "Teacher Training and Observation Scores",
    "scenario": "A district offers optional teacher training. The district reports improved classroom observation scores among teachers who completed the training and submitted all follow-up reflections.\n\nTeachers who attended but did not submit reflections are excluded from the reported outcomes.",
    "variables": [
      "X = teacher training program",
      "Y = classroom observation score",
      "S = submission of required follow-up reflections (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-196",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does teacher training program cause a change in classroom observation score?\nHow can restricting the analysis to submission of required follow-up reflections bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into submission of required follow-up reflections occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into submission of required follow-up reflections is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by submission of required follow-up reflections and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-197",
    "bucket": "BucketLarge-J",
    "title": "Rent Assistance Documentation and Evictions",
    "scenario": "A county provides rent assistance. The county reports the program prevents evictions by analyzing only households that submitted all required documentation by the deadline.\n\nHouseholds missing paperwork are excluded from eviction outcome statistics.",
    "variables": [
      "X = rent assistance program",
      "Y = eviction rate",
      "S = documentation completion (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-197",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Housing Policy",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does rent assistance program cause a change in eviction rate?\nHow can restricting the analysis to documentation completion bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into documentation completion occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into documentation completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by documentation completion and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-198",
    "bucket": "BucketLarge-J",
    "title": "Fraud Hotline and Case Duration",
    "scenario": "A firm launches an internal fraud-reporting hotline. Leadership claims the hotline speeds up resolutions because hotline cases closed quickly.\n\nThe report includes only cases resolved within the quarter; unresolved cases are excluded.",
    "variables": [
      "X = fraud-reporting hotline introduction",
      "Y = case resolution time",
      "S = cases resolved within the quarter (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-198",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Compliance",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does fraud-reporting hotline introduction cause a change in case resolution time?\nHow can restricting the analysis to cases resolved within the quarter bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into cases resolved within the quarter occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into cases resolved within the quarter is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by cases resolved within the quarter and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-199",
    "bucket": "BucketLarge-J",
    "title": "After-School Sports and Attendance",
    "scenario": "A school offers an after-school sports program to improve attendance. The school reports improved attendance among students who participated in at least 75% of practices.\n\nStudents who enrolled but rarely attended practices are excluded from the attendance comparison.",
    "variables": [
      "X = sports program enrollment",
      "Y = school attendance rate",
      "S = high practice participation (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-199",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does sports program enrollment cause a change in school attendance rate?\nHow can restricting the analysis to high practice participation bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into high practice participation occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into high practice participation is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by high practice participation and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-200",
    "bucket": "BucketLarge-J",
    "title": "Medication Reminder Texts and Refills",
    "scenario": "A clinic sends medication reminder texts. The clinic reports higher refill rates among patients who clicked the confirmation link in the texts.\n\nPatients who received texts but never clicked are excluded from the refill calculation.",
    "variables": [
      "X = text reminder program",
      "Y = medication refill rate",
      "S = clicking the confirmation link (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-200",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Public Health",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does text reminder program cause a change in medication refill rate?\nHow can restricting the analysis to clicking the confirmation link bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into clicking the confirmation link occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into clicking the confirmation link is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by clicking the confirmation link and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-201",
    "bucket": "BucketLarge-J",
    "title": "Bootcamp Scholarships and Salary Outcomes",
    "scenario": "A nonprofit funds coding bootcamp scholarships. The nonprofit reports large salary gains using only scholarship recipients who completed the bootcamp and self-reported job outcomes.\n\nRecipients who did not report outcomes are excluded from salary statistics.",
    "variables": [
      "X = bootcamp scholarship funding",
      "Y = post-bootcamp salary",
      "S = completion plus outcome reporting (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-201",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Labor Economics",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does bootcamp scholarship funding cause a change in post-bootcamp salary?\nHow can restricting the analysis to completion plus outcome reporting bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into completion plus outcome reporting occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into completion plus outcome reporting is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by completion plus outcome reporting and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-202",
    "bucket": "BucketLarge-J",
    "title": "Neighborhood Watch App and Reported Crime",
    "scenario": "A city launches a neighborhood watch app. The city claims the app reduces crime because high-adoption neighborhoods show fewer incidents.\n\nThe evaluation excludes neighborhoods where adoption was low and relies heavily on app-based reporting.",
    "variables": [
      "X = neighborhood watch app rollout",
      "Y = reported crime incidents",
      "S = active app usage / high adoption (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-202",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does neighborhood watch app rollout cause a change in reported crime incidents?\nHow can restricting the analysis to active app usage / high adoption bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into active app usage / high adoption occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into active app usage / high adoption is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by active app usage / high adoption and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-203",
    "bucket": "BucketLarge-J",
    "title": "Transit Reliability Alerts and Satisfaction",
    "scenario": "A transit agency launches a reliability initiative. The agency reports higher satisfaction by surveying riders who signed up for service-alert notifications.\n\nRiders who did not sign up are excluded from the satisfaction survey sample.",
    "variables": [
      "X = reliability initiative",
      "Y = rider satisfaction",
      "S = subscription to service alerts (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-203",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Transportation Policy",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does reliability initiative cause a change in rider satisfaction?\nHow can restricting the analysis to subscription to service alerts bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into subscription to service alerts occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into subscription to service alerts is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by subscription to service alerts and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-204",
    "bucket": "BucketLarge-J",
    "title": "Microloan Coaching and Repayment",
    "scenario": "A microfinance organization offers microloans with optional coaching. The organization claims coaching improves repayment because borrowers who attended at least three sessions repaid at higher rates.\n\nBorrowers offered coaching but attending fewer sessions are excluded from the coached-group analysis.",
    "variables": [
      "X = coaching add-on to microloans",
      "Y = loan repayment rate",
      "S = attending 3+ coaching sessions (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-204",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Development Economics",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does coaching add-on to microloans cause a change in loan repayment rate?\nHow can restricting the analysis to attending 3+ coaching sessions bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into attending 3+ coaching sessions occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into attending 3+ coaching sessions is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by attending 3+ coaching sessions and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-205",
    "bucket": "BucketLarge-J",
    "title": "Multi-day Orientation and First-Year GPA",
    "scenario": "A college redesigns orientation into a multi-day program. Administrators claim the new orientation improves academic performance because students who attended all days had higher first-year GPAs.\n\nStudents who missed days due to work or travel constraints are excluded from the “full-attendance” group.",
    "variables": [
      "X = new multi-day orientation",
      "Y = first-year GPA",
      "S = full orientation attendance (included vs. excluded)"
    ],
    "annotations": {
      "Case ID": "J2-205",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Higher Education",
      "Causal Structure": "X → S and (other factors) → S; S is related to Y; conditioning on S biases the X→Y estimate.",
      "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does new multi-day orientation cause a change in first-year GPA?\nHow can restricting the analysis to full orientation attendance bias the estimated effect?",
    "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
    "Hidden Timestamp": "Does selection into full orientation attendance occur after the intervention could influence it, and is selection related to outcomes?",
    "Conditional Answers": "Answer if selection into full orientation attendance is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
    "Wise Refusal": "I can’t estimate the intervention effect without outcomes for those excluded by full orientation attendance and a clear picture of why selection differs between treated and untreated units."
  },
  {
    "id": "T3-BucketJ-206",
    "bucket": "BucketLarge-J",
    "title": "Medication Co-pay Waiver Among Adherent Patients",
    "scenario": "A clinic waives medication co-pays for a subset of patients. Analysts compare blood-pressure outcomes only among patients who took at least 90% of doses (adherent patients) and find that those with the co-pay waiver have worse blood pressure control. They conclude the waiver harms outcomes.\n\nAdherence is influenced by both the waiver (making adherence easier) and patients’ underlying health-management capacity and stress, which also affect blood pressure.",
    "variables": [
      "X = co-pay waiver (yes vs. no)",
      "Y = blood pressure control",
      "C = adherent (90%+ doses) (conditioned-on)",
      "U = health-management capacity/stress (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-206",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are adherent (90%+ doses), does X appear to change Y?\nWhy can conditioning on adherent (90%+ doses) create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (adherent (90%+ doses)) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become adherent (90%+ doses) and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-207",
    "bucket": "BucketLarge-J",
    "title": "Parole Support Program Among Those Who Attend Meetings",
    "scenario": "A county offers a parole support program. A report compares recidivism only among parolees who attended all required meetings and finds higher recidivism in the program group, concluding the program is ineffective.\n\nMeeting attendance depends on program assignment (some meetings are mandatory under the program) and on unobserved stability factors (transportation, housing), which also affect recidivism.",
    "variables": [
      "X = parole support program assignment",
      "Y = recidivism",
      "C = fully compliant with meetings (conditioned-on)",
      "U = baseline stability/transportation access (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-207",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are fully compliant with meetings, does X appear to change Y?\nWhy can conditioning on fully compliant with meetings create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (fully compliant with meetings) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become fully compliant with meetings and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-208",
    "bucket": "BucketLarge-J",
    "title": "Scholarship Counseling Among Students Who Show Up",
    "scenario": "A university offers academic counseling to scholarship students. Evaluators compare GPA only among students who attended at least five counseling sessions and find that counseled students have lower GPAs, concluding counseling hurts performance.\n\nSession attendance is affected by counseling availability and by unobserved academic difficulty and motivation, which also influence GPA.",
    "variables": [
      "X = academic counseling offer",
      "Y = semester GPA",
      "C = attended ≥5 sessions (conditioned-on)",
      "U = unobserved academic difficulty/motivation (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-208",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Higher Education",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are attended ≥5 sessions, does X appear to change Y?\nWhy can conditioning on attended ≥5 sessions create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (attended ≥5 sessions) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become attended ≥5 sessions and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-209",
    "bucket": "BucketLarge-J",
    "title": "Job Search Platform and Active Users",
    "scenario": "A job-search platform introduces a new “smart recommendations” feature. Analysts compare job-offer rates only among users who were active weekly and find that users with the feature have lower offer rates, concluding the feature is harmful.\n\nWeekly activity is affected by feature exposure and by unobserved job-seeker urgency and constraints, which also affect job offers.",
    "variables": [
      "X = smart recommendations feature exposure",
      "Y = job-offer rate",
      "C = weekly active user (conditioned-on)",
      "U = job-seeker urgency/constraints (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-209",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Labor Economics",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are weekly active user, does X appear to change Y?\nWhy can conditioning on weekly active user create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (weekly active user) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become weekly active user and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-210",
    "bucket": "BucketLarge-J",
    "title": "Home Energy Reports Among Readers",
    "scenario": "A utility sends home energy reports to some households. The evaluation compares electricity use only among households that opened the emailed report and finds that treated households used more energy, suggesting reports backfire.\n\nEmail opening is influenced by being sent the report and by unobserved engagement levels and household routines that also affect electricity use.",
    "variables": [
      "X = receiving home energy report emails",
      "Y = electricity consumption",
      "C = opened/read the report (conditioned-on)",
      "U = household engagement/routines (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-210",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Environmental Policy",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are opened/read the report, does X appear to change Y?\nWhy can conditioning on opened/read the report create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (opened/read the report) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become opened/read the report and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-211",
    "bucket": "BucketLarge-J",
    "title": "Teacher Coaching Among Participants Who Complete Surveys",
    "scenario": "A district offers teacher coaching. Analysts compare classroom observation scores only among teachers who completed all post-coaching surveys and find coaching teachers score worse, concluding coaching reduces performance.\n\nSurvey completion is influenced by coaching participation and by unobserved conscientiousness and workload, which also influence observation outcomes.",
    "variables": [
      "X = teacher coaching participation",
      "Y = classroom observation score",
      "C = completed all follow-up surveys (conditioned-on)",
      "U = conscientiousness/workload (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-211",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Education Policy",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are completed all follow-up surveys, does X appear to change Y?\nWhy can conditioning on completed all follow-up surveys create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (completed all follow-up surveys) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become completed all follow-up surveys and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-212",
    "bucket": "BucketLarge-J",
    "title": "Mental Health App Among Daily Users",
    "scenario": "A campus provides a mental health app to students. A study compares stress scores only among students who used the app daily and finds higher stress among app users, concluding the app increases stress.\n\nDaily use is influenced by app access and by unobserved baseline stress and help-seeking behavior, which also predict later stress scores.",
    "variables": [
      "X = app access/encouragement",
      "Y = reported stress score",
      "C = daily app user (conditioned-on)",
      "U = baseline stress/help-seeking propensity (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-212",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Medium",
      "Subdomain": "Psychology",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are daily app user, does X appear to change Y?\nWhy can conditioning on daily app user create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (daily app user) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become daily app user and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-213",
    "bucket": "BucketLarge-J",
    "title": "Transit Fare Capping Among Frequent Riders",
    "scenario": "A transit agency implements fare capping. Analysts compare satisfaction only among riders who took at least 20 trips per month and find lower satisfaction among capped-fare riders, concluding fare capping reduces satisfaction.\n\nHigh trip frequency is influenced by fare capping (making frequent riding cheaper) and by unobserved commuter dependence and route constraints, which also affect satisfaction.",
    "variables": [
      "X = fare-capping policy exposure",
      "Y = rider satisfaction",
      "C = frequent rider (≥20 trips/month) (conditioned-on)",
      "U = commuter dependence/route constraints (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-213",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Transportation Policy",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are frequent rider (≥20 trips/month), does X appear to change Y?\nWhy can conditioning on frequent rider (≥20 trips/month) create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (frequent rider (≥20 trips/month)) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become frequent rider (≥20 trips/month) and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-214",
    "bucket": "BucketLarge-J",
    "title": "Community Mediation Program Among Participants Who Complete Mediation",
    "scenario": "A city offers community mediation for neighbor disputes. The evaluation compares conflict recurrence only among disputes that completed mediation sessions and finds higher recurrence when mediation was offered, concluding mediation worsens conflicts.\n\nCompletion depends on mediation offer (providing a path to completion) and on unobserved conflict intensity and willingness to compromise, which also affects recurrence.",
    "variables": [
      "X = mediation offer",
      "Y = conflict recurrence rate",
      "C = completed mediation (conditioned-on)",
      "U = conflict intensity/willingness to compromise (unobserved)"
    ],
    "annotations": {
      "Case ID": "J2-214",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Urban Policy",
      "Causal Structure": "X → C ← U → Y; conditioning on C opens X ↔ U, biasing X–Y within C=1.",
      "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment–outcome relationships.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Among those who are completed mediation, does X appear to change Y?\nWhy can conditioning on completed mediation create a spurious association between X and Y?",
    "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
    "Hidden Timestamp": "Did compliance/participation (completed mediation) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
    "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
    "Wise Refusal": "I can’t make a causal claim from the compliant-only analysis without modeling why people become completed mediation and whether those determinants also affect Y."
  },
  {
    "id": "T3-BucketJ-215",
    "bucket": "BucketLarge-J",
    "title": "Plagiarism Detector Flags in a Large Class",
    "scenario": "A professor uses an automated plagiarism detector that flags 2% of submissions. The professor says, “If the system flags you, you basically plagiarized,” and proposes an automatic penalty policy.\n\nIn reality, confirmed plagiarism is rare in this class, and the detector can produce false positives, especially on common template phrases.",
    "variables": [
      "Signal = detector flag (positive)",
      "Event = underlying condition/event of interest",
      "π = true plagiarism prevalence in the class"
    ],
    "annotations": {
      "Case ID": "J2-215",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (detector flag (positive)), what is the probability the underlying event is actually true?\nHow does the base rate (true plagiarism prevalence in the class) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates.",
    "Hidden Timestamp": "Were the base rate (true plagiarism prevalence in the class) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-216",
    "bucket": "BucketLarge-J",
    "title": "Terror Watchlist Alerts at an Airport",
    "scenario": "An airport runs passengers through a watchlist system and gets a small number of alerts. A security officer claims, “An alert means the passenger is almost certainly dangerous.”\n\nThe true prevalence of dangerous individuals among passengers is extremely low, and the system can generate false positives due to name similarity.",
    "variables": [
      "Signal = watchlist alert (positive)",
      "Event = underlying condition/event of interest",
      "π = prevalence of dangerous individuals among passengers"
    ],
    "annotations": {
      "Case ID": "J2-216",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Hard",
      "Subdomain": "Public Safety",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (watchlist alert (positive)), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of dangerous individuals among passengers) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates.",
    "Hidden Timestamp": "Were the base rate (prevalence of dangerous individuals among passengers) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-217",
    "bucket": "BucketLarge-J",
    "title": "Fraud Alerts in Online Payments",
    "scenario": "A payment platform’s fraud model flags transactions as “high risk.” A manager claims, “High-risk flagged transactions are almost always fraud,” and wants to auto-decline all flagged purchases.\n\nFraud is uncommon relative to total transactions, and the model can misclassify unusual but legitimate purchases.",
    "variables": [
      "Signal = fraud-model high-risk flag",
      "Event = underlying condition/event of interest",
      "π = prevalence of fraud among all transactions"
    ],
    "annotations": {
      "Case ID": "J2-217",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Finance",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (fraud-model high-risk flag), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of fraud among all transactions) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates.",
    "Hidden Timestamp": "Were the base rate (prevalence of fraud among all transactions) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-218",
    "bucket": "BucketLarge-J",
    "title": "Rare Disease Screening in a Workplace",
    "scenario": "A company offers screening for a rare disease. An employee tests positive and the HR office says, “A positive test means you probably have the disease,” and recommends immediate treatment.\n\nThe disease is very rare in the workforce, and the test has non-zero false positives.",
    "variables": [
      "Signal = positive screening test",
      "Event = underlying condition/event of interest",
      "π = prevalence of the disease in the screened population"
    ],
    "annotations": {
      "Case ID": "J2-218",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (positive screening test), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of the disease in the screened population) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates.",
    "Hidden Timestamp": "Were the base rate (prevalence of the disease in the screened population) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-219",
    "bucket": "BucketLarge-J",
    "title": "Defect Detection on a Production Line",
    "scenario": "A factory installs a camera system that flags items as defective. A supervisor states, “If the camera flags an item, it’s defective,” and increases scrap rates.\n\nTrue defects are uncommon on this stabilized line, and the camera sometimes flags harmless cosmetic variations.",
    "variables": [
      "Signal = camera defect flag",
      "Event = underlying condition/event of interest",
      "π = baseline defect prevalence on the line"
    ],
    "annotations": {
      "Case ID": "J2-219",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Operations",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (camera defect flag), what is the probability the underlying event is actually true?\nHow does the base rate (baseline defect prevalence on the line) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim “a positive signal almost surely means the event is true” is INVALID without incorporating the base rate and error rates.",
    "Hidden Timestamp": "Were the base rate (baseline defect prevalence on the line) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-220",
    "bucket": "BucketLarge-J",
    "title": "Interpreting a Positive Drug Test",
    "scenario": "A workplace drug test is said to be “99% accurate.” Management argues that if a test is positive, the employee almost certainly used drugs.\n\nThis reasoning uses the test’s accuracy as if it directly gave P(Drug use | Positive), without considering the base rate of drug use in the tested workforce.",
    "variables": [
      "Signal = positive drug test result",
      "Event = underlying condition/event of interest",
      "π = drug-use prevalence in the workforce"
    ],
    "annotations": {
      "Case ID": "J2-220",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Conditional Fallacy",
      "Difficulty": "Hard",
      "Subdomain": "Workplace Policy",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (positive drug test result), what is the probability the underlying event is actually true?\nHow does the base rate (drug-use prevalence in the workforce) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule.",
    "Hidden Timestamp": "Were the base rate (drug-use prevalence in the workforce) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-221",
    "bucket": "BucketLarge-J",
    "title": "Harassment Report and Guilt",
    "scenario": "A company receives an anonymous harassment report. A manager says, “Most real harassers get reported, so if someone is reported they are likely guilty.”\n\nThis confuses the likelihood of a report given guilt with the probability of guilt given a report, and ignores how common false or ambiguous reports are relative to true cases.",
    "variables": [
      "Signal = being reported",
      "Event = underlying condition/event of interest",
      "π = prevalence of actual harassment among employees"
    ],
    "annotations": {
      "Case ID": "J2-221",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Conditional Fallacy",
      "Difficulty": "Hard",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (being reported), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of actual harassment among employees) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule.",
    "Hidden Timestamp": "Were the base rate (prevalence of actual harassment among employees) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-222",
    "bucket": "BucketLarge-J",
    "title": "Predictive Policing “High-Risk” Label",
    "scenario": "A predictive policing tool labels a neighborhood as “high risk.” An official argues, “High-risk labels are accurate because most high-crime neighborhoods get labeled high risk.”\n\nThis mixes up P(labeled | high crime) with P(high crime | labeled) and ignores how many neighborhoods receive labels relative to the true high-crime base rate.",
    "variables": [
      "Signal = high-risk label",
      "Event = underlying condition/event of interest",
      "π = base rate of truly high-crime neighborhoods"
    ],
    "annotations": {
      "Case ID": "J2-222",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Conditional Fallacy",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (high-risk label), what is the probability the underlying event is actually true?\nHow does the base rate (base rate of truly high-crime neighborhoods) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule.",
    "Hidden Timestamp": "Were the base rate (base rate of truly high-crime neighborhoods) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-223",
    "bucket": "BucketLarge-J",
    "title": "Spam Filter and Important Emails",
    "scenario": "An email filter catches “95% of spam.” A user claims that any email sent to spam is almost certainly spam and deletes the folder regularly.\n\nThis uses P(sent to spam | spam) as if it were P(spam | sent to spam) and ignores the fraction of all emails that are spam and the false positive rate.",
    "variables": [
      "Signal = email sent to spam folder",
      "Event = underlying condition/event of interest",
      "π = base rate of spam among all incoming emails"
    ],
    "annotations": {
      "Case ID": "J2-223",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Conditional Fallacy",
      "Difficulty": "Medium",
      "Subdomain": "Information Systems",
      "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on π and error rates.",
      "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Given the signal (email sent to spam folder), what is the probability the underlying event is actually true?\nHow does the base rate (base rate of spam among all incoming emails) affect interpretation of the signal?",
    "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test’s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes’ rule.",
    "Hidden Timestamp": "Were the base rate (base rate of spam among all incoming emails) and the signal’s false-positive/false-negative rates established before interpreting the current flagged cases?",
    "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes’ rule to compute P(Event | Signal) rather than relying on intuition.",
    "Wise Refusal": "I can’t give the probability without the base rate and the signal’s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
  },
  {
    "id": "T3-BucketJ-224",
    "bucket": "BucketLarge-J",
    "title": "College Program Counterfactual Using Post-Program Confidence",
    "scenario": "A university asks: “Would participants have graduated if they had not joined an intensive college-success program?” An analyst proposes comparing graduates among participants and non-participants only within the subset of students who report high academic confidence at the end of the first year.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic end-of-year academic confidence.",
    "variables": [
      "X = program participation",
      "Y = graduation",
      "M = end-of-year academic confidence (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-224",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Higher Education",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on end-of-year academic confidence?\nWhy is end-of-year academic confidence a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does end-of-year academic confidence occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if end-of-year academic confidence is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if end-of-year academic confidence is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether end-of-year academic confidence is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-225",
    "bucket": "BucketLarge-J",
    "title": "Police Training Counterfactual Using Post-Training Attitudes",
    "scenario": "A department asks: “Would officer complaints have fallen if we had not implemented a de-escalation training?” A report proposes comparing trained vs. untrained officers only among those who later report high commitment to de-escalation principles.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-training de-escalation commitment.",
    "variables": [
      "X = de-escalation training",
      "Y = citizen complaints",
      "M = post-training de-escalation commitment (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-225",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-training de-escalation commitment?\nWhy is post-training de-escalation commitment a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does post-training de-escalation commitment occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if post-training de-escalation commitment is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-training de-escalation commitment is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether post-training de-escalation commitment is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-226",
    "bucket": "BucketLarge-J",
    "title": "Cash Transfer Counterfactual Using Later Savings",
    "scenario": "A city asks: “Would households have avoided eviction if we had not provided a one-time cash transfer?” An analyst suggests comparing treated and untreated households only among those who later have a high savings balance.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later savings balance.",
    "variables": [
      "X = cash transfer receipt",
      "Y = eviction occurrence",
      "M = later savings balance (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-226",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Housing Policy",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on later savings balance?\nWhy is later savings balance a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does later savings balance occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if later savings balance is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if later savings balance is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether later savings balance is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-227",
    "bucket": "BucketLarge-J",
    "title": "Remote Work Counterfactual Using Post-Policy Team Cohesion",
    "scenario": "A firm asks: “Would productivity have been higher if we had not moved to remote work?” A manager proposes comparing remote vs. on-site teams only among teams that later report high cohesion.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-policy team cohesion score.",
    "variables": [
      "X = remote-work policy exposure",
      "Y = team productivity",
      "M = post-policy team cohesion score (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-227",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-policy team cohesion score?\nWhy is post-policy team cohesion score a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does post-policy team cohesion score occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if post-policy team cohesion score is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-policy team cohesion score is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether post-policy team cohesion score is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-228",
    "bucket": "BucketLarge-J",
    "title": "Mentoring Counterfactual Using Later Network Size",
    "scenario": "A nonprofit asks: “Would mentees have found jobs as quickly if they had not received mentoring?” The evaluation compares mentored vs. non-mentored applicants only among those who later report having a large professional network.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later professional network size.",
    "variables": [
      "X = mentoring participation",
      "Y = time to job offer",
      "M = later professional network size (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-228",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Labor Economics",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on later professional network size?\nWhy is later professional network size a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does later professional network size occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if later professional network size is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if later professional network size is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether later professional network size is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-229",
    "bucket": "BucketLarge-J",
    "title": "Public Transit Subsidy Counterfactual Using Post-Subsidy Ridership",
    "scenario": "A city asks: “Would congestion have been worse if we had not subsidized transit?” An analyst compares subsidized vs. non-subsidized commuters only among those who became frequent transit riders afterward.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-subsidy ridership frequency.",
    "variables": [
      "X = transit subsidy exposure",
      "Y = traffic congestion contribution",
      "M = post-subsidy ridership frequency (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-229",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Transportation Policy",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-subsidy ridership frequency?\nWhy is post-subsidy ridership frequency a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does post-subsidy ridership frequency occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if post-subsidy ridership frequency is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-subsidy ridership frequency is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether post-subsidy ridership frequency is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-230",
    "bucket": "BucketLarge-J",
    "title": "Moderation Policy Counterfactual Using Later User Engagement",
    "scenario": "A platform asks: “Would toxicity have been higher if we had not changed the moderation policy?” A team proposes comparing communities with and without the policy only among communities that later exhibit high engagement.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later engagement level.",
    "variables": [
      "X = moderation policy change",
      "Y = toxicity rate",
      "M = later engagement level (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-230",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Platform Policy",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on later engagement level?\nWhy is later engagement level a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does later engagement level occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if later engagement level is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if later engagement level is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether later engagement level is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-231",
    "bucket": "BucketLarge-J",
    "title": "Smoking Cessation Campaign Counterfactual Using Post-Campaign Motivation",
    "scenario": "A public health agency asks: “Would smoking rates have fallen without the campaign?” A report compares exposed vs. unexposed groups only among individuals who later report high motivation to quit.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-campaign motivation to quit.",
    "variables": [
      "X = campaign exposure",
      "Y = smoking cessation",
      "M = post-campaign motivation to quit (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-231",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-campaign motivation to quit?\nWhy is post-campaign motivation to quit a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does post-campaign motivation to quit occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if post-campaign motivation to quit is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-campaign motivation to quit is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether post-campaign motivation to quit is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-232",
    "bucket": "BucketLarge-J",
    "title": "Scholarship Counterfactual Using Post-Scholarship Course Load",
    "scenario": "A college asks: “Would scholarship recipients have maintained the same GPA without the scholarship?” The analysis compares scholarship and non-scholarship students only among those who later took a heavy course load.\n\nA policymaker asks a counterfactual: “If X had been different, would Y have changed?” They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-receipt course load intensity.",
    "variables": [
      "X = scholarship receipt",
      "Y = semester GPA",
      "M = post-receipt course load intensity (post-intervention)"
    ],
    "annotations": {
      "Case ID": "J2-232",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Higher Education",
      "Causal Structure": "X → M → Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
      "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we answer the counterfactual question “What would Y have been if X were different?” by conditioning on post-receipt course load intensity?\nWhy is post-receipt course load intensity a cross-world confounder in this counterfactual comparison?",
    "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without “cross-world” assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
    "Hidden Timestamp": "Does post-receipt course load intensity occur after X is assigned, meaning X can change M before Y is realized?",
    "Conditional Answers": "Answer if post-receipt course load intensity is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-receipt course load intensity is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
    "Wise Refusal": "I can’t answer the counterfactual without clarifying whether post-receipt course load intensity is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
  },
  {
    "id": "T3-BucketJ-233",
    "bucket": "BucketLarge-J",
    "title": "Housing Vouchers and Employment: Fixing Neighborhood Quality",
    "scenario": "A county asks: “Would employment rise if we offered housing vouchers, but kept families’ neighborhood quality fixed at what it is now?” They argue this isolates the “voucher effect” that is not due to moving.\n\nIn practice, vouchers often enable moves to different neighborhoods. Neighborhood quality affects access to jobs, networks, transportation, and childcare, which all influence employment outcomes.",
    "variables": [
      "X = housing voucher receipt",
      "Y = employment status",
      "M = neighborhood quality/opportunity level (mediator)"
    ],
    "annotations": {
      "Case ID": "J2-233",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounder–Mediator Error",
      "Trap Subtype": "Mediator Fixing Error",
      "Difficulty": "Hard",
      "Subdomain": "Housing Policy",
      "Causal Structure": "X → M → Y; attempting to fix M while changing X creates an ill-defined counterfactual unless mediation assumptions are made explicit.",
      "Key Insight": "Holding a mediator fixed while changing the treatment can create an incoherent counterfactual world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to answer “What would Y be if we change X but keep M fixed?” when M is affected by X?\nWhat goes wrong when we try to fix the mediator to its observed value?",
    "expected_analysis": "This is Confounder–Mediator Error (Mediator Fixing Error).\nM lies on the causal pathway from X to Y. The proposed counterfactual “change X while holding M fixed” is generally not identifiable from standard observational data and can be logically inconsistent with how M would respond to X.\nFixing M can create a counterfactual world that is incompatible with the causal system, leading to biased or undefined effects.\nConclusion: The proposed counterfactual is INVALID unless you define a well-posed mediation estimand and justify strong assumptions (or perform a suitable intervention on M).",
    "Hidden Timestamp": "Does M occur after X and before Y, making it a mediator rather than a baseline confounder?",
    "Conditional Answers": "Answer if you want the total effect of X on Y: Do not fix M; compare Y under different X values.\nAnswer if you want a direct effect not through M: Use formal mediation analysis with explicit assumptions, or design an intervention that manipulates M.\nAnswer if M is actually pre-treatment: Then fixing M may be reasonable, but verify timing carefully.",
    "Wise Refusal": "I can’t answer the “hold M fixed” counterfactual without clarifying whether M is post-treatment and specifying the mediation estimand and assumptions."
  },
  {
    "id": "T3-BucketJ-234",
    "bucket": "BucketLarge-J",
    "title": "Overtime and Promotion: A Counterfactual Misorder (2)",
    "scenario": "A manager asks: “Would Alex have been promoted if they hadn’t worked so much overtime?” The manager treats overtime as the cause and promotion as the effect.\n\nBut in this organization, employees are assigned overtime when leadership already expects they are promotion candidates and wants to test them under pressure. Overtime increases after early promotion signals appear.",
    "variables": [
      "X = working overtime",
      "Y = promotion decision"
    ],
    "annotations": {
      "Case ID": "J2-234",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Reverse Causation",
      "Trap Subtype": "Outcome-dependent Worlds",
      "Difficulty": "Hard",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Y (or early Y) → X; treating X as a cause leads to outcome-dependent counterfactuals.",
      "Key Insight": "Counterfactuals are not valid when the “cause” is actually reacting to the outcome.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is the counterfactual “What would Y have been if X were different?” well-defined here?\nHow could the outcome (or anticipation of it) influence X, making the counterfactual world outcome-dependent?",
    "expected_analysis": "This is L3 Reverse Causation (Outcome-dependent Worlds).\nThe setup suggests X is chosen or changes in response to the outcome (or strong early signals of the outcome), so imagining “set X differently” may implicitly change the underlying situation that produced Y.\nIf X is downstream of Y (or of early manifestations of Y), naive counterfactual comparisons can be ill-posed or misleading.\nConclusion: The proposed counterfactual claim is INVALID unless timing is clarified and a causal ordering that makes X antecedent to Y is justified.",
    "Hidden Timestamp": "Did promotion decision (or early indicators of it) occur before changes in working overtime, potentially causing X rather than being caused by it?",
    "Conditional Answers": "Answer if X clearly occurs before Y and can be intervened on: Then the counterfactual is meaningful and can be analyzed with a causal model.\nAnswer if Y (or early Y signals) drives X: Then interpret the relationship as reverse causation; redesign the study to capture pre-outcome X.\nAnswer if you can instrument X with an exogenous shock: Then you may identify a causal effect with strong assumptions.",
    "Wise Refusal": "I can’t assess the counterfactual without a clear timeline showing whether working overtime precedes promotion decision and whether X can be manipulated independently of Y."
  },
  {
    "id": "T3-BucketJ-235",
    "bucket": "BucketLarge-J",
    "title": "Extra Policing and Crime: Reactive Deployment (2)",
    "scenario": "A city council asks: “Would crime have been lower if we had not increased patrols in Neighborhood Q?” A memo treats patrol increases as the cause of crime levels.\n\nPatrol hours are increased in response to early spikes in crime reports and calls for service, so rising crime drives deployment changes rather than the other way around.",
    "variables": [
      "X = increased patrol deployment",
      "Y = crime rate"
    ],
    "annotations": {
      "Case ID": "J2-235",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Reverse Causation",
      "Trap Subtype": "Outcome-dependent Worlds",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Y (or early Y) → X; treating X as a cause leads to outcome-dependent counterfactuals.",
      "Key Insight": "Counterfactuals are not valid when the “cause” is actually reacting to the outcome.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is the counterfactual “What would Y have been if X were different?” well-defined here?\nHow could the outcome (or anticipation of it) influence X, making the counterfactual world outcome-dependent?",
    "expected_analysis": "This is L3 Reverse Causation (Outcome-dependent Worlds).\nThe setup suggests X is chosen or changes in response to the outcome (or strong early signals of the outcome), so imagining “set X differently” may implicitly change the underlying situation that produced Y.\nIf X is downstream of Y (or of early manifestations of Y), naive counterfactual comparisons can be ill-posed or misleading.\nConclusion: The proposed counterfactual claim is INVALID unless timing is clarified and a causal ordering that makes X antecedent to Y is justified.",
    "Hidden Timestamp": "Did crime rate (or early indicators of it) occur before changes in increased patrol deployment, potentially causing X rather than being caused by it?",
    "Conditional Answers": "Answer if X clearly occurs before Y and can be intervened on: Then the counterfactual is meaningful and can be analyzed with a causal model.\nAnswer if Y (or early Y signals) drives X: Then interpret the relationship as reverse causation; redesign the study to capture pre-outcome X.\nAnswer if you can instrument X with an exogenous shock: Then you may identify a causal effect with strong assumptions.",
    "Wise Refusal": "I can’t assess the counterfactual without a clear timeline showing whether increased patrol deployment precedes crime rate and whether X can be manipulated independently of Y."
  },
  {
    "id": "T3-BucketJ-236",
    "bucket": "BucketLarge-J",
    "title": "Recommendation Algorithm and Polarization Over Time",
    "scenario": "A platform asks: “If we had not changed the recommendation algorithm last year, would political polarization on the site be lower today?” The team wants to extrapolate from a short A/B test run for two weeks.\n\nHowever, the algorithm shapes what content users see, which changes who stays on the platform, how users post, and what content is produced. Those shifts then change future recommendation data and future exposure patterns.",
    "variables": [
      "X = recommendation algorithm change",
      "Y = polarization on the platform",
      "F = user-content ecosystem and retention dynamics (dynamic state)"
    ],
    "annotations": {
      "Case ID": "J2-236",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Feedback Loops",
      "Trap Subtype": "Dynamic World Divergence",
      "Difficulty": "Hard",
      "Subdomain": "Platform Policy",
      "Causal Structure": "X changes system state F; F influences future exposure and outcomes; trajectories diverge across counterfactual worlds.",
      "Key Insight": "With feedback, the alternative world is a different evolving system, not a one-shot swap of X.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why can short-run estimates of changing X fail to predict long-run counterfactual Y under a different policy?\nHow does user-content ecosystem and retention dynamics create dynamic world divergence across counterfactual worlds?",
    "expected_analysis": "This is L3 Feedback Loops (Dynamic World Divergence).\nChanging X alters the system state F over time, and F then changes future behaviors and outcomes, creating different trajectories under different counterfactual policies.\nA static comparison that ignores how the environment adapts can misstate “what would have happened” under an alternative X.\nConclusion: The counterfactual claim is CONDITIONAL: you need a dynamic causal model (or simulation/longitudinal design) that accounts for feedback.",
    "Hidden Timestamp": "Does user-content ecosystem and retention dynamics evolve after X changes and then influence future Y, creating path dependence?",
    "Conditional Answers": "Answer if you only care about immediate effects before F adapts: A short-run causal estimate may be informative.\nAnswer if you care about long-run outcomes: Model the feedback dynamics explicitly; the counterfactual path under alternative X can diverge.\nAnswer if policy changes F in a way that changes who is exposed later: Then simple extrapolation from short-run data is unreliable.",
    "Wise Refusal": "I can’t answer the long-run counterfactual without assumptions (or data) about how user-content ecosystem and retention dynamics evolves and how behavior adapts under different X values."
  },
  {
    "id": "T3-BucketJ-237",
    "bucket": "BucketLarge-J",
    "title": "Policing Strategy and Community Trust Trajectories",
    "scenario": "A city asks: “If we had used a less aggressive policing strategy, would community trust be higher after three years?” An analyst tries to answer using a one-time comparison of neighborhoods with different patrol styles.\n\nPolicing style changes residents’ willingness to report incidents and cooperate, which changes observed crime statistics and subsequent allocation decisions. The policy and the environment co-evolve over time.",
    "variables": [
      "X = policing strategy aggressiveness",
      "Y = community trust after three years",
      "F = reporting/cooperation dynamics affecting future allocation (dynamic state)"
    ],
    "annotations": {
      "Case ID": "J2-237",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Feedback Loops",
      "Trap Subtype": "Dynamic World Divergence",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "X changes system state F; F influences future exposure and outcomes; trajectories diverge across counterfactual worlds.",
      "Key Insight": "With feedback, the alternative world is a different evolving system, not a one-shot swap of X.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Why can short-run estimates of changing X fail to predict long-run counterfactual Y under a different policy?\nHow does reporting/cooperation dynamics affecting future allocation create dynamic world divergence across counterfactual worlds?",
    "expected_analysis": "This is L3 Feedback Loops (Dynamic World Divergence).\nChanging X alters the system state F over time, and F then changes future behaviors and outcomes, creating different trajectories under different counterfactual policies.\nA static comparison that ignores how the environment adapts can misstate “what would have happened” under an alternative X.\nConclusion: The counterfactual claim is CONDITIONAL: you need a dynamic causal model (or simulation/longitudinal design) that accounts for feedback.",
    "Hidden Timestamp": "Does reporting/cooperation dynamics affecting future allocation evolve after X changes and then influence future Y, creating path dependence?",
    "Conditional Answers": "Answer if you only care about immediate effects before F adapts: A short-run causal estimate may be informative.\nAnswer if you care about long-run outcomes: Model the feedback dynamics explicitly; the counterfactual path under alternative X can diverge.\nAnswer if policy changes F in a way that changes who is exposed later: Then simple extrapolation from short-run data is unreliable.",
    "Wise Refusal": "I can’t answer the long-run counterfactual without assumptions (or data) about how reporting/cooperation dynamics affecting future allocation evolves and how behavior adapts under different X values."
  },
  {
    "id": "T3-BucketJ-238",
    "bucket": "BucketLarge-J",
    "title": "Early Preemption in a Protest Outcome",
    "scenario": "A city asks: “If the primary protest organizer had been arrested the night before, would the protest have happened?” In the observed world, the organizer led the march and the protest occurred.\n\nMultiple groups were prepared to take over leadership. If the arrest had happened, a backup organizer who was already mobilizing might have led an alternative march that started earlier and still resulted in a protest.",
    "variables": [
      "X = arresting the primary organizer",
      "Y = a large protest occurs",
      "A = alternative sufficient cause"
    ],
    "annotations": {
      "Case ID": "J2-238",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Preemption",
      "Trap Subtype": "Early Preemption",
      "Difficulty": "Hard",
      "Subdomain": "Political Science",
      "Causal Structure": "Two sufficient causes for Y; removing one may not change Y because the other would occur.",
      "Key Insight": "Observed causation does not imply counterfactual dependence when multiple sufficient causes can produce the outcome.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we prevent arresting the primary organizer, does Y necessarily change?\nHow does early preemption complicate attributing Y to X in the counterfactual world?",
    "expected_analysis": "This is L3 Preemption (Early Preemption).\nThere are multiple sufficient causal paths to Y. Even if X occurs in the observed world, removing X in the counterfactual world may not change Y because an alternative cause would produce Y instead.\nAttribution requires modeling which cause would have fired in the absence of X (counterfactual dependence), not just observing that X happened.\nConclusion: The naive statement “X caused Y” is CONDITIONAL; it depends on whether Y counterfactually depends on X given competing causes.",
    "Hidden Timestamp": "If arresting the primary organizer were prevented, what alternative cause would have been most likely to produce Y, and would it occur earlier or later?",
    "Conditional Answers": "Answer if no other sufficient cause exists: Then preventing X would prevent Y, so X is a but-for cause.\nAnswer if alternative sufficient causes exist: Y may still occur without X; you need a structural model of competing causes and timing.\nAnswer if timing determines which cause preempts the other: Identify whether X was early/late relative to the alternative cause to assess counterfactual dependence.",
    "Wise Refusal": "I can’t decide whether X is a but-for cause without specifying competing causes and their timing; otherwise the counterfactual is underdetermined."
  },
  {
    "id": "T3-BucketJ-239",
    "bucket": "BucketLarge-J",
    "title": "Late Preemption in a Power Outage",
    "scenario": "A utility asks: “If the first transformer had not failed, would the neighborhood still have lost power?” In the observed world, transformer A failed and an outage occurred.\n\nBut transformer B was already overheating due to the same heatwave. Even if transformer A had not failed, transformer B likely would have failed later that day, still producing an outage.",
    "variables": [
      "X = failure of transformer A",
      "Y = power outage occurs",
      "A = alternative sufficient cause"
    ],
    "annotations": {
      "Case ID": "J2-239",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Preemption",
      "Trap Subtype": "Late Preemption",
      "Difficulty": "Hard",
      "Subdomain": "Infrastructure Policy",
      "Causal Structure": "Two sufficient causes for Y; removing one may not change Y because the other would occur.",
      "Key Insight": "Observed causation does not imply counterfactual dependence when multiple sufficient causes can produce the outcome.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If we prevent failure of transformer A, does Y necessarily change?\nHow does late preemption complicate attributing Y to X in the counterfactual world?",
    "expected_analysis": "This is L3 Preemption (Late Preemption).\nThere are multiple sufficient causal paths to Y. Even if X occurs in the observed world, removing X in the counterfactual world may not change Y because an alternative cause would produce Y instead.\nAttribution requires modeling which cause would have fired in the absence of X (counterfactual dependence), not just observing that X happened.\nConclusion: The naive statement “X caused Y” is CONDITIONAL; it depends on whether Y counterfactually depends on X given competing causes.",
    "Hidden Timestamp": "If failure of transformer A were prevented, what alternative cause would have been most likely to produce Y, and would it occur earlier or later?",
    "Conditional Answers": "Answer if no other sufficient cause exists: Then preventing X would prevent Y, so X is a but-for cause.\nAnswer if alternative sufficient causes exist: Y may still occur without X; you need a structural model of competing causes and timing.\nAnswer if timing determines which cause preempts the other: Identify whether X was early/late relative to the alternative cause to assess counterfactual dependence.",
    "Wise Refusal": "I can’t decide whether X is a but-for cause without specifying competing causes and their timing; otherwise the counterfactual is underdetermined."
  },
  {
    "id": "T3-BucketJ-240",
    "bucket": "BucketLarge-J",
    "title": "Job Training, Confidence, and Employment: Fixing the Mediator",
    "scenario": "A policymaker asks: “Would employment increase if we offered job training, but kept participants’ confidence fixed at its current level?” They argue this isolates the ‘non-confidence’ part of training.\n\nTraining often changes participants’ confidence, and confidence itself affects job search intensity and employer interactions.",
    "variables": [
      "X = job training offer",
      "Y = employment probability",
      "M = self-confidence level (mediator)"
    ],
    "annotations": {
      "Case ID": "J2-240",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounder–Mediator Error",
      "Trap Subtype": "Mediator Fixing Error",
      "Difficulty": "Hard",
      "Subdomain": "Labor Economics",
      "Causal Structure": "X → M → Y; attempting to fix M while changing X creates an ill-defined counterfactual unless mediation assumptions are made explicit.",
      "Key Insight": "Holding a mediator fixed while changing the treatment can create an incoherent counterfactual world.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Is it valid to answer “What would Y be if we change X but keep M fixed?” when M is affected by X?\nWhat goes wrong when we try to fix the mediator to its observed value?",
    "expected_analysis": "This is Confounder–Mediator Error (Mediator Fixing Error).\nM lies on the causal pathway from X to Y. The proposed counterfactual “change X while holding M fixed” is generally not identifiable from standard observational data and can be logically inconsistent with how M would respond to X.\nFixing M can create a counterfactual world that is incompatible with the causal system, leading to biased or undefined effects.\nConclusion: The proposed counterfactual is INVALID unless you define a well-posed mediation estimand and justify strong assumptions (or perform a suitable intervention on M).",
    "Hidden Timestamp": "Does M occur after X and before Y, making it a mediator rather than a baseline confounder?",
    "Conditional Answers": "Answer if you want the total effect of X on Y: Do not fix M; compare Y under different X values.\nAnswer if you want a direct effect not through M: Use formal mediation analysis with explicit assumptions, or design an intervention that manipulates M.\nAnswer if M is actually pre-treatment: Then fixing M may be reasonable, but verify timing carefully.",
    "Wise Refusal": "I can’t answer the “hold M fixed” counterfactual without clarifying whether M is post-treatment and specifying the mediation estimand and assumptions."
  }
]