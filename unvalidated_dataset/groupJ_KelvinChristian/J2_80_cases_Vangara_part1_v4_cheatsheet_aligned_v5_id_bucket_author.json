[
  {
    "id": "T3-BucketJ-01",
    "bucket": "BucketLarge-J",
    "title": "The Department Graduation Rates",
    "scenario": "A university releases a report stating that Department A has a higher overall graduation rate than Department B. Administrators conclude that Department A’s curriculum is more effective and consider expanding it.\nHowever, a faculty member notes that when graduation rates are broken down by student preparedness level (high vs. low incoming GPA), Department B has higher graduation rates in both groups.\nThe discrepancy arises because Department A enrolls a much larger proportion of high-preparedness students, while Department B enrolls more low-preparedness students overall.",
    "variables": [
      "X = Department (A vs. B)",
      "Y = Graduation rate",
      "Z = Student preparedness level (high / low incoming GPA)"
    ],
    "annotations": {
      "Case ID": "J2-01",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Aggregation Bias",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Student preparedness (Z) affects graduation (Y) and differs in distribution across departments (X). Aggregating across Z reverses subgroup-level trends.",
      "Key Insight": "Aggregate performance metrics can contradict subgroup-level performance due to population composition.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the higher overall graduation rate of Department A imply that its curriculum is more effective than Department B’s?\nHow does conditioning on student preparedness level change the interpretation of the graduation data?",
    "expected_analysis": "This case requires associational reasoning and identification of Simpson’s Paradox.\nKey reasoning step: Recognize that student preparedness (Z) is a confounding variable that strongly influences graduation outcomes and is unevenly distributed across departments.\nHidden temporal structure: Student preparedness is determined prior to department enrollment, so it cannot be caused by the department.\nSubgroup analysis: When conditioning on Z, Department B outperforms Department A for both preparedness levels.\nFailure mode: Inferring causal superiority of a curriculum from aggregate outcomes without stratification.\nCorrect conclusion:\nThe claim that Department A’s curriculum is superior is INVALID. The aggregate association reflects differences in student composition rather than instructional quality.\nWise refusal:\nA valid assessment of curricular effectiveness would require controlling for preparedness or using a causal design (e.g., randomized assignment or matched cohorts).",
    "Hidden Timestamp": "Was Student preparedness level (high / low incoming GPA) determined before Department (A vs. B) was chosen, and could Student preparedness level (high / low incoming GPA) have influenced the choice of Department (A vs. B) before Graduation rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Department (A vs. B) on Graduation rate may be reversed because the mix of subgroups differs between Department (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Student preparedness level (high / low incoming GPA): Use the within-stratum differences (or a standardized effect). If Department (A vs. B) improves Graduation rate in each stratum, prefer Department (A vs. B) even if the aggregate looks worse.\nAnswer if Department (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Graduation rate by the key strata (e.g., Student preparedness level (high / low incoming GPA) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Department (A vs. B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-02",
    "bucket": "BucketLarge-J",
    "title": "The Crime Clearance Rates",
    "scenario": "A city publishes annual crime statistics showing that District East has a higher overall crime clearance rate than District West. City officials argue that policing strategies used in District East are more effective and consider expanding them citywide.\nHowever, analysts examining the data by crime severity (violent vs. non-violent offenses) find that District West has higher clearance rates for both categories.\nThe apparent contradiction arises because District East handles a much larger share of non-violent crimes, which are generally easier to solve, while District West deals disproportionately with violent crimes.",
    "variables": [
      "X = Police district (East vs. West)",
      "Y = Crime clearance rate",
      "Z = Crime severity (violent / non-violent)"
    ],
    "annotations": {
      "Case ID": "J2-02",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Imbalanced Group Composition",
      "Difficulty": "Medium",
      "Subdomain": "Criminology",
      "Causal Structure": "Crime severity (Z) affects clearance probability (Y) and differs in prevalence across districts (X). Aggregation across Z reverses subgroup-level performance.",
      "Key Insight": "Aggregate performance metrics can mask inferior performance within every relevant subgroup.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the higher overall clearance rate in District East imply more effective policing than in District West?\nWhy does stratifying by crime severity reverse the apparent ranking between the districts?",
    "expected_analysis": "This case requires associational reasoning and recognition of Simpson’s Paradox.\nKey reasoning step: Crime severity (Z) strongly influences clearance rates and is unevenly distributed across districts.\nHidden temporal structure: Crime severity is determined before police investigation begins and is not caused by district-level policing.\nSubgroup analysis: When violent and non-violent crimes are analyzed separately, District West outperforms District East in both categories.\nFailure mode: Interpreting aggregate clearance rates as evidence of policing effectiveness without accounting for case mix.\nCorrect conclusion:\nThe claim that District East’s policing strategy is superior is INVALID.\nWise refusal:\nValid comparisons require risk-adjusted or severity-adjusted clearance metrics rather than raw aggregates.",
    "Hidden Timestamp": "Was Crime severity (violent / non-violent) determined before Police district (East vs. West) was chosen, and could Crime severity (violent / non-violent) have influenced the choice of Police district (East vs. West) before Crime clearance rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Police district (East vs. West) on Crime clearance rate may be reversed because the mix of subgroups differs between Police district (East vs. West) arms.\nAnswer if you compare within strata after stratifying/standardizing by Crime severity (violent / non-violent): Use the within-stratum differences (or a standardized effect). If Police district (East vs. West) improves Crime clearance rate in each stratum, prefer Police district (East vs. West) even if the aggregate looks worse.\nAnswer if Police district (East vs. West) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Crime clearance rate by the key strata (e.g., Crime severity (violent / non-violent) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Police district (East vs. West) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-03",
    "bucket": "BucketLarge-J",
    "title": "The Online Course Completion Rates",
    "scenario": "An online education platform reports that Course X has a higher overall completion rate than Course Y. The platform promotes Course X as better designed and more engaging.\nHowever, when completion rates are examined by learner experience level (beginner vs. advanced), Course Y shows higher completion rates in both groups.\nThis occurs because Course X attracts a much larger proportion of advanced learners, who are more likely to complete any course regardless of design quality.",
    "variables": [
      "X = Course (X vs. Y)",
      "Y = Course completion rate",
      "Z = Learner experience level (beginner / advanced)"
    ],
    "annotations": {
      "Case ID": "J2-03",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Aggregation Bias",
      "Difficulty": "Easy",
      "Subdomain": "Education Technology",
      "Causal Structure": "Learner experience (Z) affects completion (Y) and differs across courses (X), producing a reversal when aggregated.",
      "Key Insight": "Differences in participant composition can dominate aggregate outcome measures.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does Course X’s higher overall completion rate demonstrate superior instructional design?\nHow does learner experience level alter the interpretation of completion rates?",
    "expected_analysis": "This case requires associational reasoning and identification of Simpson’s Paradox.\nKey reasoning step: Learner experience (Z) is a confounder that influences completion rates and is unevenly distributed across courses.\nHidden temporal structure: Learner experience is determined before course enrollment.\nSubgroup analysis: Course Y outperforms Course X among both beginners and advanced learners.\nFailure mode: Treating aggregate engagement metrics as causal indicators of quality.\nCorrect conclusion:\nThe claim that Course X is better designed is INVALID.\nWise refusal:\nCourse effectiveness should be evaluated within comparable learner groups or via randomized exposure.",
    "Hidden Timestamp": "Was Learner experience level (beginner / advanced) determined before Course (X vs. Y) was chosen, and could Learner experience level (beginner / advanced) have influenced the choice of Course (X vs. Y) before Course completion rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Course (X vs. Y) on Course completion rate may be reversed because the mix of subgroups differs between Course (X vs. Y) arms.\nAnswer if you compare within strata after stratifying/standardizing by Learner experience level (beginner / advanced): Use the within-stratum differences (or a standardized effect). If Course (X vs. Y) improves Course completion rate in each stratum, prefer Course (X vs. Y) even if the aggregate looks worse.\nAnswer if Course (X vs. Y) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Course completion rate by the key strata (e.g., Learner experience level (beginner / advanced) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Course (X vs. Y) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-04",
    "bucket": "BucketLarge-J",
    "title": "The Customer Satisfaction Survey",
    "scenario": "A company reports that 92% of customers are satisfied with its new subscription service, based on responses to a voluntary online satisfaction survey. Executives conclude that the service rollout has been a major success.\nHowever, internal data shows that only 15% of customers responded to the survey. Customer support logs indicate that dissatisfied users are more likely to cancel their subscriptions and disengage from company communications altogether.",
    "variables": [
      "X = Survey response participation",
      "Y = Reported customer satisfaction",
      "Z = Customer satisfaction status (satisfied / dissatisfied)"
    ],
    "annotations": {
      "Case ID": "J2-04",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Easy",
      "Subdomain": "Consumer Behavior",
      "Causal Structure": "Customer satisfaction (Z) affects likelihood of survey participation (X), and the outcome (Y) is measured only among respondents.",
      "Key Insight": "Observed satisfaction reflects who responds, not the true customer population.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the 92% satisfaction rate accurately represent overall customer sentiment?\nHow does voluntary participation affect the interpretation of survey results?",
    "expected_analysis": "This case requires associational reasoning and identification of selection bias.\nKey reasoning step: Survey participation (X) is not random; it depends on satisfaction status (Z).\nHidden temporal structure: Satisfaction exists before the decision to respond to the survey.\nFailure mode: Treating respondent statistics as representative of the full population.\nCorrect conclusion:\nThe claim that most customers are satisfied is INVALID.\nWise refusal:\nValid inference would require correcting for non-response or using randomized sampling methods.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Reported customer satisfaction occurred—and is selection related to Customer satisfaction status (satisfied / dissatisfied) or Reported customer satisfaction?",
    "Conditional Answers": "Answer if Survey response participation is randomly assigned: A difference in Reported customer satisfaction across Survey response participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Customer satisfaction status (satisfied / dissatisfied)): The Survey response participation vs not-Survey response participation difference in Reported customer satisfaction is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Customer satisfaction status (satisfied / dissatisfied)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Customer satisfaction status (satisfied / dissatisfied)); otherwise Survey response participation–Reported customer satisfaction differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-05",
    "bucket": "BucketLarge-J",
    "title": "The Startup Accelerator Success Rate",
    "scenario": "A startup accelerator advertises that 70% of companies in its program succeed, defining success as raising a Series A funding round within two years. Aspiring founders interpret this statistic as evidence that participation dramatically increases startup success.\nHowever, the accelerator accepts only a small fraction of applicants and explicitly selects founders with strong prior experience, existing traction, and elite educational backgrounds.",
    "variables": [
      "X = Accelerator participation",
      "Y = Startup success (Series A funding)",
      "Z = Founder quality / prior advantages"
    ],
    "annotations": {
      "Case ID": "J2-05",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Medium",
      "Subdomain": "Entrepreneurship",
      "Causal Structure": "Founder quality (Z) affects both selection into the accelerator (X) and startup success (Y).",
      "Key Insight": "High success rates may reflect who is admitted rather than program effectiveness.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the reported success rate imply that the accelerator causes startups to succeed?\nWhat role does the selection process play in shaping observed outcomes?",
    "expected_analysis": "This case requires associational reasoning and identification of selection bias.\nKey reasoning step: Accelerator participation is correlated with success because both are driven by founder quality (Z).\nHidden temporal structure: Founder characteristics exist before accelerator admission.\nFailure mode: Inferring causal impact from outcomes among a selectively chosen group.\nCorrect conclusion:\nThe causal claim about accelerator effectiveness is INVALID.\nWise refusal:\nEstimating causal impact would require comparing accepted startups to comparable rejected applicants.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Startup success (Series A funding) occurred—and is selection related to Founder quality / prior advantages or Startup success (Series A funding)?",
    "Conditional Answers": "Answer if Accelerator participation is randomly assigned: A difference in Startup success (Series A funding) across Accelerator participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Founder quality / prior advantages): The Accelerator participation vs not-Accelerator participation difference in Startup success (Series A funding) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Founder quality / prior advantages) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Founder quality / prior advantages); otherwise Accelerator participation–Startup success (Series A funding) differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-06",
    "bucket": "BucketLarge-J",
    "title": "The Employee Wellness Program",
    "scenario": "A company reports that employees enrolled in its voluntary wellness program take fewer sick days than employees who do not enroll. Management concludes that the wellness program improves employee health and considers expanding it.\nFurther examination reveals that employees who opt into the program tend to be healthier, more health-conscious, and more engaged with company initiatives even before the program begins.",
    "variables": [
      "X = Wellness program participation",
      "Y = Number of sick days taken",
      "Z = Baseline employee health / engagement"
    ],
    "annotations": {
      "Case ID": "J2-06",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Sampling-on-the-Outcome",
      "Difficulty": "Medium",
      "Subdomain": "Workplace Health",
      "Causal Structure": "Baseline health (Z) influences both program participation (X) and health outcomes (Y).",
      "Key Insight": "Voluntary programs often attract participants who would perform better regardless.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can the difference in sick days be attributed to the wellness program itself?\nWhy does voluntary enrollment complicate causal interpretation?",
    "expected_analysis": "This case requires associational reasoning and recognition of selection bias.\nKey reasoning step: Healthier employees are more likely to enroll, biasing comparisons.\nHidden temporal structure: Baseline health precedes enrollment.\nFailure mode: Mistaking correlation within a selected group for causal impact.\nCorrect conclusion:\nThe claim that the wellness program reduces sick days is INVALID.\nWise refusal:\nCausal evaluation would require random assignment or strong controls for baseline health.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Number of sick days taken occurred—and is selection related to Baseline employee health / engagement or Number of sick days taken?",
    "Conditional Answers": "Answer if Wellness program participation is randomly assigned: A difference in Number of sick days taken across Wellness program participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Baseline employee health / engagement): The Wellness program participation vs not-Wellness program participation difference in Number of sick days taken is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Baseline employee health / engagement) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Baseline employee health / engagement); otherwise Wellness program participation–Number of sick days taken differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-07",
    "bucket": "BucketLarge-J",
    "title": "The Wealthy State, Poor Voter",
    "scenario": "An analyst observes that states with higher average household income tend to vote for Party A in national elections. Based on this pattern, a commentator concludes that wealthier individuals are more likely to support Party A.\nHowever, individual-level polling data within states shows that higher-income individuals are actually more likely to support Party B, while lower-income individuals are more likely to support Party A.\nThe apparent contradiction arises because wealthier states differ from poorer states in urbanization, education levels, and industry composition.",
    "variables": [
      "X = State-level average income",
      "Y = Voting outcome (Party A vs. Party B)",
      "Z = Individual income level"
    ],
    "annotations": {
      "Case ID": "J2-07",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Socioeconomic",
      "Difficulty": "Medium",
      "Subdomain": "Political Science",
      "Causal Structure": "State-level income aggregates over heterogeneous individuals; group-level correlations do not reflect individual-level behavior.",
      "Key Insight": "Correlations observed at the group level cannot be assumed to hold at the individual level.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the voting pattern of wealthy states imply that wealthy individuals support Party A?\nWhy can state-level correlations mislead conclusions about individual behavior?",
    "expected_analysis": "This case requires associational reasoning and identification of the ecological fallacy.\nKey reasoning step: State-level income is an aggregate statistic that obscures within-state heterogeneity.\nHidden temporal structure: Individual income precedes voting decisions; aggregation occurs afterward.\nFailure mode: Inferring individual preferences from group-level data.\nCorrect conclusion:\nThe claim that wealthy individuals support Party A is INVALID.\nWise refusal:\nIndividual-level data is required to infer individual voting behavior; state averages are insufficient.",
    "Hidden Timestamp": "Is State-level average income measured at an aggregate level while Voting outcome (Party A vs. Party B) is an individual claim, and when/where does aggregation into Individual income level happen relative to measuring Voting outcome (Party A vs. Party B)?",
    "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how State-level average income relates to Voting outcome (Party A vs. Party B) for a person."
  },
  {
    "id": "T3-BucketJ-08",
    "bucket": "BucketLarge-J",
    "title": "Neighborhood Education and Student Achievement",
    "scenario": "A city education report shows that neighborhoods with higher average adult education levels have higher student test scores. Policymakers infer that students living in highly educated neighborhoods perform better academically.\nHowever, individual-level data reveals that students from less-educated households can perform as well as their peers when controlling for school quality and family support, regardless of neighborhood averages.\nThe neighborhood-level correlation reflects broader structural differences rather than individual household effects.",
    "variables": [
      "X = Neighborhood average education level",
      "Y = Student test scores",
      "Z = Household-level educational support"
    ],
    "annotations": {
      "Case ID": "J2-08",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Socioeconomic",
      "Difficulty": "Easy",
      "Subdomain": "Education Sociology",
      "Causal Structure": "Neighborhood averages mask variation across households; individual outcomes depend on household-level factors.",
      "Key Insight": "Group characteristics do not deterministically apply to individuals within the group.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can neighborhood education averages be used to predict individual student performance?\nWhy does this inference risk ecological fallacy?",
    "expected_analysis": "This case requires associational reasoning and recognition of the ecological fallacy.\nKey reasoning step: Neighborhood-level statistics aggregate diverse households.\nHidden temporal structure: Household characteristics precede both neighborhood averages and student outcomes.\nFailure mode: Assuming individuals inherit group-level attributes.\nCorrect conclusion:\nThe inference about individual performance is INVALID.\nWise refusal:\nIndividual-level causal claims require individual-level data.",
    "Hidden Timestamp": "Is Neighborhood average education level measured at an aggregate level while Student test scores is an individual claim, and when/where does aggregation into Household-level educational support happen relative to measuring Student test scores?",
    "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Neighborhood average education level relates to Student test scores for a person."
  },
  {
    "id": "T3-BucketJ-09",
    "bucket": "BucketLarge-J",
    "title": "National Happiness and Personal Well-Being",
    "scenario": "International surveys show that Country A ranks higher than Country B on average happiness scores. Commentators argue that citizens of Country A are happier in their daily lives.\nHowever, within both countries, individual happiness varies widely, and individual-level analysis shows that factors such as income security, health, and social relationships are stronger predictors of personal well-being than nationality.\nThe national ranking reflects aggregated survey responses rather than uniform individual experiences.",
    "variables": [
      "X = Country of residence",
      "Y = Average happiness score",
      "Z = Individual-level well-being determinants"
    ],
    "annotations": {
      "Case ID": "J2-09",
      "Pearl Level": "L1 (Association)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Socioeconomic",
      "Difficulty": "Easy",
      "Subdomain": "Sociology",
      "Causal Structure": "National averages collapse heterogeneous individual experiences into a single metric.",
      "Key Insight": "Country-level statistics cannot substitute for individual-level causal explanations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does a higher national happiness score imply that every individual is happier?\nWhy is this reasoning flawed?",
    "expected_analysis": "This case requires associational reasoning and identification of the ecological fallacy.\nKey reasoning step: National happiness scores are averages, not individual guarantees.\nHidden temporal structure: Individual experiences determine survey responses; aggregation follows.\nFailure mode: Treating aggregate indicators as individual truths.\nCorrect conclusion:\nThe claim that individuals in Country A are happier is INVALID.\nWise refusal:\nIndividual well-being must be assessed at the individual level, not inferred from national averages.",
    "Hidden Timestamp": "Is Country of residence measured at an aggregate level while Average happiness score is an individual claim, and when/where does aggregation into Individual-level well-being determinants happen relative to measuring Average happiness score?",
    "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Country of residence relates to Average happiness score for a person."
  },
  {
    "id": "T3-BucketJ-10",
    "bucket": "BucketLarge-J",
    "title": "The New Medical Treatment Rollout",
    "scenario": "A hospital system compares two treatments for a chronic condition: Treatment A (new) and Treatment B (standard). Hospital leadership reports that Treatment A has a higher overall recovery rate and decides to adopt it system-wide.\nHowever, when outcomes are analyzed separately for mild cases and severe cases, Treatment B has a higher recovery rate in both severity groups.\nThe apparent superiority of Treatment A arises because it is used far more often for mild cases, while Treatment B is disproportionately used for severe cases.",
    "variables": [
      "X = Treatment type (A vs. B)",
      "Y = Recovery outcome",
      "Z = Disease severity (mild / severe)"
    ],
    "annotations": {
      "Case ID": "J2-10",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Health Policy",
      "Causal Structure": "Disease severity (Z) influences both treatment assignment (X) and recovery (Y). Aggregating outcomes across severity levels reverses the treatment comparison.",
      "Key Insight": "An intervention may appear effective overall while being inferior within every relevant subgroup.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "If the hospital switches all patients to Treatment A, should it expect higher recovery rates?\nWhy does stratifying by disease severity reverse the apparent treatment effect?",
    "expected_analysis": "This case requires interventional reasoning and identification of Simpson’s Paradox under intervention.\nKey reasoning step: Disease severity (Z) is a pre-treatment variable that affects both treatment choice and outcomes.\nIntervention framing: The question concerns the effect of doing X = Treatment A for all patients.\nSubgroup analysis: Within both mild and severe cases, Treatment B yields better recovery outcomes.\nFailure mode: Inferring that an intervention is beneficial based on aggregate observational success rates.\nCorrect conclusion:\nAdopting Treatment A system-wide is INVALID based on the given evidence.\nWise refusal:\nA valid policy decision would require randomized assignment or severity-adjusted comparisons before intervention.",
    "Hidden Timestamp": "Was Disease severity (mild / severe) determined before Treatment type (A vs. B) was chosen, and could Disease severity (mild / severe) have influenced the choice of Treatment type (A vs. B) before Recovery outcome was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Treatment type (A vs. B) on Recovery outcome may be reversed because the mix of subgroups differs between Treatment type (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Disease severity (mild / severe): Use the within-stratum differences (or a standardized effect). If Treatment type (A vs. B) improves Recovery outcome in each stratum, prefer Treatment type (A vs. B) even if the aggregate looks worse.\nAnswer if Treatment type (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Recovery outcome by the key strata (e.g., Disease severity (mild / severe) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Treatment type (A vs. B) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-11",
    "bucket": "BucketLarge-J",
    "title": "The Class Size Reduction Program",
    "scenario": "A school district pilots a class size reduction program in several schools and reports that schools with smaller classes have higher average test scores. Based on these results, district officials propose expanding the program to all schools.\nHowever, when test scores are analyzed separately for high-performing schools and low-performing schools, schools without class size reductions outperform those with reductions in both categories.\nThis discrepancy arises because class size reductions were primarily implemented in already high-performing schools, while struggling schools retained larger classes.",
    "variables": [
      "X = Class size intervention (reduced vs. standard)",
      "Y = Student test scores",
      "Z = Baseline school performance (high / low)"
    ],
    "annotations": {
      "Case ID": "J2-11",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Education Policy",
      "Causal Structure": "Baseline school performance (Z) affects both likelihood of receiving the intervention (X) and student outcomes (Y), leading to aggregate reversal.",
      "Key Insight": "An intervention’s apparent success may reflect where it was implemented rather than its causal effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Would expanding class size reduction to all schools likely improve test scores?\nWhy does conditioning on baseline school performance change the interpretation?",
    "expected_analysis": "This case requires interventional reasoning and recognition of Simpson’s Paradox.\nKey reasoning step: Baseline performance (Z) is determined before intervention and strongly influences outcomes.\nIntervention framing: The policy question is about the effect of doing X = reducing class sizes everywhere.\nSubgroup analysis: Within both high- and low-performing schools, non-reduced classes perform better.\nFailure mode: Mistaking correlation between intervention presence and outcomes for causal effect.\nCorrect conclusion:\nThe proposal to expand the program is INVALID based on current evidence.\nWise refusal:\nRandomized rollout or matched comparisons are needed to estimate the true effect of class size reduction.",
    "Hidden Timestamp": "Was Baseline school performance (high / low) determined before Class size intervention (reduced vs. standard) was chosen, and could Baseline school performance (high / low) have influenced the choice of Class size intervention (reduced vs. standard) before Student test scores was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Class size intervention (reduced vs. standard) on Student test scores may be reversed because the mix of subgroups differs between Class size intervention (reduced vs. standard) arms.\nAnswer if you compare within strata after stratifying/standardizing by Baseline school performance (high / low): Use the within-stratum differences (or a standardized effect). If Class size intervention (reduced vs. standard) improves Student test scores in each stratum, prefer Class size intervention (reduced vs. standard) even if the aggregate looks worse.\nAnswer if Class size intervention (reduced vs. standard) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Student test scores by the key strata (e.g., Baseline school performance (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Class size intervention (reduced vs. standard) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-12",
    "bucket": "BucketLarge-J",
    "title": "The Productivity Training Initiative",
    "scenario": "A corporation pilots a productivity training program across several departments. Management reports that employees who received the training show higher average productivity scores than those who did not, and proposes mandatory rollout.\nWhen productivity is analyzed separately for junior and senior employees, however, untrained employees outperform trained employees in both groups.\nThe discrepancy arises because the training was offered primarily to senior employees, who are more productive on average regardless of training.",
    "variables": [
      "X = Training participation (yes / no)",
      "Y = Productivity score",
      "Z = Employee seniority (junior / senior)"
    ],
    "annotations": {
      "Case ID": "J2-12",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Seniority (Z) affects both training participation (X) and productivity (Y), reversing subgroup trends when aggregated.",
      "Key Insight": "Aggregate productivity gains may reflect employee mix rather than training impact.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Should the company mandate the training program for all employees?\nWhy does separating employees by seniority reverse the apparent effect?",
    "expected_analysis": "Seniority precedes training and productivity.\nWithin both junior and senior groups, training is associated with lower productivity.\nAggregate improvement is driven by overrepresentation of senior employees in the trained group.\nConclusion: Mandatory rollout is INVALID based on current evidence.\nWise refusal: A randomized or staggered rollout is required to estimate causal impact.",
    "Hidden Timestamp": "Was Employee seniority (junior / senior) determined before Training participation (yes / no) was chosen, and could Employee seniority (junior / senior) have influenced the choice of Training participation (yes / no) before Productivity score was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Training participation (yes / no) on Productivity score may be reversed because the mix of subgroups differs between Training participation (yes / no) arms.\nAnswer if you compare within strata after stratifying/standardizing by Employee seniority (junior / senior): Use the within-stratum differences (or a standardized effect). If Training participation (yes / no) improves Productivity score in each stratum, prefer Training participation (yes / no) even if the aggregate looks worse.\nAnswer if Training participation (yes / no) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Productivity score by the key strata (e.g., Employee seniority (junior / senior) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Training participation (yes / no) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-13",
    "bucket": "BucketLarge-J",
    "title": "The Digital Policing Tool",
    "scenario": "A police department introduces a predictive analytics tool and observes that precincts using the tool report higher crime resolution rates. City leaders plan to deploy the tool across all precincts.\nHowever, when outcomes are examined separately for high-crime and low-crime precincts, precincts without the tool show higher resolution rates in both categories.\nThe tool was initially deployed in low-crime precincts where resolution rates are naturally higher.",
    "variables": [
      "X = Predictive tool deployment",
      "Y = Crime resolution rate",
      "Z = Baseline precinct crime rate (high / low)"
    ],
    "annotations": {
      "Case ID": "J2-13",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Criminology",
      "Causal Structure": "Baseline crime rate (Z) affects both deployment (X) and outcomes (Y), leading to misleading aggregates.",
      "Key Insight": "Apparent intervention success may reflect selective deployment.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Will deploying the tool citywide improve crime resolution?\nWhy does conditioning on baseline crime rate change the conclusion?",
    "expected_analysis": "Baseline crime rate is pre-intervention.\nWithin both high- and low-crime precincts, non-tool precincts perform better.\nAggregate benefit reflects deployment pattern, not tool efficacy.\nConclusion: Citywide deployment is INVALID.\nWise refusal: Proper evaluation requires randomized precinct assignment.",
    "Hidden Timestamp": "Was Baseline precinct crime rate (high / low) determined before Predictive tool deployment was chosen, and could Baseline precinct crime rate (high / low) have influenced the choice of Predictive tool deployment before Crime resolution rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Predictive tool deployment on Crime resolution rate may be reversed because the mix of subgroups differs between Predictive tool deployment arms.\nAnswer if you compare within strata after stratifying/standardizing by Baseline precinct crime rate (high / low): Use the within-stratum differences (or a standardized effect). If Predictive tool deployment improves Crime resolution rate in each stratum, prefer Predictive tool deployment even if the aggregate looks worse.\nAnswer if Predictive tool deployment can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Crime resolution rate by the key strata (e.g., Baseline precinct crime rate (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Predictive tool deployment is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-14",
    "bucket": "BucketLarge-J",
    "title": "The Marketing Campaign Conversion Rates",
    "scenario": "A company runs a new marketing campaign and reports that customers exposed to it have a higher overall conversion rate. The campaign is labeled a success and expanded nationally.\nWhen analyzed by customer purchasing power (high vs. low), however, customers not exposed to the campaign convert at higher rates in both segments.\nThe campaign was targeted primarily at high-spending customers, inflating aggregate performance.",
    "variables": [
      "X = Campaign exposure",
      "Y = Purchase conversion",
      "Z = Customer purchasing power (high / low)"
    ],
    "annotations": {
      "Case ID": "J2-14",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Marketing Analytics",
      "Causal Structure": "Purchasing power (Z) influences both exposure (X) and conversion (Y), reversing subgroup effects.",
      "Key Insight": "Targeted interventions can distort aggregate success metrics.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the campaign cause higher conversion rates?\nHow does targeting affect causal interpretation?",
    "expected_analysis": "Purchasing power precedes exposure and conversion.\nWithin each purchasing group, the campaign underperforms.\nAggregate success is driven by customer mix.\nConclusion: Expansion decision is INVALID.\nWise refusal: A/B testing across comparable customers is required.",
    "Hidden Timestamp": "Was Customer purchasing power (high / low) determined before Campaign exposure was chosen, and could Customer purchasing power (high / low) have influenced the choice of Campaign exposure before Purchase conversion was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Campaign exposure on Purchase conversion may be reversed because the mix of subgroups differs between Campaign exposure arms.\nAnswer if you compare within strata after stratifying/standardizing by Customer purchasing power (high / low): Use the within-stratum differences (or a standardized effect). If Campaign exposure improves Purchase conversion in each stratum, prefer Campaign exposure even if the aggregate looks worse.\nAnswer if Campaign exposure can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Purchase conversion by the key strata (e.g., Customer purchasing power (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Campaign exposure is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-15",
    "bucket": "BucketLarge-J",
    "title": "The School Technology Grant",
    "scenario": "A government issues technology grants to schools and finds that grant-receiving schools show higher average student performance. Officials propose expanding funding.\nWhen performance is analyzed separately for urban and rural schools, non-recipient schools outperform recipients in both categories.\nThe grants were disproportionately awarded to urban schools, which already have stronger academic performance.",
    "variables": [
      "X = Grant receipt",
      "Y = Student performance",
      "Z = School location (urban / rural)"
    ],
    "annotations": {
      "Case ID": "J2-15",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Public Policy",
      "Causal Structure": "Location (Z) affects both grant allocation (X) and outcomes (Y), producing aggregate reversal.",
      "Key Insight": "Geographic imbalance can dominate apparent policy effects.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Should the grant program be expanded nationwide?\nWhy does stratifying by school location alter the conclusion?",
    "expected_analysis": "School location is fixed prior to grant allocation.\nWithin both urban and rural strata, non-recipient schools perform better.\nAggregate effect reflects allocation bias.\nConclusion: Expansion is INVALID.\nWise refusal: Causal evaluation requires randomized or needs-based allocation.\nL2-A (Simpson’s Paradox under intervention)",
    "Hidden Timestamp": "Was School location (urban / rural) determined before Grant receipt was chosen, and could School location (urban / rural) have influenced the choice of Grant receipt before Student performance was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Grant receipt on Student performance may be reversed because the mix of subgroups differs between Grant receipt arms.\nAnswer if you compare within strata after stratifying/standardizing by School location (urban / rural): Use the within-stratum differences (or a standardized effect). If Grant receipt improves Student performance in each stratum, prefer Grant receipt even if the aggregate looks worse.\nAnswer if Grant receipt can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Student performance by the key strata (e.g., School location (urban / rural) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Grant receipt is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-16",
    "bucket": "BucketLarge-J",
    "title": "The Hospital Staffing Reform",
    "scenario": "A hospital increases nurse-to-patient ratios in select wards and reports that wards with higher staffing levels have lower overall mortality rates. Administrators propose expanding the staffing reform hospital-wide.\nHowever, when mortality is examined separately for high-risk and low-risk patients, wards without the staffing increase show lower mortality in both groups.\nThe staffing reform was initially implemented in wards that treated a larger proportion of low-risk patients.",
    "variables": [
      "X = Staffing reform (higher vs. standard staffing)",
      "Y = Patient mortality",
      "Z = Patient risk level (high / low)"
    ],
    "annotations": {
      "Case ID": "J2-16",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Hard",
      "Subdomain": "Health Policy",
      "Causal Structure": "Patient risk (Z) affects both staffing assignment (X) and mortality (Y), reversing subgroup-level effects when aggregated.",
      "Key Insight": "Apparent benefits of an intervention may be driven by patient composition rather than causal impact.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Would expanding the staffing reform reduce mortality hospital-wide?\nWhy does stratifying by patient risk reverse the conclusion?",
    "expected_analysis": "Patient risk is determined prior to staffing decisions.\nWithin both risk strata, standard-staffed wards perform better.\nAggregate benefit reflects selective placement.\nConclusion: Expansion is INVALID.\nWise refusal: Randomized ward assignment or risk-adjusted analysis is required.",
    "Hidden Timestamp": "Was Patient risk level (high / low) determined before Staffing reform (higher vs. standard staffing) was chosen, and could Patient risk level (high / low) have influenced the choice of Staffing reform (higher vs. standard staffing) before Patient mortality was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Staffing reform (higher vs. standard staffing) on Patient mortality may be reversed because the mix of subgroups differs between Staffing reform (higher vs. standard staffing) arms.\nAnswer if you compare within strata after stratifying/standardizing by Patient risk level (high / low): Use the within-stratum differences (or a standardized effect). If Staffing reform (higher vs. standard staffing) improves Patient mortality in each stratum, prefer Staffing reform (higher vs. standard staffing) even if the aggregate looks worse.\nAnswer if Staffing reform (higher vs. standard staffing) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Patient mortality by the key strata (e.g., Patient risk level (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Staffing reform (higher vs. standard staffing) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-17",
    "bucket": "BucketLarge-J",
    "title": "The University Tutoring Program",
    "scenario": "A university offers an optional tutoring program and finds that participants have higher overall course pass rates. Administrators consider making tutoring mandatory.\nWhen outcomes are analyzed separately for introductory and advanced courses, non-participants outperform participants in both categories.\nTutoring was most commonly used by students enrolled in advanced courses, which already have higher pass rates.",
    "variables": [
      "X = Tutoring participation",
      "Y = Course pass rate",
      "Z = Course level (introductory / advanced)"
    ],
    "annotations": {
      "Case ID": "J2-17",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Higher Education",
      "Causal Structure": "Course level (Z) influences both tutoring use (X) and pass rates (Y), producing aggregate reversal.",
      "Key Insight": "Participation patterns can dominate aggregate intervention outcomes.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Should tutoring be made mandatory for all students?\nHow does course level affect interpretation?",
    "expected_analysis": "Course level is fixed prior to tutoring.\nWithin both course strata, tutoring underperforms.\nAggregate success reflects enrollment mix.\nConclusion: Mandatory tutoring is INVALID.\nWise refusal: Causal effect requires random assignment within courses.",
    "Hidden Timestamp": "Was Course level (introductory / advanced) determined before Tutoring participation was chosen, and could Course level (introductory / advanced) have influenced the choice of Tutoring participation before Course pass rate was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Tutoring participation on Course pass rate may be reversed because the mix of subgroups differs between Tutoring participation arms.\nAnswer if you compare within strata after stratifying/standardizing by Course level (introductory / advanced): Use the within-stratum differences (or a standardized effect). If Tutoring participation improves Course pass rate in each stratum, prefer Tutoring participation even if the aggregate looks worse.\nAnswer if Tutoring participation can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Course pass rate by the key strata (e.g., Course level (introductory / advanced) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Tutoring participation is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-18",
    "bucket": "BucketLarge-J",
    "title": "The Remote Work Policy",
    "scenario": "A firm allows employees to opt into remote work and reports that remote workers have higher average productivity. Leadership considers mandating remote work for all eligible roles.\nHowever, when productivity is examined separately for technical and non-technical roles, in-office employees outperform remote employees in both categories.\nRemote work was disproportionately adopted by technical staff, who are more productive on average.",
    "variables": [
      "X = Work arrangement (remote / in-office)",
      "Y = Productivity",
      "Z = Job role type (technical / non-technical)"
    ],
    "annotations": {
      "Case ID": "J2-18",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Simpson’s Paradox",
      "Trap Subtype": "Stratified Intervention Reversal",
      "Difficulty": "Medium",
      "Subdomain": "Workplace Policy",
      "Causal Structure": "Job role (Z) affects both remote eligibility (X) and productivity (Y), reversing subgroup effects when aggregated.",
      "Key Insight": "Apparent productivity gains may reflect workforce composition.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Would mandating remote work increase productivity?\nWhy does role stratification change the conclusion?",
    "expected_analysis": "Role type precedes work arrangement.\nWithin both role categories, in-office work performs better.\nAggregate effect is misleading.\nConclusion: Mandate is INVALID.\nWise refusal: Policy evaluation requires role-stratified experimentation.\n✅ L2-A COMPLETE\nL2-A cases completed: 9 / 9\nSimpson’s under intervention fully covered\nL2-B (Composition Effects)\n(Population changes, not treatment effects)",
    "Hidden Timestamp": "Was Job role type (technical / non-technical) determined before Work arrangement (remote / in-office) was chosen, and could Job role type (technical / non-technical) have influenced the choice of Work arrangement (remote / in-office) before Productivity was measured?",
    "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Work arrangement (remote / in-office) on Productivity may be reversed because the mix of subgroups differs between Work arrangement (remote / in-office) arms.\nAnswer if you compare within strata after stratifying/standardizing by Job role type (technical / non-technical): Use the within-stratum differences (or a standardized effect). If Work arrangement (remote / in-office) improves Productivity in each stratum, prefer Work arrangement (remote / in-office) even if the aggregate looks worse.\nAnswer if Work arrangement (remote / in-office) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Please report Productivity by the key strata (e.g., Job role type (technical / non-technical) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Work arrangement (remote / in-office) is unevenly applied across strata."
  },
  {
    "id": "T3-BucketJ-19",
    "bucket": "BucketLarge-J",
    "title": "The Gentrification Income Report",
    "scenario": "A city reports that after a neighborhood redevelopment project, average household income in the area increased by 40%. Officials conclude that redevelopment improved residents’ economic well-being.\nFurther analysis shows that many original low-income residents moved out, while higher-income residents moved in.",
    "variables": [
      "X = Neighborhood redevelopment",
      "Y = Average household income",
      "Z = Resident population composition"
    ],
    "annotations": {
      "Case ID": "J2-19",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Easy",
      "Subdomain": "Urban Economics",
      "Causal Structure": "Redevelopment changes who lives in the neighborhood (Z), which alters average income (Y) without improving original residents’ outcomes.",
      "Key Insight": "Aggregate improvement can reflect population turnover, not individual benefit.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Did redevelopment make original residents wealthier?\nHow does population change affect income statistics?",
    "expected_analysis": "Redevelopment affects population composition.\nAverage income rises despite no improvement for original residents.\nConclusion: Claim of resident enrichment is INVALID.\nWise refusal: Individual-level longitudinal data is required.",
    "Hidden Timestamp": "Did the intervention/change in Neighborhood redevelopment alter the composition (Resident population composition) of who is counted before Average household income was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Average household income after changing Neighborhood redevelopment can reflect a real outcome shift.\nAnswer if Neighborhood redevelopment changes who is counted via Resident population composition: The aggregate Average household income can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Average household income may be moving because the denominator/population changed after Neighborhood redevelopment via composition variable Resident population composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-20",
    "bucket": "BucketLarge-J",
    "title": "The Crime Reduction Initiative",
    "scenario": "A city introduces a crime reduction initiative in a high-crime area. One year later, the neighborhood’s crime rate drops significantly. Officials claim the initiative was successful.\nSubsequent analysis reveals that many high-risk residents relocated during the same period due to rising housing costs, while lower-risk residents moved in.",
    "variables": [
      "X = Crime reduction initiative",
      "Y = Neighborhood crime rate",
      "Z = Population turnover"
    ],
    "annotations": {
      "Case ID": "J2-20",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Criminology",
      "Causal Structure": "Population change (Z) drives crime reduction (Y), independent of the intervention.",
      "Key Insight": "Crime rates can fall due to who leaves, not what policies change.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can the crime reduction be attributed to the initiative?\nWhy does population displacement complicate causal inference?",
    "expected_analysis": "Initiative coincides with population change.\nReduced crime reflects altered risk pool.\nConclusion: Attribution to the initiative is INVALID.\nWise refusal: Evaluation requires tracking crime risk among original residents.\nL2-B (Composition Effects)",
    "Hidden Timestamp": "Did the intervention/change in Crime reduction initiative alter the composition (Population turnover) of who is counted before Neighborhood crime rate was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Neighborhood crime rate after changing Crime reduction initiative can reflect a real outcome shift.\nAnswer if Crime reduction initiative changes who is counted via Population turnover: The aggregate Neighborhood crime rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Neighborhood crime rate may be moving because the denominator/population changed after Crime reduction initiative via composition variable Population turnover. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-21",
    "bucket": "BucketLarge-J",
    "title": "The School Test Score Improvement",
    "scenario": "A school district reports that after implementing a new admissions lottery, the average test scores across district schools increased. Officials conclude that the lottery policy improved academic performance.\nFurther analysis shows that the lottery led to a redistribution of students: higher-performing students concentrated in certain schools, while lower-performing students were reassigned elsewhere.",
    "variables": [
      "X = Admissions lottery policy",
      "Y = Average test scores",
      "Z = Student distribution across schools"
    ],
    "annotations": {
      "Case ID": "J2-21",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "The lottery changes student composition (Z), altering school averages (Y) without changing individual achievement.",
      "Key Insight": "Improved averages can result from reshuffling students, not learning gains.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Did the lottery policy improve individual student performance?\nHow does redistribution affect average test scores?",
    "expected_analysis": "Policy affects student allocation, not instruction.\nAverage scores rise due to compositional changes.\nConclusion: Individual performance improvement claim is INVALID.\nWise refusal: Longitudinal student-level data is required.",
    "Hidden Timestamp": "Did the intervention/change in Admissions lottery policy alter the composition (Student distribution across schools) of who is counted before Average test scores was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Average test scores after changing Admissions lottery policy can reflect a real outcome shift.\nAnswer if Admissions lottery policy changes who is counted via Student distribution across schools: The aggregate Average test scores can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Average test scores may be moving because the denominator/population changed after Admissions lottery policy via composition variable Student distribution across schools. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-22",
    "bucket": "BucketLarge-J",
    "title": "The Workforce Diversity Metric",
    "scenario": "A corporation announces that after a diversity initiative, the percentage of women in leadership roles increased. Leadership attributes this to improved promotion practices.\nFurther inspection reveals that the increase is driven largely by hiring women directly into senior roles, while promotion rates within the firm remain unchanged.",
    "variables": [
      "X = Diversity initiative",
      "Y = Share of women in leadership",
      "Z = Entry vs. promotion composition"
    ],
    "annotations": {
      "Case ID": "J2-22",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Easy",
      "Subdomain": "Labor & Organizations",
      "Causal Structure": "Leadership composition (Y) changes due to hiring mix (Z), not internal advancement.",
      "Key Insight": "Stock metrics can change without flow changes.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Did the initiative improve promotion equity?\nWhy does leadership composition change without internal progress?",
    "expected_analysis": "Initiative changes who enters leadership.\nInternal dynamics remain unchanged.\nConclusion: Promotion improvement claim is INVALID.\nWise refusal: Promotion-rate analysis is required.",
    "Hidden Timestamp": "Did the intervention/change in Diversity initiative alter the composition (Entry vs. promotion composition) of who is counted before Share of women in leadership was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Share of women in leadership after changing Diversity initiative can reflect a real outcome shift.\nAnswer if Diversity initiative changes who is counted via Entry vs. promotion composition: The aggregate Share of women in leadership can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Share of women in leadership may be moving because the denominator/population changed after Diversity initiative via composition variable Entry vs. promotion composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-23",
    "bucket": "BucketLarge-J",
    "title": "The Hospital Readmission Decline",
    "scenario": "A hospital reports a decline in 30-day readmission rates after introducing a discharge planning program. Administrators credit the program with improving patient outcomes.\nLater analysis shows that more high-risk patients were transferred to long-term care facilities rather than discharged home during the same period.",
    "variables": [
      "X = Discharge planning program",
      "Y = Readmission rate",
      "Z = Discharged patient risk profile"
    ],
    "annotations": {
      "Case ID": "J2-23",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Healthcare Management",
      "Causal Structure": "Patient mix (Z) changes the denominator for readmissions (Y).",
      "Key Insight": "Outcome metrics can improve by excluding high-risk cases.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the program reduce readmissions for comparable patients?\nHow does discharge selection affect reported rates?",
    "expected_analysis": "High-risk patients are removed from measurement pool.\nReadmission rate declines mechanically.\nConclusion: Program effectiveness claim is INVALID.\nWise refusal: Risk-adjusted readmission analysis is required.",
    "Hidden Timestamp": "Did the intervention/change in Discharge planning program alter the composition (Discharged patient risk profile) of who is counted before Readmission rate was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Readmission rate after changing Discharge planning program can reflect a real outcome shift.\nAnswer if Discharge planning program changes who is counted via Discharged patient risk profile: The aggregate Readmission rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Readmission rate may be moving because the denominator/population changed after Discharge planning program via composition variable Discharged patient risk profile. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-24",
    "bucket": "BucketLarge-J",
    "title": "The Immigration Employment Statistic",
    "scenario": "A city reports that after an influx of immigrants, the unemployment rate declined. Officials claim immigration strengthened the local labor market.\nCloser inspection reveals that immigrants were more likely to be employed upon arrival, while some unemployed residents moved away due to rising rents.",
    "variables": [
      "X = Immigration influx",
      "Y = Unemployment rate",
      "Z = Labor force composition"
    ],
    "annotations": {
      "Case ID": "J2-24",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Employment statistics change due to who enters and exits the labor force.",
      "Key Insight": "Labor metrics are sensitive to population flows.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Did immigration create jobs for existing residents?\nWhy can unemployment fall without job creation?",
    "expected_analysis": "Employment rate reflects labor pool composition.\nDecline not attributable to job growth.\nConclusion: Job creation claim is INVALID.\nWise refusal: Separate employment effects by resident status.",
    "Hidden Timestamp": "Did the intervention/change in Immigration influx alter the composition (Labor force composition) of who is counted before Unemployment rate was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Unemployment rate after changing Immigration influx can reflect a real outcome shift.\nAnswer if Immigration influx changes who is counted via Labor force composition: The aggregate Unemployment rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Unemployment rate may be moving because the denominator/population changed after Immigration influx via composition variable Labor force composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-25",
    "bucket": "BucketLarge-J",
    "title": "The Public Transit Ridership Surge",
    "scenario": "After expanding a public transit line, a city reports a 30% increase in ridership and claims the expansion reduced car usage.\nFurther analysis shows that many riders were former bus users whose routes were discontinued, forcing them onto the new line.",
    "variables": [
      "X = Transit line expansion",
      "Y = Ridership counts",
      "Z = Mode substitution patterns"
    ],
    "annotations": {
      "Case ID": "J2-25",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Easy",
      "Subdomain": "Transportation Policy",
      "Causal Structure": "Ridership growth reflects reclassification of existing users.",
      "Key Insight": "Usage metrics can rise without behavior change.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Did the expansion reduce car travel?\nHow does reclassification inflate ridership numbers?",
    "expected_analysis": "Riders shift between transit categories.\nNo evidence of reduced car usage.\nConclusion: Car reduction claim is INVALID.\nWise refusal: Mode-shift analysis is required.",
    "Hidden Timestamp": "Did the intervention/change in Transit line expansion alter the composition (Mode substitution patterns) of who is counted before Ridership counts was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Ridership counts after changing Transit line expansion can reflect a real outcome shift.\nAnswer if Transit line expansion changes who is counted via Mode substitution patterns: The aggregate Ridership counts can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Ridership counts may be moving because the denominator/population changed after Transit line expansion via composition variable Mode substitution patterns. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-26",
    "bucket": "BucketLarge-J",
    "title": "The University Ranking Improvement",
    "scenario": "A university rises significantly in national rankings after launching a selective honors program. Administrators claim that the program improved overall academic quality.\nFurther analysis shows that the university admitted a smaller cohort of highly qualified honors students while reducing enrollment elsewhere, without changing instructional practices for existing students.",
    "variables": [
      "X = Honors program introduction",
      "Y = University ranking metrics",
      "Z = Student intake composition"
    ],
    "annotations": {
      "Case ID": "J2-26",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Higher Education Policy",
      "Causal Structure": "Rankings improve because the student body composition (Z) changes, not because educational quality improves.",
      "Key Insight": "Institutional metrics can improve through selective enrollment rather than better outcomes.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Did the honors program improve education for existing students?\nWhy can rankings rise without instructional change?",
    "expected_analysis": "Program changes who is admitted.\nRankings reflect input quality, not value added.\nConclusion: Educational improvement claim is INVALID.\nWise refusal: Value-added measures are required.",
    "Hidden Timestamp": "Did the intervention/change in Honors program introduction alter the composition (Student intake composition) of who is counted before University ranking metrics was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in University ranking metrics after changing Honors program introduction can reflect a real outcome shift.\nAnswer if Honors program introduction changes who is counted via Student intake composition: The aggregate University ranking metrics can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for University ranking metrics may be moving because the denominator/population changed after Honors program introduction via composition variable Student intake composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-27",
    "bucket": "BucketLarge-J",
    "title": "The Environmental Emissions Drop",
    "scenario": "A city reports a decline in average per-capita carbon emissions after adopting a climate action plan. Officials credit the plan with reducing emissions.\nFurther investigation reveals that several energy-intensive factories closed during the same period, relocating to neighboring regions.",
    "variables": [
      "X = Climate action plan",
      "Y = Per-capita carbon emissions",
      "Z = Industrial activity composition"
    ],
    "annotations": {
      "Case ID": "J2-27",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Environmental Policy",
      "Causal Structure": "Emissions fall because polluting activity exits the measurement region.",
      "Key Insight": "Environmental metrics can improve via relocation, not reduction.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Did the policy reduce emissions behavior?\nWhy does displacement complicate attribution?",
    "expected_analysis": "Emissions decline reflects industrial exit.\nNo evidence of cleaner production.\nConclusion: Policy effectiveness claim is INVALID.\nWise refusal: Consumption-based emissions accounting is required.\n✅ L2-B COMPLETE\nL2-B cases: 9 / 9\nL2-C (Selection into Treatment)\n(People who receive the intervention differ systematically)",
    "Hidden Timestamp": "Did the intervention/change in Climate action plan alter the composition (Industrial activity composition) of who is counted before Per-capita carbon emissions was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Per-capita carbon emissions after changing Climate action plan can reflect a real outcome shift.\nAnswer if Climate action plan changes who is counted via Industrial activity composition: The aggregate Per-capita carbon emissions can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Per-capita carbon emissions may be moving because the denominator/population changed after Climate action plan via composition variable Industrial activity composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-28",
    "bucket": "BucketLarge-J",
    "title": "The Job Training Program",
    "scenario": "A government reports that participants in a job training program have higher post-program employment rates than non-participants. Officials conclude the program is effective.\nHowever, enrollment in the program is voluntary, and participants are more motivated and actively job-seeking than non-participants even before enrollment.",
    "variables": [
      "X = Program participation",
      "Y = Employment outcome",
      "Z = Job-seeking motivation"
    ],
    "annotations": {
      "Case ID": "J2-28",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Labor Policy",
      "Causal Structure": "Motivation (Z) affects both participation (X) and employment (Y).",
      "Key Insight": "Participants would have better outcomes regardless of treatment.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does higher employment among participants imply program effectiveness?\nWhy does voluntary enrollment bias inference?",
    "expected_analysis": "Motivation precedes treatment.\nEmployment differences reflect selection.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Random assignment is required.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Employment outcome occurred—and is selection related to Job-seeking motivation or Employment outcome?",
    "Conditional Answers": "Answer if Program participation is randomly assigned: A difference in Employment outcome across Program participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Job-seeking motivation): The Program participation vs not-Program participation difference in Employment outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Job-seeking motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Job-seeking motivation); otherwise Program participation–Employment outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-29",
    "bucket": "BucketLarge-J",
    "title": "The Preventive Health Screening",
    "scenario": "Patients who undergo preventive health screenings have lower mortality rates than those who do not. Health officials promote screenings as life-saving.\nHowever, individuals who choose screenings tend to be healthier, wealthier, and more health-conscious than those who decline.",
    "variables": [
      "X = Screening participation",
      "Y = Mortality",
      "Z = Baseline health behavior"
    ],
    "annotations": {
      "Case ID": "J2-29",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Medium",
      "Subdomain": "Public Health",
      "Causal Structure": "Health behavior (Z) influences both screening (X) and mortality (Y).",
      "Key Insight": "Observed benefit may reflect who chooses screening.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does screening causally reduce mortality?\nWhy are screened and unscreened populations incomparable?",
    "expected_analysis": "Health behavior precedes screening.\nMortality differences are confounded.\nConclusion: Causal claim is INVALID.\nWise refusal: Controlled trials or instrumental variables are needed.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Mortality occurred—and is selection related to Baseline health behavior or Mortality?",
    "Conditional Answers": "Answer if Screening participation is randomly assigned: A difference in Mortality across Screening participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Baseline health behavior): The Screening participation vs not-Screening participation difference in Mortality is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Baseline health behavior) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Baseline health behavior); otherwise Screening participation–Mortality differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-30",
    "bucket": "BucketLarge-J",
    "title": "The Leadership Development Program",
    "scenario": "Employees who attend a leadership development program are promoted at higher rates than those who do not. Management credits the program with improving leadership skills.\nHowever, attendance is limited to employees already identified as high-potential by senior managers.",
    "variables": [
      "X = Program attendance",
      "Y = Promotion outcome",
      "Z = Pre-existing leadership potential"
    ],
    "annotations": {
      "Case ID": "J2-30",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Leadership potential (Z) affects both selection into the program (X) and promotion (Y).",
      "Key Insight": "Programs targeting high performers inflate apparent effectiveness.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the program cause higher promotion rates?\nHow does pre-selection bias evaluation?",
    "expected_analysis": "Selection precedes treatment.\nPromotions reflect prior assessments.\nConclusion: Program impact claim is INVALID.\nWise refusal: Compare with matched non-selected employees.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Promotion outcome occurred—and is selection related to Pre-existing leadership potential or Promotion outcome?",
    "Conditional Answers": "Answer if Program attendance is randomly assigned: A difference in Promotion outcome across Program attendance groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Pre-existing leadership potential): The Program attendance vs not-Program attendance difference in Promotion outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Pre-existing leadership potential) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Pre-existing leadership potential); otherwise Program attendance–Promotion outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-31",
    "bucket": "BucketLarge-J",
    "title": "The College Scholarship Program",
    "scenario": "Students who receive a merit-based scholarship graduate at higher rates than those who do not. University administrators conclude that the scholarship improves student success.\nHowever, scholarship recipients are selected based on prior academic achievement, strong recommendations, and demonstrated motivation.",
    "variables": [
      "X = Scholarship receipt",
      "Y = Graduation outcome",
      "Z = Prior academic achievement and motivation"
    ],
    "annotations": {
      "Case ID": "J2-31",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Time-varying Confounding",
      "Difficulty": "Easy",
      "Subdomain": "Higher Education",
      "Causal Structure": "Prior achievement (Z) affects both scholarship receipt (X) and graduation (Y).",
      "Key Insight": "Scholarships may select strong students rather than create success.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does receiving the scholarship cause higher graduation rates?\nWhy does merit-based selection bias the comparison?",
    "expected_analysis": "Academic strength precedes scholarship.\nGraduation differences reflect selection.\nConclusion: Causal claim is INVALID.\nWise refusal: Randomized or need-based assignment is required.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Graduation outcome occurred—and is selection related to Prior academic achievement and motivation or Graduation outcome?",
    "Conditional Answers": "Answer if Scholarship receipt is randomly assigned: A difference in Graduation outcome across Scholarship receipt groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Prior academic achievement and motivation): The Scholarship receipt vs not-Scholarship receipt difference in Graduation outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Prior academic achievement and motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Prior academic achievement and motivation); otherwise Scholarship receipt–Graduation outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-32",
    "bucket": "BucketLarge-J",
    "title": "The Voluntary Online Learning Platform",
    "scenario": "Users who enroll in a voluntary online learning platform show greater skill improvement than non-users. Platform developers claim the platform is effective.\nHowever, enrollment is optional, and users are typically more self-motivated and already interested in skill development.",
    "variables": [
      "X = Platform enrollment",
      "Y = Skill improvement",
      "Z = Learner motivation"
    ],
    "annotations": {
      "Case ID": "J2-32",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Education Technology",
      "Causal Structure": "Motivation (Z) influences both enrollment (X) and learning outcomes (Y).",
      "Key Insight": "Voluntary participation inflates perceived effectiveness.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does platform usage cause skill gains?\nWhy are users and non-users incomparable?",
    "expected_analysis": "Motivation predates platform use.\nGains reflect selection bias.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Randomized access or encouragement design is needed.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Skill improvement occurred—and is selection related to Learner motivation or Skill improvement?",
    "Conditional Answers": "Answer if Platform enrollment is randomly assigned: A difference in Skill improvement across Platform enrollment groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Learner motivation): The Platform enrollment vs not-Platform enrollment difference in Skill improvement is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Learner motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Learner motivation); otherwise Platform enrollment–Skill improvement differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-33",
    "bucket": "BucketLarge-J",
    "title": "The Early Childhood Education Program",
    "scenario": "Children enrolled in a voluntary early childhood education program perform better academically later in life. Policymakers cite this as evidence of program success.\nHowever, parents who enroll their children tend to be more engaged, have higher educational attainment, and provide more academic support at home.",
    "variables": [
      "X = Program enrollment",
      "Y = Later academic performance",
      "Z = Parental engagement"
    ],
    "annotations": {
      "Case ID": "J2-33",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Parental engagement (Z) affects both enrollment (X) and child outcomes (Y).",
      "Key Insight": "Family background confounds program evaluation.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can better outcomes be attributed to the program itself?\nHow does parental choice bias evaluation?",
    "expected_analysis": "Engagement precedes enrollment.\nOutcomes reflect family inputs.\nConclusion: Program impact claim is INVALID.\nWise refusal: Randomized access or sibling comparisons are required.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Later academic performance occurred—and is selection related to Parental engagement or Later academic performance?",
    "Conditional Answers": "Answer if Program enrollment is randomly assigned: A difference in Later academic performance across Program enrollment groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Parental engagement): The Program enrollment vs not-Program enrollment difference in Later academic performance is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Parental engagement) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Parental engagement); otherwise Program enrollment–Later academic performance differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-34",
    "bucket": "BucketLarge-J",
    "title": "The Fitness App Effectiveness Claim",
    "scenario": "Users of a fitness tracking app lose more weight than non-users. The app is marketed as effective for weight loss.\nHowever, app users are typically more health-conscious and already motivated to exercise.",
    "variables": [
      "X = App usage",
      "Y = Weight loss",
      "Z = Health motivation"
    ],
    "annotations": {
      "Case ID": "J2-34",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Easy",
      "Subdomain": "Digital Health",
      "Causal Structure": "Motivation (Z) influences both app adoption (X) and outcomes (Y).",
      "Key Insight": "Technology uptake selects motivated users.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does app usage cause weight loss?\nWhy is comparing users and non-users misleading?",
    "expected_analysis": "Motivation predates app usage.\nWeight loss reflects user characteristics.\nConclusion: App effectiveness claim is INVALID.\nWise refusal: Randomized trials are needed.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Weight loss occurred—and is selection related to Health motivation or Weight loss?",
    "Conditional Answers": "Answer if App usage is randomly assigned: A difference in Weight loss across App usage groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Health motivation): The App usage vs not-App usage difference in Weight loss is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Health motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Health motivation); otherwise App usage–Weight loss differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-35",
    "bucket": "BucketLarge-J",
    "title": "The Mentorship Program Outcomes",
    "scenario": "Employees who participate in a mentorship program receive higher performance ratings than non-participants. Managers argue that mentoring improves performance.\nHowever, mentors are assigned to employees already identified as high performers.",
    "variables": [
      "X = Mentorship participation",
      "Y = Performance rating",
      "Z = Prior performance level"
    ],
    "annotations": {
      "Case ID": "J2-35",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Unblocked Backdoor",
      "Difficulty": "Medium",
      "Subdomain": "Human Resources",
      "Causal Structure": "Prior performance (Z) influences mentorship assignment (X) and ratings (Y).",
      "Key Insight": "Programs targeting strong performers exaggerate impact.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does mentorship cause higher performance?\nHow does selection bias affect evaluation?",
    "expected_analysis": "Performance precedes mentoring.\nRatings reflect prior ability.\nConclusion: Mentorship impact claim is INVALID.\nWise refusal: Compare with matched non-selected employees.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Performance rating occurred—and is selection related to Prior performance level or Performance rating?",
    "Conditional Answers": "Answer if Mentorship participation is randomly assigned: A difference in Performance rating across Mentorship participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Prior performance level): The Mentorship participation vs not-Mentorship participation difference in Performance rating is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Prior performance level) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Prior performance level); otherwise Mentorship participation–Performance rating differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-36",
    "bucket": "BucketLarge-J",
    "title": "The Therapy Program Success Rate",
    "scenario": "Patients who complete a voluntary therapy program show better mental health outcomes than those who drop out or never enroll. Providers claim the therapy is effective.\nHowever, patients who complete therapy are those who respond early or have fewer barriers to participation.",
    "variables": [
      "X = Therapy completion",
      "Y = Mental health outcome",
      "Z = Treatment adherence capacity"
    ],
    "annotations": {
      "Case ID": "J2-36",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Hard",
      "Subdomain": "Mental Health",
      "Causal Structure": "Adherence capacity (Z) influences both completion (X) and outcomes (Y).",
      "Key Insight": "Conditioning on completion induces selection bias.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does completing therapy cause better outcomes?\nWhy is completion a biased conditioning variable?",
    "expected_analysis": "Adherence differences precede outcomes.\nCompletion-based comparison is biased.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Intention-to-treat analysis is required.\nL2-D (Collider Bias) — COMPLETE (9 cases)",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Mental health outcome occurred—and is selection related to Treatment adherence capacity or Mental health outcome?",
    "Conditional Answers": "Answer if Therapy completion is randomly assigned: A difference in Mental health outcome across Therapy completion groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Treatment adherence capacity): The Therapy completion vs not-Therapy completion difference in Mental health outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Treatment adherence capacity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Treatment adherence capacity); otherwise Therapy completion–Mental health outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-37",
    "bucket": "BucketLarge-J",
    "title": "The Hospital Survival Paradox",
    "scenario": "A hospital studies patients admitted with a severe illness and finds that smokers have lower mortality rates than non-smokers among admitted patients. Administrators speculate that smoking may be protective.\nHowever, hospital admission occurs only for patients who become seriously ill. Smoking and other health conditions both increase the likelihood of severe illness and admission.",
    "variables": [
      "X = Smoking status",
      "Y = Mortality",
      "Z = Hospital admission (conditioning variable)"
    ],
    "annotations": {
      "Case ID": "J2-37",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "Smoking and mortality both influence hospital admission; conditioning on admission induces spurious correlation.",
      "Key Insight": "Conditioning on a collider can reverse associations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does smoking reduce mortality among hospitalized patients?\nWhy does conditioning on hospital admission distort the association?",
    "expected_analysis": "Admission is a collider influenced by smoking and health.\nConditioning induces negative correlation.\nConclusion: Protective effect claim is INVALID.\nWise refusal: Analyze population-level data without conditioning on admission.",
    "Hidden Timestamp": "Is the analysis conditioning on Hospital admission (conditioning variable) that is determined after upstream factors affecting both Smoking status and Mortality, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Hospital admission (conditioning variable)): Associations between Smoking status and Mortality can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Hospital admission (conditioning variable)), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-38",
    "bucket": "BucketLarge-J",
    "title": "The Obesity Survival Puzzle",
    "scenario": "Among patients with heart disease, overweight patients appear to have better survival rates than normal-weight patients. Some interpret this as evidence of an “obesity paradox.”\nHeart disease diagnosis depends on both underlying health risks and body weight, creating a selected population.",
    "variables": [
      "X = Body weight",
      "Y = Survival outcome",
      "Z = Heart disease diagnosis"
    ],
    "annotations": {
      "Case ID": "J2-38",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on disease status biases associations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Diagnosis is a collider.\nApparent benefit is spurious.\nConclusion: Causal claim is INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on Heart disease diagnosis that is determined after upstream factors affecting both Body weight and Survival outcome, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Heart disease diagnosis): Associations between Body weight and Survival outcome can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Heart disease diagnosis), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-39",
    "bucket": "BucketLarge-J",
    "title": "Elite College Admissions and Success",
    "scenario": "Among students admitted to elite colleges, students from less privileged backgrounds outperform wealthier peers academically. Commentators argue that disadvantage improves performance.\nAdmission depends on both background and academic potential.",
    "variables": [
      "X = Socioeconomic background",
      "Y = Academic performance",
      "Z = Elite college admission"
    ],
    "annotations": {
      "Case ID": "J2-39",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Education",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on selective admission distorts comparisons.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Admission is a collider.\nHigh-performing disadvantaged students are overrepresented.\nConclusion: Advantage of disadvantage claim is INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on Elite college admission that is determined after upstream factors affecting both Socioeconomic background and Academic performance, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Elite college admission): Associations between Socioeconomic background and Academic performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Elite college admission), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-40",
    "bucket": "BucketLarge-J",
    "title": "Promotion and Job Satisfaction",
    "scenario": "Among employees who are promoted, those with lower job satisfaction before promotion show higher post-promotion performance. Managers infer dissatisfaction drives improvement.\nPromotion depends on both performance and dissatisfaction signals.",
    "variables": [
      "X = Job satisfaction",
      "Y = Performance",
      "Z = Promotion status"
    ],
    "annotations": {
      "Case ID": "J2-40",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on selection outcomes creates false correlations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Promotion is a collider.\nNegative association is spurious.\nConclusion: Interpretation is INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on Promotion status that is determined after upstream factors affecting both Job satisfaction and Performance, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Promotion status): Associations between Job satisfaction and Performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Promotion status), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-41",
    "bucket": "BucketLarge-J",
    "title": "Startup Founder Traits",
    "scenario": "Among founders of successful startups, those without formal business training appear more innovative. Observers conclude training stifles creativity.\nStartup success depends on both training and innovation.",
    "variables": [
      "X = Business training",
      "Y = Innovation",
      "Z = Startup success"
    ],
    "annotations": {
      "Case ID": "J2-41",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Entrepreneurship",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on success induces tradeoff illusion.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Success is a collider.\nApparent inverse relationship is spurious.\nConclusion: Claim is INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on Startup success that is determined after upstream factors affecting both Business training and Innovation, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Startup success): Associations between Business training and Innovation can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Startup success), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-42",
    "bucket": "BucketLarge-J",
    "title": "Disaster Survivor Health",
    "scenario": "Among disaster survivors, individuals with chronic conditions appear more resilient. Analysts infer chronic illness builds resilience.\nSurvival depends on both health status and exposure.",
    "variables": [
      "X = Chronic illness",
      "Y = Resilience",
      "Z = Survival"
    ],
    "annotations": {
      "Case ID": "J2-42",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Public Health",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on survival distorts health associations.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Survival is a collider.\nResilience effect is spurious.\nConclusion: Claim is INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on Survival that is determined after upstream factors affecting both Chronic illness and Resilience, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Survival): Associations between Chronic illness and Resilience can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Survival), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-43",
    "bucket": "BucketLarge-J",
    "title": "Academic Publication Bias",
    "scenario": "Among published papers, researchers without prestigious affiliations have higher citation counts. Some argue prestige harms impact.\nPublication depends on both institutional prestige and paper quality.",
    "variables": [
      "X = Institutional prestige",
      "Y = Citation impact",
      "Z = Publication"
    ],
    "annotations": {
      "Case ID": "J2-43",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Science of Science",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on publication induces spurious tradeoffs.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Publication is a collider.\nEffect is spurious.\nConclusion: Interpretation is INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on Publication that is determined after upstream factors affecting both Institutional prestige and Citation impact, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Publication): Associations between Institutional prestige and Citation impact can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Publication), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-44",
    "bucket": "BucketLarge-J",
    "title": "Hiring from Elite Firms",
    "scenario": "Among employees hired from elite firms, those with weaker resumes perform as well as strong candidates from non-elite firms. Managers infer elite firms overvalue credentials.\nHiring depends on both resume strength and firm pedigree.",
    "variables": [
      "X = Resume strength",
      "Y = Job performance",
      "Z = Hiring decision"
    ],
    "annotations": {
      "Case ID": "J2-44",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Labor & Hiring",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on hiring distorts performance signals.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Hiring is a collider.\nComparisons are biased.\nConclusion: Claim is INVALID.",
    "Hidden Timestamp": "Is the analysis conditioning on Hiring decision that is determined after upstream factors affecting both Resume strength and Job performance, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Hiring decision): Associations between Resume strength and Job performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Hiring decision), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-45",
    "bucket": "BucketLarge-J",
    "title": "The Athlete Selection Effect",
    "scenario": "Among professional athletes, those with poorer early training outperform peers later. Analysts argue early training is overrated.\nProfessional selection depends on both training quality and innate talent.",
    "variables": [
      "X = Early training quality",
      "Y = Later performance",
      "Z = Professional selection"
    ],
    "annotations": {
      "Case ID": "J2-45",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Collider",
      "Trap Subtype": "Conditioning on Compliance",
      "Difficulty": "Hard",
      "Subdomain": "Sports Analytics",
      "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
      "Key Insight": "Conditioning on elite selection induces spurious inversions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Selection is a collider.\nApparent reversal is spurious.\nConclusion: Claim is INVALID.\nL2-E (Base-Rate Distortion) — COMPLETE (9 cases)",
    "Hidden Timestamp": "Is the analysis conditioning on Professional selection that is determined after upstream factors affecting both Early training quality and Later performance, potentially inducing a spurious association?",
    "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Professional selection): Associations between Early training quality and Later performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Professional selection), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
  },
  {
    "id": "T3-BucketJ-46",
    "bucket": "BucketLarge-J",
    "title": "Vaccine Hospitalization Reports",
    "scenario": "A news report states that most hospitalized COVID patients are vaccinated, leading commentators to claim that vaccines are ineffective.\nHowever, a large majority of the population is vaccinated, while only a small fraction is unvaccinated.",
    "variables": [
      "X = Vaccination status",
      "Y = Hospitalization",
      "Z = Population base rate of vaccination"
    ],
    "annotations": {
      "Case ID": "J2-46",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Public Health",
      "Causal Structure": "Hospitalization counts reflect population proportions rather than individual risk.",
      "Key Insight": "High counts do not imply high risk without denominators.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the hospitalization statistic imply vaccines are ineffective?\nWhy are base rates necessary for interpretation?",
    "expected_analysis": "Vaccinated individuals dominate the population.\nRisk per person is lower among vaccinated.\nConclusion: Ineffectiveness claim is INVALID.\nWise refusal: Compare hospitalization rates, not counts.",
    "Hidden Timestamp": "Were the case counts for Hospitalization measured over the same time window as the base rate/denominator Population base rate of vaccination, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Hospitalization: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Hospitalization with denominator/base rate Population base rate of vaccination: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Hospitalization are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Population base rate of vaccination (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-47",
    "bucket": "BucketLarge-J",
    "title": "Crime Arrest Statistics",
    "scenario": "Police data shows that most arrests are from Neighborhood A, prompting claims that residents of Neighborhood A commit more crimes.\nHowever, Neighborhood A has twice the population of other neighborhoods.",
    "variables": [
      "X = Neighborhood",
      "Y = Arrest count",
      "Z = Neighborhood population size"
    ],
    "annotations": {
      "Case ID": "J2-47",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Counts must be normalized by population.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Higher counts reflect larger population.\nConclusion: Crime propensity claim is INVALID.\nWise refusal: Use per-capita arrest rates.",
    "Hidden Timestamp": "Were the case counts for Arrest count measured over the same time window as the base rate/denominator Neighborhood population size, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Arrest count: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Arrest count with denominator/base rate Neighborhood population size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Arrest count are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Neighborhood population size (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-48",
    "bucket": "BucketLarge-J",
    "title": "Loan Default Claims",
    "scenario": "A bank reports that most loan defaults come from low-income borrowers, concluding they are riskier.\nHowever, most borrowers in the bank’s portfolio are low-income.",
    "variables": [
      "X = Income group",
      "Y = Loan default",
      "Z = Borrower distribution"
    ],
    "annotations": {
      "Case ID": "J2-48",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Finance",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Default counts track borrower mix.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Risk requires conditional default rates.\nConclusion: Risk claim is INVALID.",
    "Hidden Timestamp": "Were the case counts for Loan default measured over the same time window as the base rate/denominator Borrower distribution, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Loan default: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Loan default with denominator/base rate Borrower distribution: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Loan default are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Borrower distribution (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-49",
    "bucket": "BucketLarge-J",
    "title": "Recidivism Rate Comparison",
    "scenario": "A report states that Group A accounts for more repeat offenses than Group B, suggesting harsher sentencing is needed for Group A.\nHowever, Group A represents a much larger share of the formerly incarcerated population.",
    "variables": [
      "X = Group membership",
      "Y = Recidivism",
      "Z = Released population size"
    ],
    "annotations": {
      "Case ID": "J2-49",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Exposure determines opportunity for reoffense.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Compare rates per released individual.\nConclusion: Sentencing inference is INVALID.",
    "Hidden Timestamp": "Were the case counts for Recidivism measured over the same time window as the base rate/denominator Released population size, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Recidivism: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Recidivism with denominator/base rate Released population size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Recidivism are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Released population size (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-50",
    "bucket": "BucketLarge-J",
    "title": "Workplace Accident Reports",
    "scenario": "A company reports that most workplace accidents occur in Factory X, concluding it is unsafe.\nFactory X employs far more workers than other sites.",
    "variables": [
      "X = Factory site",
      "Y = Accident count",
      "Z = Workforce size"
    ],
    "annotations": {
      "Case ID": "J2-50",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Occupational Safety",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Larger sites generate more incidents.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Normalize by hours worked.\nConclusion: Safety claim is INVALID.",
    "Hidden Timestamp": "Were the case counts for Accident count measured over the same time window as the base rate/denominator Workforce size, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Accident count: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Accident count with denominator/base rate Workforce size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Accident count are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Workforce size (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-51",
    "bucket": "BucketLarge-J",
    "title": "Academic Misconduct Cases",
    "scenario": "A university notes that most academic misconduct cases involve first-year students, implying they cheat more.\nHowever, first-year students comprise the largest enrollment cohort.",
    "variables": [
      "X = Academic year",
      "Y = Misconduct case",
      "Z = Enrollment size"
    ],
    "annotations": {
      "Case ID": "J2-51",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Education",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Case counts reflect cohort size.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Compare misconduct rates by cohort.\nConclusion: Cheating inference is INVALID.",
    "Hidden Timestamp": "Were the case counts for Misconduct case measured over the same time window as the base rate/denominator Enrollment size, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Misconduct case: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Misconduct case with denominator/base rate Enrollment size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Misconduct case are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Enrollment size (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-52",
    "bucket": "BucketLarge-J",
    "title": "Fraud Detection Alerts",
    "scenario": "An algorithm flags more fraudulent transactions among online purchases than in-store purchases, leading to claims that online shopping is riskier.\nHowever, online transactions vastly outnumber in-store ones.",
    "variables": [
      "X = Transaction type",
      "Y = Fraud alert",
      "Z = Transaction volume"
    ],
    "annotations": {
      "Case ID": "J2-52",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Finance & Compliance",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Alert counts track transaction volume.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Compare fraud rates, not alerts.\nConclusion: Risk claim is INVALID.",
    "Hidden Timestamp": "Were the case counts for Fraud alert measured over the same time window as the base rate/denominator Transaction volume, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Fraud alert: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Fraud alert with denominator/base rate Transaction volume: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Fraud alert are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Transaction volume (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-53",
    "bucket": "BucketLarge-J",
    "title": "School Discipline Disparities",
    "scenario": "A district reports that most suspensions involve students from School A, concluding discipline problems are worse there.\nSchool A enrolls substantially more students.",
    "variables": [
      "X = School",
      "Y = Suspension",
      "Z = Enrollment size"
    ],
    "annotations": {
      "Case ID": "J2-53",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Education Policy",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Discipline counts must be normalized.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Use suspension rates per student.\nConclusion: Discipline severity claim is INVALID.",
    "Hidden Timestamp": "Were the case counts for Suspension measured over the same time window as the base rate/denominator Enrollment size, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Suspension: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Suspension with denominator/base rate Enrollment size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Suspension are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Enrollment size (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-54",
    "bucket": "BucketLarge-J",
    "title": "Medical Side-Effect Reports",
    "scenario": "A drug safety report shows that most side-effect reports involve Drug A, raising concerns about its safety.\nDrug A is prescribed far more frequently than alternatives.",
    "variables": [
      "X = Drug type",
      "Y = Side-effect report",
      "Z = Prescription frequency"
    ],
    "annotations": {
      "Case ID": "J2-54",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Base-rate Neglect",
      "Trap Subtype": "Prior Ignorance",
      "Difficulty": "Medium",
      "Subdomain": "Healthcare",
      "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
      "Key Insight": "Report volume follows usage volume.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Compare side-effects per prescription.\nConclusion: Safety inference is INVALID.\nL2-F (Measurement / Denominator Mismatch) — COMPLETE (9 cases)",
    "Hidden Timestamp": "Were the case counts for Side-effect report measured over the same time window as the base rate/denominator Prescription frequency, and are we comparing rates rather than raw counts?",
    "Conditional Answers": "Answer if you only have raw counts of Side-effect report: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Side-effect report with denominator/base rate Prescription frequency: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Side-effect report are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Prescription frequency (and by subgroup/time) before concluding anything."
  },
  {
    "id": "T3-BucketJ-55",
    "bucket": "BucketLarge-J",
    "title": "The Class Size Paradox",
    "scenario": "A school district reports that students are in larger classes than ever before, even though the average class size per school has decreased. Officials argue that overcrowding is worsening.\nThe discrepancy arises because larger schools with many small classes enroll more students, while smaller schools with fewer large classes enroll fewer students.",
    "variables": [
      "X = School",
      "Y = Class size",
      "Z = Weighting scheme (student-weighted vs. class-weighted)"
    ],
    "annotations": {
      "Case ID": "J2-55",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education Statistics",
      "Causal Structure": "Different averaging schemes produce conflicting summaries without any underlying change.",
      "Key Insight": "“Average” depends on what is being averaged.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Are students actually experiencing larger classes?\nWhy do the two averages disagree?",
    "expected_analysis": "Class-weighted and student-weighted averages differ.\nNo contradiction exists.\nConclusion: Overcrowding claim is CONDITIONAL.\nWise refusal: Specify the unit of analysis.",
    "Hidden Timestamp": "Did the intervention/change in School alter the composition (Weighting scheme (student-weighted vs. class-weighted)) of who is counted before Class size was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Class size after changing School can reflect a real outcome shift.\nAnswer if School changes who is counted via Weighting scheme (student-weighted vs. class-weighted): The aggregate Class size can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Class size may be moving because the denominator/population changed after School via composition variable Weighting scheme (student-weighted vs. class-weighted). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-56",
    "bucket": "BucketLarge-J",
    "title": "The Average Income Misinterpretation",
    "scenario": "A city reports that average income increased, while surveys show most residents feel poorer. Officials dismiss public concern.\nIncome growth is driven by gains among a small number of very high earners.",
    "variables": [
      "X = Time period",
      "Y = Income",
      "Z = Mean vs median statistic"
    ],
    "annotations": {
      "Case ID": "J2-56",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Economics",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Means are sensitive to outliers.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Median income better reflects typical experience.\nConclusion: Prosperity claim is CONDITIONAL.",
    "Hidden Timestamp": "Did the intervention/change in Time period alter the composition (Mean vs median statistic) of who is counted before Income was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Income after changing Time period can reflect a real outcome shift.\nAnswer if Time period changes who is counted via Mean vs median statistic: The aggregate Income can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Income may be moving because the denominator/population changed after Time period via composition variable Mean vs median statistic. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-57",
    "bucket": "BucketLarge-J",
    "title": "Course Rating Inflation",
    "scenario": "A university finds that average course ratings increased, and concludes teaching quality improved.\nHowever, high-enrollment courses received lower ratings, while small seminars received higher ratings.",
    "variables": [
      "X = Course",
      "Y = Rating",
      "Z = Enrollment weighting"
    ],
    "annotations": {
      "Case ID": "J2-57",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Education",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Course-weighted averages differ from student-weighted experience.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Weighting determines interpretation.\nConclusion: Teaching improvement claim is CONDITIONAL.",
    "Hidden Timestamp": "Did the intervention/change in Course alter the composition (Enrollment weighting) of who is counted before Rating was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Rating after changing Course can reflect a real outcome shift.\nAnswer if Course changes who is counted via Enrollment weighting: The aggregate Rating can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Rating may be moving because the denominator/population changed after Course via composition variable Enrollment weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-58",
    "bucket": "BucketLarge-J",
    "title": "Hospital Wait Time Reporting",
    "scenario": "A hospital reports that average patient wait times decreased, while many patients report longer waits.\nShort visits dominate the average, masking longer waits for complex cases.",
    "variables": [
      "X = Visit type",
      "Y = Wait time",
      "Z = Visit weighting"
    ],
    "annotations": {
      "Case ID": "J2-58",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Healthcare Operations",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Aggregates hide heterogeneity.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Typical patient experience differs by visit type.\nConclusion: Efficiency claim is CONDITIONAL.",
    "Hidden Timestamp": "Did the intervention/change in Visit type alter the composition (Visit weighting) of who is counted before Wait time was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Wait time after changing Visit type can reflect a real outcome shift.\nAnswer if Visit type changes who is counted via Visit weighting: The aggregate Wait time can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Wait time may be moving because the denominator/population changed after Visit type via composition variable Visit weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-59",
    "bucket": "BucketLarge-J",
    "title": "Employee Satisfaction Scores",
    "scenario": "A firm reports rising average employee satisfaction, despite increased turnover.\nSatisfied long-tenured employees remain, while dissatisfied employees leave.",
    "variables": [
      "X = Employee tenure",
      "Y = Satisfaction score",
      "Z = Survivor weighting"
    ],
    "annotations": {
      "Case ID": "J2-59",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Organizational Behavior",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Averages reflect who remains.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Satisfaction increase reflects composition of respondents.\nConclusion: Workplace improvement claim is CONDITIONAL.",
    "Hidden Timestamp": "Did the intervention/change in Employee tenure alter the composition (Survivor weighting) of who is counted before Satisfaction score was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Satisfaction score after changing Employee tenure can reflect a real outcome shift.\nAnswer if Employee tenure changes who is counted via Survivor weighting: The aggregate Satisfaction score can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Satisfaction score may be moving because the denominator/population changed after Employee tenure via composition variable Survivor weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-60",
    "bucket": "BucketLarge-J",
    "title": "Traffic Congestion Metrics",
    "scenario": "A city reports that average commute times decreased, while drivers complain of worse traffic.\nOff-peak trips increased, lowering the average.",
    "variables": [
      "X = Time of travel",
      "Y = Commute duration",
      "Z = Trip distribution"
    ],
    "annotations": {
      "Case ID": "J2-60",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Transportation",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Averages depend on trip timing.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Peak-hour experience worsened.\nConclusion: Traffic improvement claim is CONDITIONAL.",
    "Hidden Timestamp": "Did the intervention/change in Time of travel alter the composition (Trip distribution) of who is counted before Commute duration was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Commute duration after changing Time of travel can reflect a real outcome shift.\nAnswer if Time of travel changes who is counted via Trip distribution: The aggregate Commute duration can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Commute duration may be moving because the denominator/population changed after Time of travel via composition variable Trip distribution. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-61",
    "bucket": "BucketLarge-J",
    "title": "Carbon Emissions Reporting",
    "scenario": "A country reports lower per-capita emissions, claiming environmental progress.\nTotal emissions increased due to population growth.",
    "variables": [
      "X = Population size",
      "Y = Emissions metric",
      "Z = Per-capita vs total denominator"
    ],
    "annotations": {
      "Case ID": "J2-61",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Environmental Policy",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Different denominators answer different questions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Both statements can be true.\nConclusion: Progress claim is CONDITIONAL.",
    "Hidden Timestamp": "Did the intervention/change in Population size alter the composition (Per-capita vs total denominator) of who is counted before Emissions metric was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Emissions metric after changing Population size can reflect a real outcome shift.\nAnswer if Population size changes who is counted via Per-capita vs total denominator: The aggregate Emissions metric can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Emissions metric may be moving because the denominator/population changed after Population size via composition variable Per-capita vs total denominator. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-62",
    "bucket": "BucketLarge-J",
    "title": "Energy Efficiency Ratings",
    "scenario": "Appliance A is rated as more energy efficient per use, while households using it consume more total energy.\nAppliance A is used more frequently.",
    "variables": [
      "X = Appliance type",
      "Y = Energy use",
      "Z = Usage frequency"
    ],
    "annotations": {
      "Case ID": "J2-62",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Energy Policy",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Efficiency does not equal total consumption.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Usage drives totals.\nConclusion: Efficiency claim is CONDITIONAL.",
    "Hidden Timestamp": "Did the intervention/change in Appliance type alter the composition (Usage frequency) of who is counted before Energy use was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Energy use after changing Appliance type can reflect a real outcome shift.\nAnswer if Appliance type changes who is counted via Usage frequency: The aggregate Energy use can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Energy use may be moving because the denominator/population changed after Appliance type via composition variable Usage frequency. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-63",
    "bucket": "BucketLarge-J",
    "title": "Social Media Engagement Rates",
    "scenario": "A platform reports higher average engagement per post, while users report declining reach.\nHigh-engagement posts dominate the average, while most posts perform poorly.",
    "variables": [
      "X = Post type",
      "Y = Engagement metric",
      "Z = Distribution skew"
    ],
    "annotations": {
      "Case ID": "J2-63",
      "Pearl Level": "L2 (Intervention)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Selection Bias",
      "Trap Subtype": "Post-intervention Selection",
      "Difficulty": "Medium",
      "Subdomain": "Digital Media",
      "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
      "Key Insight": "Skewed distributions distort averages.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Median engagement is more informative.\nConclusion: Platform success claim is CONDITIONAL.\nL3-A (Individual Counterfactuals) — 9 cases",
    "Hidden Timestamp": "Did the intervention/change in Post type alter the composition (Distribution skew) of who is counted before Engagement metric was computed?",
    "Conditional Answers": "Answer if the population/denominator is stable: A change in Engagement metric after changing Post type can reflect a real outcome shift.\nAnswer if Post type changes who is counted via Distribution skew: The aggregate Engagement metric can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Engagement metric may be moving because the denominator/population changed after Post type via composition variable Distribution skew. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
  },
  {
    "id": "T3-BucketJ-64",
    "bucket": "BucketLarge-J",
    "title": "The Missed Exam Retake",
    "scenario": "A student failed a qualifying exam and chose not to retake it, later leaving the program. The student claims that retaking the exam would not have changed the outcome.\nHowever, historical data shows that students with similar initial scores who retook the exam often passed.",
    "variables": [
      "X = Decision to retake the exam (yes / no)",
      "Y = Program continuation",
      "Z = Student preparation and ability"
    ],
    "annotations": {
      "Case ID": "J2-64",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Medium",
      "Subdomain": "Education Outcomes",
      "Causal Structure": "Only one action (retake or not) is observed for the student; the alternative outcome is unobserved.",
      "Key Insight": "Individual-level causal claims require counterfactual comparison.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can the student conclude that retaking the exam would not have mattered?\nWhy is this counterfactual inherently unobservable?",
    "expected_analysis": "The student’s outcome under retaking (Y₁) is unobserved.\nSimilar students provide suggestive but imperfect evidence.\nConclusion: The student’s claim is UNDETERMINED.\nWise refusal: Individual causal effects cannot be known without assumptions or experimental design.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Program continuation occurred—and is selection related to Student preparation and ability or Program continuation?",
    "Conditional Answers": "Answer if Decision to retake the exam (yes / no) is randomly assigned: A difference in Program continuation across Decision to retake the exam (yes / no) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Student preparation and ability): The Decision to retake the exam (yes / no) vs not-Decision to retake the exam (yes / no) difference in Program continuation is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Student preparation and ability) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Student preparation and ability); otherwise Decision to retake the exam (yes / no)–Program continuation differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-65",
    "bucket": "BucketLarge-J",
    "title": "The Declined Job Offer",
    "scenario": "An individual declined a job offer and later experienced slower career progression. They claim that accepting the offer would not have improved their career.\nComparable candidates who accepted similar offers often advanced more quickly.",
    "variables": [
      "X = Job offer acceptance",
      "Y = Career progression",
      "Z = Skill level and career ambition"
    ],
    "annotations": {
      "Case ID": "J2-65",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Reverse Causation",
      "Trap Subtype": "Outcome-dependent Worlds",
      "Difficulty": "Hard",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Personal counterfactual outcomes are unobserved.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Only one trajectory is observed.\nPeer comparisons are imperfect substitutes.\nConclusion: Claim is UNDETERMINED.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Career progression occurred—and is selection related to Skill level and career ambition or Career progression?",
    "Conditional Answers": "Answer if Job offer acceptance is randomly assigned: A difference in Career progression across Job offer acceptance groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Skill level and career ambition): The Job offer acceptance vs not-Job offer acceptance difference in Career progression is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Skill level and career ambition) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Skill level and career ambition); otherwise Job offer acceptance–Career progression differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-66",
    "bucket": "BucketLarge-J",
    "title": "The Medical Treatment Refusal",
    "scenario": "A patient declined a recommended medical treatment and later recovered naturally. They conclude the treatment was unnecessary.\nClinical evidence suggests that many patients who refused treatment deteriorated.",
    "variables": [
      "X = Treatment acceptance",
      "Y = Health outcome",
      "Z = Disease severity"
    ],
    "annotations": {
      "Case ID": "J2-66",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Reverse Causation",
      "Trap Subtype": "Outcome-dependent Worlds",
      "Difficulty": "Hard",
      "Subdomain": "Healthcare",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Recovery does not reveal what would have happened under treatment.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Natural recovery does not imply treatment uselessness.\nConclusion: Claim is INVALID.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Health outcome occurred—and is selection related to Disease severity or Health outcome?",
    "Conditional Answers": "Answer if Treatment acceptance is randomly assigned: A difference in Health outcome across Treatment acceptance groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Disease severity): The Treatment acceptance vs not-Treatment acceptance difference in Health outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Disease severity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Disease severity); otherwise Treatment acceptance–Health outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-67",
    "bucket": "BucketLarge-J",
    "title": "The Parole Board Decision",
    "scenario": "A parole board denies parole to an inmate, who later reoffends after release. The board claims parole denial was justified.\nThe counterfactual—what would have happened if parole had been granted earlier—is unobserved.",
    "variables": [
      "X = Parole decision",
      "Y = Recidivism",
      "Z = Risk profile"
    ],
    "annotations": {
      "Case ID": "J2-67",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Reverse Causation",
      "Trap Subtype": "Outcome-dependent Worlds",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Observed outcomes do not validate the decision taken.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Reoffending does not prove denial was optimal.\nConclusion: Claim is INVALID.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Recidivism occurred—and is selection related to Risk profile or Recidivism?",
    "Conditional Answers": "Answer if Parole decision is randomly assigned: A difference in Recidivism across Parole decision groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Risk profile): The Parole decision vs not-Parole decision difference in Recidivism is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Risk profile) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Risk profile); otherwise Parole decision–Recidivism differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-68",
    "bucket": "BucketLarge-J",
    "title": "The Scholarship Cutoff",
    "scenario": "A student narrowly missed a scholarship cutoff and later struggled financially. Administrators argue the cutoff was fair.\nWhether the student would have succeeded with the scholarship is unobserved.",
    "variables": [
      "X = Scholarship receipt",
      "Y = Academic success",
      "Z = Financial stability"
    ],
    "annotations": {
      "Case ID": "J2-68",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Small differences around cutoffs hide large causal uncertainty.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Near-threshold comparisons are suggestive but uncertain.\nConclusion: Claim is UNDETERMINED.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Academic success occurred—and is selection related to Financial stability or Academic success?",
    "Conditional Answers": "Answer if Scholarship receipt is randomly assigned: A difference in Academic success across Scholarship receipt groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Financial stability): The Scholarship receipt vs not-Scholarship receipt difference in Academic success is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Financial stability) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Financial stability); otherwise Scholarship receipt–Academic success differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-69",
    "bucket": "BucketLarge-J",
    "title": "The Therapy Dropout",
    "scenario": "A patient drops out of therapy early and does not improve. They claim therapy was ineffective for them.\nThe outcome had they completed therapy is unknown.",
    "variables": [
      "X = Therapy completion",
      "Y = Mental health outcome",
      "Z = Adherence capacity"
    ],
    "annotations": {
      "Case ID": "J2-69",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Psychology",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Dropout obscures treatment counterfactual.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Non-completion masks potential benefit.\nConclusion: Claim is INVALID.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Mental health outcome occurred—and is selection related to Adherence capacity or Mental health outcome?",
    "Conditional Answers": "Answer if Therapy completion is randomly assigned: A difference in Mental health outcome across Therapy completion groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Adherence capacity): The Therapy completion vs not-Therapy completion difference in Mental health outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Adherence capacity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Adherence capacity); otherwise Therapy completion–Mental health outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-70",
    "bucket": "BucketLarge-J",
    "title": "The Missed Investment Opportunity",
    "scenario": "An investor chose not to invest in a startup that later succeeded. They conclude investing would have yielded large returns.\nThe outcome had they invested—including dilution, exit timing, or failure—is unknowable.",
    "variables": [
      "X = Investment decision",
      "Y = Financial return",
      "Z = Market volatility"
    ],
    "annotations": {
      "Case ID": "J2-70",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Reverse Causation",
      "Trap Subtype": "Outcome-dependent Worlds",
      "Difficulty": "Hard",
      "Subdomain": "Finance",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Observing success does not reveal individual counterfactual payoff.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Success path is not deterministic.\nConclusion: Claim is UNDETERMINED.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Financial return occurred—and is selection related to Market volatility or Financial return?",
    "Conditional Answers": "Answer if Investment decision is randomly assigned: A difference in Financial return across Investment decision groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Market volatility): The Investment decision vs not-Investment decision difference in Financial return is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Market volatility) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Market volatility); otherwise Investment decision–Financial return differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-71",
    "bucket": "BucketLarge-J",
    "title": "The Alternative School Choice",
    "scenario": "A parent chose a private school for their child, who later excelled academically. The parent claims public school would have led to worse outcomes.\nThe child’s public-school trajectory is unobserved.",
    "variables": [
      "X = School choice",
      "Y = Academic outcome",
      "Z = Family support"
    ],
    "annotations": {
      "Case ID": "J2-71",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Education",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Success does not validate the chosen path.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Only one schooling path observed.\nConclusion: Claim is UNDETERMINED.\nL3-B (Policy Counterfactuals) — 6 cases",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Academic outcome occurred—and is selection related to Family support or Academic outcome?",
    "Conditional Answers": "Answer if School choice is randomly assigned: A difference in Academic outcome across School choice groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Family support): The School choice vs not-School choice difference in Academic outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Family support) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Family support); otherwise School choice–Academic outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-72",
    "bucket": "BucketLarge-J",
    "title": "The Minimum Wage Increase",
    "scenario": "A city raises the minimum wage, after which employment levels remain stable. Officials claim the policy had no negative employment effects.\nHowever, the counterfactual—what employment would have been without the wage increase—is unobserved. Economic conditions were improving during the same period.",
    "variables": [
      "X = Minimum wage policy",
      "Y = Employment level",
      "Z = Economic trend"
    ],
    "annotations": {
      "Case ID": "J2-72",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Labor Economics",
      "Causal Structure": "Observed employment reflects both policy and macroeconomic forces.",
      "Key Insight": "Stable outcomes do not imply zero policy effect.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Can we conclude the wage increase had no effect on employment?\nWhy is the counterfactual employment trajectory unobservable?",
    "expected_analysis": "Employment without the policy is unobserved.\nEconomic growth may mask negative effects.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires synthetic control or difference-in-differences.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Employment level occurred—and is selection related to Economic trend or Employment level?",
    "Conditional Answers": "Answer if Minimum wage policy is randomly assigned: A difference in Employment level across Minimum wage policy groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Economic trend): The Minimum wage policy vs not-Minimum wage policy difference in Employment level is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Economic trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Economic trend); otherwise Minimum wage policy–Employment level differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-73",
    "bucket": "BucketLarge-J",
    "title": "The Policing Strategy Shift",
    "scenario": "A city shifts to a community policing strategy and observes a decline in crime. Officials credit the strategy.\nNeighboring cities without the policy also experienced crime declines.",
    "variables": [
      "X = Policing strategy",
      "Y = Crime rate",
      "Z = Regional crime trend"
    ],
    "annotations": {
      "Case ID": "J2-73",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Criminal Justice",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Common trends obscure counterfactual outcomes.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Crime may have declined anyway.\nConclusion: Policy impact claim is UNDETERMINED.\nWise refusal: Requires comparison cities.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Crime rate occurred—and is selection related to Regional crime trend or Crime rate?",
    "Conditional Answers": "Answer if Policing strategy is randomly assigned: A difference in Crime rate across Policing strategy groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Regional crime trend): The Policing strategy vs not-Policing strategy difference in Crime rate is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Regional crime trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Regional crime trend); otherwise Policing strategy–Crime rate differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-74",
    "bucket": "BucketLarge-J",
    "title": "Education Funding Reform",
    "scenario": "A state increases education funding, and student outcomes improve. Legislators claim success.\nOther reforms were implemented simultaneously.",
    "variables": [
      "X = Funding reform",
      "Y = Student outcomes",
      "Z = Concurrent reforms"
    ],
    "annotations": {
      "Case ID": "J2-74",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Education Policy",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Isolating policy effects requires disentangling reforms.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Counterfactual funding-only effect is unknown.\nConclusion: Claim is UNDETERMINED.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Student outcomes occurred—and is selection related to Concurrent reforms or Student outcomes?",
    "Conditional Answers": "Answer if Funding reform is randomly assigned: A difference in Student outcomes across Funding reform groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Concurrent reforms): The Funding reform vs not-Funding reform difference in Student outcomes is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Concurrent reforms) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Concurrent reforms); otherwise Funding reform–Student outcomes differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-75",
    "bucket": "BucketLarge-J",
    "title": "Housing Construction Ban",
    "scenario": "A city imposes a housing construction ban, after which housing prices stabilize. Officials argue the ban prevented price increases.\nPrices might have stabilized regardless due to slowing demand.",
    "variables": [
      "X = Construction ban",
      "Y = Housing prices",
      "Z = Demand trend"
    ],
    "annotations": {
      "Case ID": "J2-75",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Housing Policy",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Observed stability does not reveal prevented change.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Counterfactual price trajectory unknown.\nConclusion: Claim is UNDETERMINED.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Housing prices occurred—and is selection related to Demand trend or Housing prices?",
    "Conditional Answers": "Answer if Construction ban is randomly assigned: A difference in Housing prices across Construction ban groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Demand trend): The Construction ban vs not-Construction ban difference in Housing prices is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Demand trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Demand trend); otherwise Construction ban–Housing prices differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-76",
    "bucket": "BucketLarge-J",
    "title": "Environmental Regulation Rollout",
    "scenario": "A country introduces emissions regulations, and emissions fall. Leaders credit regulation.\nGlobal energy prices also rose sharply.",
    "variables": [
      "X = Regulation",
      "Y = Emissions",
      "Z = Energy price shocks"
    ],
    "annotations": {
      "Case ID": "J2-76",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Environmental Policy",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "External forces affect counterfactual emissions.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Cannot isolate regulation effect.\nConclusion: Claim is UNDETERMINED.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Emissions occurred—and is selection related to Energy price shocks or Emissions?",
    "Conditional Answers": "Answer if Regulation is randomly assigned: A difference in Emissions across Regulation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Energy price shocks): The Regulation vs not-Regulation difference in Emissions is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Energy price shocks) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Energy price shocks); otherwise Regulation–Emissions differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-77",
    "bucket": "BucketLarge-J",
    "title": "Public Transit Expansion",
    "scenario": "A city expands public transit, after which traffic congestion eases. Officials credit the expansion.\nRemote work adoption increased during the same period.",
    "variables": [
      "X = Transit expansion",
      "Y = Congestion",
      "Z = Remote work prevalence"
    ],
    "annotations": {
      "Case ID": "J2-77",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Transportation Policy",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Multiple causes obscure the counterfactual.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Congestion might have declined anyway.\nConclusion: Claim is UNDETERMINED.\nL3-C (Fairness & Causal Attribution) — 2 cases",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Congestion occurred—and is selection related to Remote work prevalence or Congestion?",
    "Conditional Answers": "Answer if Transit expansion is randomly assigned: A difference in Congestion across Transit expansion groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Remote work prevalence): The Transit expansion vs not-Transit expansion difference in Congestion is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Remote work prevalence) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Remote work prevalence); otherwise Transit expansion–Congestion differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-78",
    "bucket": "BucketLarge-J",
    "title": "Algorithmic Hiring Fairness",
    "scenario": "A hiring algorithm results in fewer hires from Group A. The company claims the algorithm is discriminatory.\nHowever, the counterfactual—how many from Group A would have been hired without the algorithm—is unobserved.",
    "variables": [
      "X = Algorithmic hiring",
      "Y = Hiring outcome",
      "Z = Applicant qualification distribution"
    ],
    "annotations": {
      "Case ID": "J2-78",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "N/A",
      "Causal Structure": "Disparity does not imply discrimination without a counterfactual baseline.",
      "Key Insight": "Fairness claims require causal comparisons.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "Does the hiring disparity prove discrimination?\nWhat counterfactual is missing?",
    "expected_analysis": "Need hiring outcomes under alternative process.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires causal fairness evaluation.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Hiring outcome occurred—and is selection related to Applicant qualification distribution or Hiring outcome?",
    "Conditional Answers": "Answer if Algorithmic hiring is randomly assigned: A difference in Hiring outcome across Algorithmic hiring groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Applicant qualification distribution): The Algorithmic hiring vs not-Algorithmic hiring difference in Hiring outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Applicant qualification distribution) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Applicant qualification distribution); otherwise Algorithmic hiring–Hiring outcome differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-79",
    "bucket": "BucketLarge-J",
    "title": "Discrimination Attribution Fraction",
    "scenario": "A wage gap is observed between two demographic groups. Analysts claim a specific percentage is due to discrimination.\nThe fraction attributable to discrimination depends on the counterfactual wage distribution absent discrimination.",
    "variables": [
      "X = Group membership",
      "Y = Wages",
      "Z = Job characteristics"
    ],
    "annotations": {
      "Case ID": "J2-79",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Sociology & Law",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Attribution requires assumptions about counterfactual worlds.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Multiple valid counterfactuals exist.\nConclusion: Attribution percentage is UNDETERMINED.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Wages occurred—and is selection related to Job characteristics or Wages?",
    "Conditional Answers": "Answer if Group membership is randomly assigned: A difference in Wages across Group membership groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Job characteristics): The Group membership vs not-Group membership difference in Wages is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Job characteristics) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Job characteristics); otherwise Group membership–Wages differences may reflect selection rather than effect."
  },
  {
    "id": "T3-BucketJ-80",
    "bucket": "BucketLarge-J",
    "title": "Election Outcome Counterfactual",
    "scenario": "An election was decided by a narrow margin. Commentators claim a specific policy decision “caused” the loss.\nThe election outcome absent that policy is unknowable.",
    "variables": [
      "X = Policy decision",
      "Y = Election outcome",
      "Z = Voter preferences"
    ],
    "annotations": {
      "Case ID": "J2-80",
      "Pearl Level": "L3 (Counterfactual)",
      "Domain": "D10 (Social Science)",
      "Trap Type": "Confounding",
      "Trap Subtype": "Cross-world Confounder",
      "Difficulty": "Hard",
      "Subdomain": "Political Science",
      "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
      "Key Insight": "Singular historical events lack observable counterfactuals.",
      "author": "Sreya Vangara",
      "num_annotators": 2
    },
    "questions": "",
    "expected_analysis": "Many plausible counterfactual worlds.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires formal causal modeling assumptions.",
    "Hidden Timestamp": "At what point were units selected into the observed sample—before or after Election outcome occurred—and is selection related to Voter preferences or Election outcome?",
    "Conditional Answers": "Answer if Policy decision is randomly assigned: A difference in Election outcome across Policy decision groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Voter preferences): The Policy decision vs not-Policy decision difference in Election outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Voter preferences) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
    "Wise Refusal": "I don’t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Voter preferences); otherwise Policy decision–Election outcome differences may reflect selection rather than effect."
  }
]