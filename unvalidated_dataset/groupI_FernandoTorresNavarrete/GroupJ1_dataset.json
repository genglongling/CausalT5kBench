[
    {
        "id": "T3-BucketJ-81",
        "bucket": "BucketLarge-J",
        "title": "Scholarship Acceptance Rates by Major",
        "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Acceptance rate. \nBut when the metric is broken down by Applicant preparedness level (high/low), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
        "variables": [
            "X = Group (A vs. B)",
            "Y = Acceptance rate",
            "Z = Applicant preparedness level (high/low)"
        ],
        "annotations": {
            "Case ID": "J2-81",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Aggregation Bias",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
            "Key Insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the higher overall Acceptance rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
        "expected_analysis": "Identify Simpson\u2019s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Acceptance rate in every subgroup.\nConclusion: The \u201cGroup A is better\u201d interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required.",
        "Hidden Timestamp": "Was Applicant preparedness level (high/low) determined before Group (A vs. B) was chosen, and could Applicant preparedness level (high/low) have influenced the choice of Group (A vs. B) before Acceptance rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Acceptance rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Applicant preparedness level (high/low): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Acceptance rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.\nAnswer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Acceptance rate by the key strata (e.g., Applicant preparedness level (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-82",
        "bucket": "BucketLarge-J",
        "title": "Library Program Participation",
        "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Completion rate. \nBut when the metric is broken down by Student schedule flexibility (high/low), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
        "variables": [
            "X = Group (A vs. B)",
            "Y = Completion rate",
            "Z = Student schedule flexibility (high/low)"
        ],
        "annotations": {
            "Case ID": "J2-82",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Aggregation Bias",
            "Difficulty": "Medium",
            "Subdomain": "Education Sociology",
            "Causal Structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
            "Key Insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the higher overall Completion rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
        "expected_analysis": "Identify Simpson\u2019s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Completion rate in every subgroup.\nConclusion: The \u201cGroup A is better\u201d interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required.",
        "Hidden Timestamp": "Was Student schedule flexibility (high/low) determined before Group (A vs. B) was chosen, and could Student schedule flexibility (high/low) have influenced the choice of Group (A vs. B) before Completion rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Completion rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Student schedule flexibility (high/low): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Completion rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.\nAnswer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Completion rate by the key strata (e.g., Student schedule flexibility (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-83",
        "bucket": "BucketLarge-J",
        "title": "Neighborhood Recycling Compliance",
        "scenario": "A city or institution releases a headline metric claiming Group A outperforms Group B on Recycling compliance rate. \nBut when the metric is broken down by Housing type (single-family/apartment), Group B outperforms Group A in every subgroup. \nThe contradiction appears because Group A contains a much larger share of the subgroup that tends to do well regardless of group membership.",
        "variables": [
            "X = Group (A vs. B)",
            "Y = Recycling compliance rate",
            "Z = Housing type (single-family/apartment)"
        ],
        "annotations": {
            "Case ID": "J2-83",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Aggregation Bias",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "Z affects Y and differs in distribution across X. Aggregating across Z can reverse subgroup trends.",
            "Key Insight": "An aggregate association can flip relative to every subgroup due to unequal subgroup weights.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the higher overall Recycling compliance rate for Group A imply Group A is better? Why or why not?\nHow does conditioning on Z change the interpretation?",
        "expected_analysis": "Identify Simpson\u2019s Paradox: the overall association between X and Y is driven by different Z-mixes across groups.\nZ is determined prior to (or independent of) X in the scenario, so it is not an effect of X.\nWhen stratifying by Z, Group B has higher Recycling compliance rate in every subgroup.\nConclusion: The \u201cGroup A is better\u201d interpretation is INVALID from aggregate data alone; subgroup-adjusted comparisons are required.",
        "Hidden Timestamp": "Was Housing type (single-family/apartment) determined before Group (A vs. B) was chosen, and could Housing type (single-family/apartment) have influenced the choice of Group (A vs. B) before Recycling compliance rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Group (A vs. B) on Recycling compliance rate may be reversed because the mix of subgroups differs between Group (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Housing type (single-family/apartment): Use the within-stratum differences (or a standardized effect). If Group (A vs. B) improves Recycling compliance rate in each stratum, prefer Group (A vs. B) even if the aggregate looks worse.\nAnswer if Group (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Recycling compliance rate by the key strata (e.g., Housing type (single-family/apartment) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Group (A vs. B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-84",
        "bucket": "BucketLarge-J",
        "title": "The Alumni Donation Email Survey",
        "scenario": "An organization reports a very positive statistic for Reported donation intent based only on observations from a subset of people.\nThe subset is formed by Survey response that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
        "variables": [
            "X = Survey response",
            "Y = Reported donation intent",
            "Z = Underlying true outcome (positive/negative)"
        ],
        "annotations": {
            "Case ID": "J2-84",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Easy",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
            "Key Insight": "Observed outcomes reflect who is observed, not the full population.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
        "expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Reported donation intent statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Reported donation intent occurred\u2014and is selection related to Underlying true outcome (positive/negative) or Reported donation intent?",
        "Conditional Answers": "Answer if Survey response is randomly assigned: A difference in Reported donation intent across Survey response groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Survey response vs not-Survey response difference in Reported donation intent is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Survey response\u2013Reported donation intent differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-85",
        "bucket": "BucketLarge-J",
        "title": "The 'Top Reviewer' Restaurant Ratings",
        "scenario": "An organization reports a very positive statistic for Average star rating based only on observations from a subset of people.\nThe subset is formed by Who leaves reviews that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
        "variables": [
            "X = Who leaves reviews",
            "Y = Average star rating",
            "Z = Underlying true outcome (positive/negative)"
        ],
        "annotations": {
            "Case ID": "J2-85",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Easy",
            "Subdomain": "Digital Media",
            "Causal Structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
            "Key Insight": "Observed outcomes reflect who is observed, not the full population.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
        "expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Average star rating statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Average star rating occurred\u2014and is selection related to Underlying true outcome (positive/negative) or Average star rating?",
        "Conditional Answers": "Answer if Who leaves reviews is randomly assigned: A difference in Average star rating across Who leaves reviews groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Who leaves reviews vs not-Who leaves reviews difference in Average star rating is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Who leaves reviews\u2013Average star rating differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-86",
        "bucket": "BucketLarge-J",
        "title": "The Parenting App Success Stories",
        "scenario": "An organization reports a very positive statistic for Reported improvement based only on observations from a subset of people.\nThe subset is formed by Who shares outcomes that is voluntary or outcome-dependent.\nInternal notes suggest people with negative experiences are less likely to be observed in the dataset.",
        "variables": [
            "X = Who shares outcomes",
            "Y = Reported improvement",
            "Z = Underlying true outcome (positive/negative)"
        ],
        "annotations": {
            "Case ID": "J2-86",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Easy",
            "Subdomain": "Psychology",
            "Causal Structure": "Z affects whether an observation is recorded (X), and Y is computed only among observed cases.",
            "Key Insight": "Observed outcomes reflect who is observed, not the full population.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is the reported statistic likely representative of the full population? What could bias it?\nWhat additional data would you need to estimate the true average outcome?",
        "expected_analysis": "Recognize selection bias: the observed sample is not a random draw.\nParticipation/visibility (X) depends on the underlying outcome Z, so Y measured among observed cases is biased.\nConclusion: The reported Reported improvement statistic is NOT a reliable estimate of the population value; the causal claim or generalization is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Reported improvement occurred\u2014and is selection related to Underlying true outcome (positive/negative) or Reported improvement?",
        "Conditional Answers": "Answer if Who shares outcomes is randomly assigned: A difference in Reported improvement across Who shares outcomes groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Underlying true outcome (positive/negative)): The Who shares outcomes vs not-Who shares outcomes difference in Reported improvement is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Underlying true outcome (positive/negative)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Underlying true outcome (positive/negative)); otherwise Who shares outcomes\u2013Reported improvement differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-87",
        "bucket": "BucketLarge-J",
        "title": "County-Level Broadband and Grades",
        "scenario": "A report finds that regions/organizations with higher County average broadband access also have higher Student grades on average.\nA commentator concludes that any individual inside a region with higher County average broadband access will have higher Student grades.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
        "variables": [
            "X = County average broadband access (group-level)",
            "Y = Student grades (individual-level)",
            "Z = Individual-level characteristics"
        ],
        "annotations": {
            "Case ID": "J2-87",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Socioeconomic",
            "Difficulty": "Medium",
            "Subdomain": "Education Sociology",
            "Causal Structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
            "Key Insight": "You cannot infer individual behavior reliably from group-level correlations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
        "expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID.",
        "Hidden Timestamp": "Is County average broadband access (group-level) measured at an aggregate level while Student grades (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Student grades (individual-level)?",
        "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how County average broadband access (group-level) relates to Student grades (individual-level) for a person."
    },
    {
        "id": "T3-BucketJ-88",
        "bucket": "BucketLarge-J",
        "title": "State-Level Unemployment and Crime",
        "scenario": "A report finds that regions/organizations with higher State unemployment rate also have higher Individual criminal behavior on average.\nA commentator concludes that any individual inside a region with higher State unemployment rate will have higher Individual criminal behavior.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
        "variables": [
            "X = State unemployment rate (group-level)",
            "Y = Individual criminal behavior (individual-level)",
            "Z = Individual-level characteristics"
        ],
        "annotations": {
            "Case ID": "J2-88",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Socioeconomic",
            "Difficulty": "Medium",
            "Subdomain": "Criminology",
            "Causal Structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
            "Key Insight": "You cannot infer individual behavior reliably from group-level correlations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
        "expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID.",
        "Hidden Timestamp": "Is State unemployment rate (group-level) measured at an aggregate level while Individual criminal behavior (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Individual criminal behavior (individual-level)?",
        "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how State unemployment rate (group-level) relates to Individual criminal behavior (individual-level) for a person."
    },
    {
        "id": "T3-BucketJ-89",
        "bucket": "BucketLarge-J",
        "title": "Company-Level Diversity and Satisfaction",
        "scenario": "A report finds that regions/organizations with higher Company diversity percentage also have higher Individual satisfaction on average.\nA commentator concludes that any individual inside a region with higher Company diversity percentage will have higher Individual satisfaction.\nHowever, individual-level data within regions shows large variation, and the within-region relationship is weak or opposite once relevant individual factors are considered.",
        "variables": [
            "X = Company diversity percentage (group-level)",
            "Y = Individual satisfaction (individual-level)",
            "Z = Individual-level characteristics"
        ],
        "annotations": {
            "Case ID": "J2-89",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Socioeconomic",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Group-level aggregates summarize heterogeneous individuals; group-level correlation does not imply the same relation at the individual level.",
            "Key Insight": "You cannot infer individual behavior reliably from group-level correlations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why is it invalid to conclude individual outcomes from the group-level correlation?\nWhat data level is required to support the individual-level claim?",
        "expected_analysis": "Identify ecological fallacy: the correlation is at the group level, but the claim is about individuals. Without individual-level modeling and controls for Z, the inference is INVALID.",
        "Hidden Timestamp": "Is Company diversity percentage (group-level) measured at an aggregate level while Individual satisfaction (individual-level) is an individual claim, and when/where does aggregation into Individual-level characteristics happen relative to measuring Individual satisfaction (individual-level)?",
        "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Company diversity percentage (group-level) relates to Individual satisfaction (individual-level) for a person."
    },
    {
        "id": "T3-BucketJ-90",
        "bucket": "BucketLarge-J",
        "title": "Choosing Between Two Tutoring Programs",
        "scenario": "A manager must choose between two interventions (Program A vs Program B) to improve test pass rate.\nA pilot dataset reports that, overall, intervention A has a lower test pass rate than intervention B.\nBut when the pilot results are stratified by baseline preparedness (high/low), intervention A has a higher test pass rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Program A vs Program B)",
            "Y = test pass rate",
            "Z = baseline preparedness (high/low)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-90",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize test pass rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves test pass rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was baseline preparedness (high/low) determined before Intervention choice (Program A vs Program B) was chosen, and could baseline preparedness (high/low) have influenced the choice of Intervention choice (Program A vs Program B) before test pass rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Program A vs Program B) on test pass rate may be reversed because the mix of subgroups differs between Intervention choice (Program A vs Program B) arms.\nAnswer if you compare within strata after stratifying/standardizing by baseline preparedness (high/low): Use the within-stratum differences (or a standardized effect). If Intervention choice (Program A vs Program B) improves test pass rate in each stratum, prefer Intervention choice (Program A vs Program B) even if the aggregate looks worse.\nAnswer if Intervention choice (Program A vs Program B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report test pass rate by the key strata (e.g., baseline preparedness (high/low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Program A vs Program B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-91",
        "bucket": "BucketLarge-J",
        "title": "Comparing Two Telehealth Platforms",
        "scenario": "A manager must choose between two interventions (Platform A vs Platform B) to improve resolution rate within 7 days.\nA pilot dataset reports that, overall, intervention A has a lower resolution rate within 7 days than intervention B.\nBut when the pilot results are stratified by case complexity (simple/complex), intervention A has a higher resolution rate within 7 days in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Platform A vs Platform B)",
            "Y = resolution rate within 7 days",
            "Z = case complexity (simple/complex)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-91",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Health Policy",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize resolution rate within 7 days? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves resolution rate within 7 days in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was case complexity (simple/complex) determined before Intervention choice (Platform A vs Platform B) was chosen, and could case complexity (simple/complex) have influenced the choice of Intervention choice (Platform A vs Platform B) before resolution rate within 7 days was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Platform A vs Platform B) on resolution rate within 7 days may be reversed because the mix of subgroups differs between Intervention choice (Platform A vs Platform B) arms.\nAnswer if you compare within strata after stratifying/standardizing by case complexity (simple/complex): Use the within-stratum differences (or a standardized effect). If Intervention choice (Platform A vs Platform B) improves resolution rate within 7 days in each stratum, prefer Intervention choice (Platform A vs Platform B) even if the aggregate looks worse.\nAnswer if Intervention choice (Platform A vs Platform B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report resolution rate within 7 days by the key strata (e.g., case complexity (simple/complex) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Platform A vs Platform B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-92",
        "bucket": "BucketLarge-J",
        "title": "Evaluating Two Marketing Messages",
        "scenario": "A manager must choose between two interventions (Message A vs Message B) to improve purchase conversion rate.\nA pilot dataset reports that, overall, intervention A has a lower purchase conversion rate than intervention B.\nBut when the pilot results are stratified by customer segment (new/returning), intervention A has a higher purchase conversion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Message A vs Message B)",
            "Y = purchase conversion rate",
            "Z = customer segment (new/returning)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-92",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize purchase conversion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves purchase conversion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was customer segment (new/returning) determined before Intervention choice (Message A vs Message B) was chosen, and could customer segment (new/returning) have influenced the choice of Intervention choice (Message A vs Message B) before purchase conversion rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Message A vs Message B) on purchase conversion rate may be reversed because the mix of subgroups differs between Intervention choice (Message A vs Message B) arms.\nAnswer if you compare within strata after stratifying/standardizing by customer segment (new/returning): Use the within-stratum differences (or a standardized effect). If Intervention choice (Message A vs Message B) improves purchase conversion rate in each stratum, prefer Intervention choice (Message A vs Message B) even if the aggregate looks worse.\nAnswer if Intervention choice (Message A vs Message B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report purchase conversion rate by the key strata (e.g., customer segment (new/returning) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Message A vs Message B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-93",
        "bucket": "BucketLarge-J",
        "title": "Promotion Policy Across Departments",
        "scenario": "A manager must choose between two interventions (Policy A vs Policy B) to improve promotion rate.\nA pilot dataset reports that, overall, intervention A has a lower promotion rate than intervention B.\nBut when the pilot results are stratified by department (sales/engineering), intervention A has a higher promotion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Policy A vs Policy B)",
            "Y = promotion rate",
            "Z = department (sales/engineering)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-93",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize promotion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves promotion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was department (sales/engineering) determined before Intervention choice (Policy A vs Policy B) was chosen, and could department (sales/engineering) have influenced the choice of Intervention choice (Policy A vs Policy B) before promotion rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Policy A vs Policy B) on promotion rate may be reversed because the mix of subgroups differs between Intervention choice (Policy A vs Policy B) arms.\nAnswer if you compare within strata after stratifying/standardizing by department (sales/engineering): Use the within-stratum differences (or a standardized effect). If Intervention choice (Policy A vs Policy B) improves promotion rate in each stratum, prefer Intervention choice (Policy A vs Policy B) even if the aggregate looks worse.\nAnswer if Intervention choice (Policy A vs Policy B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report promotion rate by the key strata (e.g., department (sales/engineering) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Policy A vs Policy B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-94",
        "bucket": "BucketLarge-J",
        "title": "Route Choice for Delivery Drivers",
        "scenario": "A manager must choose between two interventions (Route A vs Route B) to improve on-time delivery rate.\nA pilot dataset reports that, overall, intervention A has a lower on-time delivery rate than intervention B.\nBut when the pilot results are stratified by traffic day type (normal/event), intervention A has a higher on-time delivery rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Route A vs Route B)",
            "Y = on-time delivery rate",
            "Z = traffic day type (normal/event)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-94",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize on-time delivery rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves on-time delivery rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was traffic day type (normal/event) determined before Intervention choice (Route A vs Route B) was chosen, and could traffic day type (normal/event) have influenced the choice of Intervention choice (Route A vs Route B) before on-time delivery rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Route A vs Route B) on on-time delivery rate may be reversed because the mix of subgroups differs between Intervention choice (Route A vs Route B) arms.\nAnswer if you compare within strata after stratifying/standardizing by traffic day type (normal/event): Use the within-stratum differences (or a standardized effect). If Intervention choice (Route A vs Route B) improves on-time delivery rate in each stratum, prefer Intervention choice (Route A vs Route B) even if the aggregate looks worse.\nAnswer if Intervention choice (Route A vs Route B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report on-time delivery rate by the key strata (e.g., traffic day type (normal/event) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Route A vs Route B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-95",
        "bucket": "BucketLarge-J",
        "title": "Online Course Format Change",
        "scenario": "A manager must choose between two interventions (Format A vs Format B) to improve course completion rate.\nA pilot dataset reports that, overall, intervention A has a lower course completion rate than intervention B.\nBut when the pilot results are stratified by student work hours (low/high), intervention A has a higher course completion rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Format A vs Format B)",
            "Y = course completion rate",
            "Z = student work hours (low/high)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-95",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Education Sociology",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize course completion rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves course completion rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was student work hours (low/high) determined before Intervention choice (Format A vs Format B) was chosen, and could student work hours (low/high) have influenced the choice of Intervention choice (Format A vs Format B) before course completion rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Format A vs Format B) on course completion rate may be reversed because the mix of subgroups differs between Intervention choice (Format A vs Format B) arms.\nAnswer if you compare within strata after stratifying/standardizing by student work hours (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Format A vs Format B) improves course completion rate in each stratum, prefer Intervention choice (Format A vs Format B) even if the aggregate looks worse.\nAnswer if Intervention choice (Format A vs Format B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report course completion rate by the key strata (e.g., student work hours (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Format A vs Format B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-96",
        "bucket": "BucketLarge-J",
        "title": "Vaccination Outreach Strategy",
        "scenario": "A manager must choose between two interventions (Outreach A vs Outreach B) to improve vaccination uptake.\nA pilot dataset reports that, overall, intervention A has a lower vaccination uptake than intervention B.\nBut when the pilot results are stratified by neighborhood access level (low/high), intervention A has a higher vaccination uptake in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Outreach A vs Outreach B)",
            "Y = vaccination uptake",
            "Z = neighborhood access level (low/high)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-96",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Public Policy",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize vaccination uptake? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves vaccination uptake in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was neighborhood access level (low/high) determined before Intervention choice (Outreach A vs Outreach B) was chosen, and could neighborhood access level (low/high) have influenced the choice of Intervention choice (Outreach A vs Outreach B) before vaccination uptake was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Outreach A vs Outreach B) on vaccination uptake may be reversed because the mix of subgroups differs between Intervention choice (Outreach A vs Outreach B) arms.\nAnswer if you compare within strata after stratifying/standardizing by neighborhood access level (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Outreach A vs Outreach B) improves vaccination uptake in each stratum, prefer Intervention choice (Outreach A vs Outreach B) even if the aggregate looks worse.\nAnswer if Intervention choice (Outreach A vs Outreach B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report vaccination uptake by the key strata (e.g., neighborhood access level (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Outreach A vs Outreach B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-97",
        "bucket": "BucketLarge-J",
        "title": "Job Interview Screen Tool",
        "scenario": "A manager must choose between two interventions (Tool A vs Tool B) to improve offer rate.\nA pilot dataset reports that, overall, intervention A has a lower offer rate than intervention B.\nBut when the pilot results are stratified by applicant experience (junior/senior), intervention A has a higher offer rate in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Tool A vs Tool B)",
            "Y = offer rate",
            "Z = applicant experience (junior/senior)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-97",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize offer rate? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves offer rate in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was applicant experience (junior/senior) determined before Intervention choice (Tool A vs Tool B) was chosen, and could applicant experience (junior/senior) have influenced the choice of Intervention choice (Tool A vs Tool B) before offer rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Tool A vs Tool B) on offer rate may be reversed because the mix of subgroups differs between Intervention choice (Tool A vs Tool B) arms.\nAnswer if you compare within strata after stratifying/standardizing by applicant experience (junior/senior): Use the within-stratum differences (or a standardized effect). If Intervention choice (Tool A vs Tool B) improves offer rate in each stratum, prefer Intervention choice (Tool A vs Tool B) even if the aggregate looks worse.\nAnswer if Intervention choice (Tool A vs Tool B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report offer rate by the key strata (e.g., applicant experience (junior/senior) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Tool A vs Tool B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-98",
        "bucket": "BucketLarge-J",
        "title": "Customer Support Triage Rule",
        "scenario": "A manager must choose between two interventions (Rule A vs Rule B) to improve satisfaction score.\nA pilot dataset reports that, overall, intervention A has a lower satisfaction score than intervention B.\nBut when the pilot results are stratified by ticket severity (low/high), intervention A has a higher satisfaction score in every stratum.\nThe overall comparison is distorted because intervention A was disproportionately used in the harder-to-improve stratum.",
        "variables": [
            "X = Intervention choice (Rule A vs Rule B)",
            "Y = satisfaction score",
            "Z = ticket severity (low/high)",
            "U = underlying difficulty / baseline risk that affects Y"
        ],
        "annotations": {
            "Case ID": "J2-98",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Digital Media",
            "Causal Structure": "Z (or U) affects Y; X assignment depends on Z/U. Aggregate difference mixes strata unequally.",
            "Key Insight": "The causal effect of X on Y must be evaluated within comparable strata; aggregate can reverse.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Based on the pilot, should the manager roll out intervention A or B to maximize satisfaction score? Explain.\nWhat analysis would estimate the causal effect of X on Y here?",
        "expected_analysis": "Diagnose Simpson\u2019s paradox under intervention: assignment of X is imbalanced across Z/U.\nCompute/compare A vs B within each Z stratum (or adjust via stratification/standardization).\nIf A improves satisfaction score in every stratum, the aggregate 'B is better' claim is misleading.\nConclusion: Prefer A if the goal is to improve outcomes holding Z constant; report a standardized effect.",
        "Hidden Timestamp": "Was ticket severity (low/high) determined before Intervention choice (Rule A vs Rule B) was chosen, and could ticket severity (low/high) have influenced the choice of Intervention choice (Rule A vs Rule B) before satisfaction score was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Intervention choice (Rule A vs Rule B) on satisfaction score may be reversed because the mix of subgroups differs between Intervention choice (Rule A vs Rule B) arms.\nAnswer if you compare within strata after stratifying/standardizing by ticket severity (low/high): Use the within-stratum differences (or a standardized effect). If Intervention choice (Rule A vs Rule B) improves satisfaction score in each stratum, prefer Intervention choice (Rule A vs Rule B) even if the aggregate looks worse.\nAnswer if Intervention choice (Rule A vs Rule B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report satisfaction score by the key strata (e.g., ticket severity (low/high) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Intervention choice (Rule A vs Rule B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-99",
        "bucket": "BucketLarge-J",
        "title": "Gentrification and Average Income",
        "scenario": "A policymaker implements an intervention: New housing permits.\nAfter the change, the reported metric average neighborhood income improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (New housing permits)",
            "Y = Reported metric (average neighborhood income)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-99",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average neighborhood income prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (New housing permits) change who is included in the denominator before Reported metric (average neighborhood income) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average neighborhood income) after changing Intervention (New housing permits) can reflect a real outcome shift.\nAnswer if Intervention (New housing permits) changes who is counted: The aggregate Reported metric (average neighborhood income) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average neighborhood income) may be moving because the denominator/population changed after Intervention (New housing permits). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-100",
        "bucket": "BucketLarge-J",
        "title": "Expanded Advanced-Placement Access",
        "scenario": "A policymaker implements an intervention: Opening AP enrollment.\nAfter the change, the reported metric average AP exam score improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Opening AP enrollment)",
            "Y = Reported metric (average AP exam score)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-100",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average AP exam score prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Opening AP enrollment) change who is included in the denominator before Reported metric (average AP exam score) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average AP exam score) after changing Intervention (Opening AP enrollment) can reflect a real outcome shift.\nAnswer if Intervention (Opening AP enrollment) changes who is counted: The aggregate Reported metric (average AP exam score) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average AP exam score) may be moving because the denominator/population changed after Intervention (Opening AP enrollment). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-101",
        "bucket": "BucketLarge-J",
        "title": "Corporate Merger and Diversity Metrics",
        "scenario": "A policymaker implements an intervention: Merging with a contractor-heavy firm.\nAfter the change, the reported metric percent women in 'full-time staff' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Merging with a contractor-heavy firm)",
            "Y = Reported metric (percent women in 'full-time staff')",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-101",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in percent women in 'full-time staff' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Merging with a contractor-heavy firm) change who is included in the denominator before Reported metric (percent women in 'full-time staff') was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (percent women in 'full-time staff') after changing Intervention (Merging with a contractor-heavy firm) can reflect a real outcome shift.\nAnswer if Intervention (Merging with a contractor-heavy firm) changes who is counted: The aggregate Reported metric (percent women in 'full-time staff') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (percent women in 'full-time staff') may be moving because the denominator/population changed after Intervention (Merging with a contractor-heavy firm). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-102",
        "bucket": "BucketLarge-J",
        "title": "New Transit Line and Commute Times",
        "scenario": "A policymaker implements an intervention: Launching a transit line.\nAfter the change, the reported metric average commute time of 'drivers' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Launching a transit line)",
            "Y = Reported metric (average commute time of 'drivers')",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-102",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Public Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average commute time of 'drivers' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Launching a transit line) change who is included in the denominator before Reported metric (average commute time of 'drivers') was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average commute time of 'drivers') after changing Intervention (Launching a transit line) can reflect a real outcome shift.\nAnswer if Intervention (Launching a transit line) changes who is counted: The aggregate Reported metric (average commute time of 'drivers') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average commute time of 'drivers') may be moving because the denominator/population changed after Intervention (Launching a transit line). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-103",
        "bucket": "BucketLarge-J",
        "title": "Clinic Screening Campaign",
        "scenario": "A policymaker implements an intervention: Free screening days.\nAfter the change, the reported metric positive test fraction among tested improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Free screening days)",
            "Y = Reported metric (positive test fraction among tested)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-103",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Health Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in positive test fraction among tested prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Free screening days) change who is included in the denominator before Reported metric (positive test fraction among tested) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (positive test fraction among tested) after changing Intervention (Free screening days) can reflect a real outcome shift.\nAnswer if Intervention (Free screening days) changes who is counted: The aggregate Reported metric (positive test fraction among tested) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (positive test fraction among tested) may be moving because the denominator/population changed after Intervention (Free screening days). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-104",
        "bucket": "BucketLarge-J",
        "title": "School Attendance Incentives",
        "scenario": "A policymaker implements an intervention: Attendance rewards.\nAfter the change, the reported metric average absence rate among 'enrolled students' improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Attendance rewards)",
            "Y = Reported metric (average absence rate among 'enrolled students')",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-104",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Sociology",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average absence rate among 'enrolled students' prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Attendance rewards) change who is included in the denominator before Reported metric (average absence rate among 'enrolled students') was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average absence rate among 'enrolled students') after changing Intervention (Attendance rewards) can reflect a real outcome shift.\nAnswer if Intervention (Attendance rewards) changes who is counted: The aggregate Reported metric (average absence rate among 'enrolled students') can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average absence rate among 'enrolled students') may be moving because the denominator/population changed after Intervention (Attendance rewards). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-105",
        "bucket": "BucketLarge-J",
        "title": "Immigration Policy and Unemployment",
        "scenario": "A policymaker implements an intervention: New work visa program.\nAfter the change, the reported metric city unemployment rate improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (New work visa program)",
            "Y = Reported metric (city unemployment rate)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-105",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Labor Economics",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in city unemployment rate prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (New work visa program) change who is included in the denominator before Reported metric (city unemployment rate) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (city unemployment rate) after changing Intervention (New work visa program) can reflect a real outcome shift.\nAnswer if Intervention (New work visa program) changes who is counted: The aggregate Reported metric (city unemployment rate) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (city unemployment rate) may be moving because the denominator/population changed after Intervention (New work visa program). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-106",
        "bucket": "BucketLarge-J",
        "title": "Crime Reporting App",
        "scenario": "A policymaker implements an intervention: Launching a reporting app.\nAfter the change, the reported metric reported crime incidents improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Launching a reporting app)",
            "Y = Reported metric (reported crime incidents)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-106",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Criminology",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in reported crime incidents prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Launching a reporting app) change who is included in the denominator before Reported metric (reported crime incidents) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (reported crime incidents) after changing Intervention (Launching a reporting app) can reflect a real outcome shift.\nAnswer if Intervention (Launching a reporting app) changes who is counted: The aggregate Reported metric (reported crime incidents) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (reported crime incidents) may be moving because the denominator/population changed after Intervention (Launching a reporting app). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-107",
        "bucket": "BucketLarge-J",
        "title": "Social Platform Safety Push",
        "scenario": "A policymaker implements an intervention: Stricter moderation.\nAfter the change, the reported metric average toxicity among remaining posts improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Stricter moderation)",
            "Y = Reported metric (average toxicity among remaining posts)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-107",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Digital Media",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average toxicity among remaining posts prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Stricter moderation) change who is included in the denominator before Reported metric (average toxicity among remaining posts) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average toxicity among remaining posts) after changing Intervention (Stricter moderation) can reflect a real outcome shift.\nAnswer if Intervention (Stricter moderation) changes who is counted: The aggregate Reported metric (average toxicity among remaining posts) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average toxicity among remaining posts) may be moving because the denominator/population changed after Intervention (Stricter moderation). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-108",
        "bucket": "BucketLarge-J",
        "title": "Workplace Safety Outsourcing",
        "scenario": "A policymaker implements an intervention: Outsourcing hazardous tasks.\nAfter the change, the reported metric injury rate among remaining employees improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Outsourcing hazardous tasks)",
            "Y = Reported metric (injury rate among remaining employees)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-108",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in injury rate among remaining employees prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Outsourcing hazardous tasks) change who is included in the denominator before Reported metric (injury rate among remaining employees) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (injury rate among remaining employees) after changing Intervention (Outsourcing hazardous tasks) can reflect a real outcome shift.\nAnswer if Intervention (Outsourcing hazardous tasks) changes who is counted: The aggregate Reported metric (injury rate among remaining employees) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (injury rate among remaining employees) may be moving because the denominator/population changed after Intervention (Outsourcing hazardous tasks). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-109",
        "bucket": "BucketLarge-J",
        "title": "Scholarship Program Expansion",
        "scenario": "A policymaker implements an intervention: Lowering eligibility threshold.\nAfter the change, the reported metric average GPA of scholarship recipients improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Lowering eligibility threshold)",
            "Y = Reported metric (average GPA of scholarship recipients)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-109",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average GPA of scholarship recipients prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Lowering eligibility threshold) change who is included in the denominator before Reported metric (average GPA of scholarship recipients) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average GPA of scholarship recipients) after changing Intervention (Lowering eligibility threshold) can reflect a real outcome shift.\nAnswer if Intervention (Lowering eligibility threshold) changes who is counted: The aggregate Reported metric (average GPA of scholarship recipients) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average GPA of scholarship recipients) may be moving because the denominator/population changed after Intervention (Lowering eligibility threshold). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-110",
        "bucket": "BucketLarge-J",
        "title": "Vaccination Priority Change",
        "scenario": "A policymaker implements an intervention: Expanding eligibility.\nAfter the change, the reported metric hospitalizations among vaccinated people improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Expanding eligibility)",
            "Y = Reported metric (hospitalizations among vaccinated people)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-110",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Public Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in hospitalizations among vaccinated people prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Expanding eligibility) change who is included in the denominator before Reported metric (hospitalizations among vaccinated people) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (hospitalizations among vaccinated people) after changing Intervention (Expanding eligibility) can reflect a real outcome shift.\nAnswer if Intervention (Expanding eligibility) changes who is counted: The aggregate Reported metric (hospitalizations among vaccinated people) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (hospitalizations among vaccinated people) may be moving because the denominator/population changed after Intervention (Expanding eligibility). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-111",
        "bucket": "BucketLarge-J",
        "title": "Loan Program Tightening",
        "scenario": "A policymaker implements an intervention: Raising credit score requirement.\nAfter the change, the reported metric default rate among approved borrowers improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Raising credit score requirement)",
            "Y = Reported metric (default rate among approved borrowers)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-111",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Labor Economics",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in default rate among approved borrowers prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Raising credit score requirement) change who is included in the denominator before Reported metric (default rate among approved borrowers) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (default rate among approved borrowers) after changing Intervention (Raising credit score requirement) can reflect a real outcome shift.\nAnswer if Intervention (Raising credit score requirement) changes who is counted: The aggregate Reported metric (default rate among approved borrowers) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (default rate among approved borrowers) may be moving because the denominator/population changed after Intervention (Raising credit score requirement). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-112",
        "bucket": "BucketLarge-J",
        "title": "University Admissions Expansion",
        "scenario": "A policymaker implements an intervention: Adding a new campus.\nAfter the change, the reported metric average SAT score of admitted students improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Adding a new campus)",
            "Y = Reported metric (average SAT score of admitted students)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-112",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Sociology",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average SAT score of admitted students prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Adding a new campus) change who is included in the denominator before Reported metric (average SAT score of admitted students) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average SAT score of admitted students) after changing Intervention (Adding a new campus) can reflect a real outcome shift.\nAnswer if Intervention (Adding a new campus) changes who is counted: The aggregate Reported metric (average SAT score of admitted students) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average SAT score of admitted students) may be moving because the denominator/population changed after Intervention (Adding a new campus). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-113",
        "bucket": "BucketLarge-J",
        "title": "Housing Voucher Reform",
        "scenario": "A policymaker implements an intervention: Changing voucher eligibility.\nAfter the change, the reported metric average rent paid by voucher holders improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Changing voucher eligibility)",
            "Y = Reported metric (average rent paid by voucher holders)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-113",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Public Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average rent paid by voucher holders prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Changing voucher eligibility) change who is included in the denominator before Reported metric (average rent paid by voucher holders) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average rent paid by voucher holders) after changing Intervention (Changing voucher eligibility) can reflect a real outcome shift.\nAnswer if Intervention (Changing voucher eligibility) changes who is counted: The aggregate Reported metric (average rent paid by voucher holders) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average rent paid by voucher holders) may be moving because the denominator/population changed after Intervention (Changing voucher eligibility). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-114",
        "bucket": "BucketLarge-J",
        "title": "Wellness Program Redesign",
        "scenario": "A policymaker implements an intervention: Changing sign-up incentives.\nAfter the change, the reported metric average weight loss among participants improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Changing sign-up incentives)",
            "Y = Reported metric (average weight loss among participants)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-114",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average weight loss among participants prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Changing sign-up incentives) change who is included in the denominator before Reported metric (average weight loss among participants) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average weight loss among participants) after changing Intervention (Changing sign-up incentives) can reflect a real outcome shift.\nAnswer if Intervention (Changing sign-up incentives) changes who is counted: The aggregate Reported metric (average weight loss among participants) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average weight loss among participants) may be moving because the denominator/population changed after Intervention (Changing sign-up incentives). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-115",
        "bucket": "BucketLarge-J",
        "title": "Traffic Enforcement Shift",
        "scenario": "A policymaker implements an intervention: Moving enforcement to highways.\nAfter the change, the reported metric average speed on arterial roads improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Moving enforcement to highways)",
            "Y = Reported metric (average speed on arterial roads)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-115",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average speed on arterial roads prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Moving enforcement to highways) change who is included in the denominator before Reported metric (average speed on arterial roads) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average speed on arterial roads) after changing Intervention (Moving enforcement to highways) can reflect a real outcome shift.\nAnswer if Intervention (Moving enforcement to highways) changes who is counted: The aggregate Reported metric (average speed on arterial roads) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average speed on arterial roads) may be moving because the denominator/population changed after Intervention (Moving enforcement to highways). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-116",
        "bucket": "BucketLarge-J",
        "title": "Teacher Evaluation Metric Change",
        "scenario": "A policymaker implements an intervention: Switching evaluation to test-score growth.\nAfter the change, the reported metric average teacher rating improves (or worsens) sharply.\nHowever, the intervention also changes *who is included* in the metric (the denominator or membership of the measured group), \nso the observed shift could be driven by population composition rather than any individual's outcome changing.",
        "variables": [
            "X = Intervention (Switching evaluation to test-score growth)",
            "Y = Reported metric (average teacher rating)",
            "M = membership / eligibility / inclusion rule (who is counted)",
            "Y_ind = individual-level outcome for a fixed person"
        ],
        "annotations": {
            "Case ID": "J2-116",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "X affects M (who is measured) and possibly Y_ind; Y aggregates over M, so Y can change via M alone.",
            "Key Insight": "Averages can change because the set of people counted changes, even if nobody improves.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the change in average teacher rating prove the intervention improved outcomes for the original residents/participants? Why or why not?\nWhat statistic would isolate changes in individual outcomes rather than composition?",
        "expected_analysis": "Identify composition effect: X changes membership M (eligibility, participation, reporting, or who remains).\nDecompose the metric change into (i) within-person changes Y_ind and (ii) changes due to who is counted.\nConclusion: The naive causal claim 'X improved outcomes' is CONDITIONAL; it is valid only if composition is held fixed or adjusted (e.g., follow a cohort over time).",
        "Hidden Timestamp": "Did Intervention (Switching evaluation to test-score growth) change who is included in the denominator before Reported metric (average teacher rating) was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Reported metric (average teacher rating) after changing Intervention (Switching evaluation to test-score growth) can reflect a real outcome shift.\nAnswer if Intervention (Switching evaluation to test-score growth) changes who is counted: The aggregate Reported metric (average teacher rating) can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Reported metric (average teacher rating) may be moving because the denominator/population changed after Intervention (Switching evaluation to test-score growth). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-117",
        "bucket": "BucketLarge-J",
        "title": "Voluntary Leadership Workshop",
        "scenario": "A decision-maker considers scaling an intervention: Attending the workshop.\nIn observational data, people who receive the intervention have better outcomes on subsequent performance rating than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (Attending the workshop)",
            "Y = Outcome (subsequent performance rating)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-117",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering Attending the workshop to everyone will improve subsequent performance rating? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (subsequent performance rating) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (Attending the workshop) is randomly assigned: A difference in Outcome (subsequent performance rating) across Intervention uptake (Attending the workshop) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Attending the workshop) vs not-Intervention uptake (Attending the workshop) difference in Outcome (subsequent performance rating) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Attending the workshop)\u2013Outcome (subsequent performance rating) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-118",
        "bucket": "BucketLarge-J",
        "title": "Optional SAT Prep Course",
        "scenario": "A decision-maker considers scaling an intervention: Taking the prep course.\nIn observational data, people who receive the intervention have better outcomes on SAT score improvement than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (Taking the prep course)",
            "Y = Outcome (SAT score improvement)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-118",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Education Sociology",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering Taking the prep course to everyone will improve SAT score improvement? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (SAT score improvement) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (Taking the prep course) is randomly assigned: A difference in Outcome (SAT score improvement) across Intervention uptake (Taking the prep course) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Taking the prep course) vs not-Intervention uptake (Taking the prep course) difference in Outcome (SAT score improvement) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Taking the prep course)\u2013Outcome (SAT score improvement) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-119",
        "bucket": "BucketLarge-J",
        "title": "Fitness App Premium Subscription",
        "scenario": "A decision-maker considers scaling an intervention: Upgrading to premium.\nIn observational data, people who receive the intervention have better outcomes on weekly workouts than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (Upgrading to premium)",
            "Y = Outcome (weekly workouts)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-119",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering Upgrading to premium to everyone will improve weekly workouts? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (weekly workouts) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (Upgrading to premium) is randomly assigned: A difference in Outcome (weekly workouts) across Intervention uptake (Upgrading to premium) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Upgrading to premium) vs not-Intervention uptake (Upgrading to premium) difference in Outcome (weekly workouts) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Upgrading to premium)\u2013Outcome (weekly workouts) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-120",
        "bucket": "BucketLarge-J",
        "title": "Mental Health Counseling",
        "scenario": "A decision-maker considers scaling an intervention: Starting therapy.\nIn observational data, people who receive the intervention have better outcomes on reported stress score than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (Starting therapy)",
            "Y = Outcome (reported stress score)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-120",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Psychology",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering Starting therapy to everyone will improve reported stress score? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (reported stress score) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (Starting therapy) is randomly assigned: A difference in Outcome (reported stress score) across Intervention uptake (Starting therapy) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Starting therapy) vs not-Intervention uptake (Starting therapy) difference in Outcome (reported stress score) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Starting therapy)\u2013Outcome (reported stress score) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-121",
        "bucket": "BucketLarge-J",
        "title": "Charter School Enrollment",
        "scenario": "A decision-maker considers scaling an intervention: Enrolling in charter school.\nIn observational data, people who receive the intervention have better outcomes on graduation likelihood than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (Enrolling in charter school)",
            "Y = Outcome (graduation likelihood)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-121",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Education Policy",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering Enrolling in charter school to everyone will improve graduation likelihood? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (graduation likelihood) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (Enrolling in charter school) is randomly assigned: A difference in Outcome (graduation likelihood) across Intervention uptake (Enrolling in charter school) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Enrolling in charter school) vs not-Intervention uptake (Enrolling in charter school) difference in Outcome (graduation likelihood) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Enrolling in charter school)\u2013Outcome (graduation likelihood) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-122",
        "bucket": "BucketLarge-J",
        "title": "Job Training Voucher",
        "scenario": "A decision-maker considers scaling an intervention: Using the voucher.\nIn observational data, people who receive the intervention have better outcomes on employment after 6 months than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (Using the voucher)",
            "Y = Outcome (employment after 6 months)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-122",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Labor Economics",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering Using the voucher to everyone will improve employment after 6 months? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (employment after 6 months) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (Using the voucher) is randomly assigned: A difference in Outcome (employment after 6 months) across Intervention uptake (Using the voucher) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (Using the voucher) vs not-Intervention uptake (Using the voucher) difference in Outcome (employment after 6 months) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (Using the voucher)\u2013Outcome (employment after 6 months) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-123",
        "bucket": "BucketLarge-J",
        "title": "Preventive Health Screening",
        "scenario": "A decision-maker considers scaling an intervention: getting screened.\nIn observational data, people who receive the intervention have better outcomes on later hospitalization than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (getting screened)",
            "Y = Outcome (later hospitalization)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-123",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Health Policy",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering getting screened to everyone will improve later hospitalization? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (later hospitalization) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (getting screened) is randomly assigned: A difference in Outcome (later hospitalization) across Intervention uptake (getting screened) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (getting screened) vs not-Intervention uptake (getting screened) difference in Outcome (later hospitalization) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (getting screened)\u2013Outcome (later hospitalization) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-124",
        "bucket": "BucketLarge-J",
        "title": "Remote Work Opt-In",
        "scenario": "A decision-maker considers scaling an intervention: opting into remote work.\nIn observational data, people who receive the intervention have better outcomes on output per hour than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (opting into remote work)",
            "Y = Outcome (output per hour)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-124",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Labor Economics",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering opting into remote work to everyone will improve output per hour? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (output per hour) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (opting into remote work) is randomly assigned: A difference in Outcome (output per hour) across Intervention uptake (opting into remote work) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (opting into remote work) vs not-Intervention uptake (opting into remote work) difference in Outcome (output per hour) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (opting into remote work)\u2013Outcome (output per hour) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-125",
        "bucket": "BucketLarge-J",
        "title": "Startup Accelerator Admission",
        "scenario": "A decision-maker considers scaling an intervention: joining accelerator.\nIn observational data, people who receive the intervention have better outcomes on funding raised than those who do not.\nBut participation is voluntary or constrained, so participants differ systematically from non-participants (e.g., motivation, resources, baseline risk).\nThe key question is whether *doing X* would change Y for the same person.",
        "variables": [
            "X = Intervention uptake (joining accelerator)",
            "Y = Outcome (funding raised)",
            "U = latent factors (motivation, baseline health/skill) affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-125",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Digital Media",
            "Causal Structure": "U \u2192 X and U \u2192 Y; naive comparison of treated vs untreated confounds effect of X with U.",
            "Key Insight": "Differences between treated and untreated may reflect who chooses/gets treated, not the treatment effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude that offering joining accelerator to everyone will improve funding raised? Why or why not?\nName one study design or adjustment that would better estimate the causal effect.",
        "expected_analysis": "Recognize selection into treatment. The treated group is not comparable to the untreated group because of U.\nEstimate causal effect via random assignment, matching on pre-treatment covariates, instrumental variables, or difference-in-differences if suitable.\nConclusion: The naive causal claim is INVALID without correcting for selection.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Outcome (funding raised) occurred\u2014and could selection depend on the outcome?",
        "Conditional Answers": "Answer if Intervention uptake (joining accelerator) is randomly assigned: A difference in Outcome (funding raised) across Intervention uptake (joining accelerator) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected: The Intervention uptake (joining accelerator) vs not-Intervention uptake (joining accelerator) difference in Outcome (funding raised) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection; otherwise Intervention uptake (joining accelerator)\u2013Outcome (funding raised) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-126",
        "bucket": "BucketLarge-J",
        "title": "Among Admitted Applicants",
        "scenario": "An analyst studies the relationship between test score and essay quality, but only within a selected group defined by admission decision.\nInside that selected group, test score and essay quality appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (admission decision) is influenced by both test score and essay quality, making it a collider.",
        "variables": [
            "A = test score",
            "B = essay quality",
            "S = selection criterion (admission decision)",
            "Y = Outcome (first-year GPA)"
        ],
        "annotations": {
            "Case ID": "J2-126",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (first-year GPA)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (first-year GPA) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-127",
        "bucket": "BucketLarge-J",
        "title": "Among Promoted Employees",
        "scenario": "An analyst studies the relationship between hours worked and manager liking, but only within a selected group defined by promotion.\nInside that selected group, hours worked and manager liking appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (promotion) is influenced by both hours worked and manager liking, making it a collider.",
        "variables": [
            "A = hours worked",
            "B = manager liking",
            "S = selection criterion (promotion)",
            "Y = Outcome (later performance)"
        ],
        "annotations": {
            "Case ID": "J2-127",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (later performance)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (later performance) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-128",
        "bucket": "BucketLarge-J",
        "title": "Among Hospitalized Patients",
        "scenario": "An analyst studies the relationship between age and comorbidity index, but only within a selected group defined by hospitalization.\nInside that selected group, age and comorbidity index appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (hospitalization) is influenced by both age and comorbidity index, making it a collider.",
        "variables": [
            "A = age",
            "B = comorbidity index",
            "S = selection criterion (hospitalization)",
            "Y = Outcome (mortality)"
        ],
        "annotations": {
            "Case ID": "J2-128",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Health Policy",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (mortality)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (mortality) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-129",
        "bucket": "BucketLarge-J",
        "title": "Among Viral Posts",
        "scenario": "An analyst studies the relationship between topic controversy and creator follower count, but only within a selected group defined by going viral.\nInside that selected group, topic controversy and creator follower count appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (going viral) is influenced by both topic controversy and creator follower count, making it a collider.",
        "variables": [
            "A = topic controversy",
            "B = creator follower count",
            "S = selection criterion (going viral)",
            "Y = Outcome (misinformation rate)"
        ],
        "annotations": {
            "Case ID": "J2-129",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Digital Media",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (misinformation rate)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (misinformation rate) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-130",
        "bucket": "BucketLarge-J",
        "title": "Among Scholarship Winners",
        "scenario": "An analyst studies the relationship between family income and grades, but only within a selected group defined by winning scholarship.\nInside that selected group, family income and grades appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (winning scholarship) is influenced by both family income and grades, making it a collider.",
        "variables": [
            "A = family income",
            "B = grades",
            "S = selection criterion (winning scholarship)",
            "Y = Outcome (college persistence)"
        ],
        "annotations": {
            "Case ID": "J2-130",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Education Sociology",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (college persistence)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (college persistence) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-131",
        "bucket": "BucketLarge-J",
        "title": "Among Loan Approvals",
        "scenario": "An analyst studies the relationship between income and credit history, but only within a selected group defined by loan approval.\nInside that selected group, income and credit history appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (loan approval) is influenced by both income and credit history, making it a collider.",
        "variables": [
            "A = income",
            "B = credit history",
            "S = selection criterion (loan approval)",
            "Y = Outcome (default)"
        ],
        "annotations": {
            "Case ID": "J2-131",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Labor Economics",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (default)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (default) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-132",
        "bucket": "BucketLarge-J",
        "title": "Among Arrested Individuals",
        "scenario": "An analyst studies the relationship between crime severity and police presence, but only within a selected group defined by being arrested.\nInside that selected group, crime severity and police presence appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (being arrested) is influenced by both crime severity and police presence, making it a collider.",
        "variables": [
            "A = crime severity",
            "B = police presence",
            "S = selection criterion (being arrested)",
            "Y = Outcome (conviction)"
        ],
        "annotations": {
            "Case ID": "J2-132",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Criminology",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (conviction)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (conviction) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-133",
        "bucket": "BucketLarge-J",
        "title": "Among Top-Sales Reps",
        "scenario": "An analyst studies the relationship between territory quality and call volume, but only within a selected group defined by being top-10% in sales.\nInside that selected group, territory quality and call volume appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (being top-10% in sales) is influenced by both territory quality and call volume, making it a collider.",
        "variables": [
            "A = territory quality",
            "B = call volume",
            "S = selection criterion (being top-10% in sales)",
            "Y = Outcome (customer churn)"
        ],
        "annotations": {
            "Case ID": "J2-133",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (customer churn)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (customer churn) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-134",
        "bucket": "BucketLarge-J",
        "title": "Among Published Papers",
        "scenario": "An analyst studies the relationship between study novelty and p-value, but only within a selected group defined by publication.\nInside that selected group, study novelty and p-value appear strongly correlated (sometimes even negatively correlated), and the analyst interprets this as a real causal link.\nHowever, membership in the group (publication) is influenced by both study novelty and p-value, making it a collider.",
        "variables": [
            "A = study novelty",
            "B = p-value",
            "S = selection criterion (publication)",
            "Y = Outcome (replication success)"
        ],
        "annotations": {
            "Case ID": "J2-134",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Psychology",
            "Causal Structure": "A \u2192 S \u2190 B (collider). Conditioning on S induces a spurious association between A and B; can mislead causal interpretation for Y.",
            "Key Insight": "Conditioning on a collider creates correlations that do not reflect causal relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why might A and B look correlated within the group even if they are independent in the full population?\nHow could this distort conclusions about what causes Y?",
        "expected_analysis": "Explain collider bias: S is affected by both A and B. Restricting analysis to S=1 opens a non-causal path between A and B.\nTherefore, associations among selected cases cannot be interpreted causally without modeling the selection mechanism.\nConclusion: The within-group correlation is spurious; causal claims are INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on a post-selection variable that occurs after factors affecting both X (treatment/exposure) and Outcome (replication success)?",
        "Conditional Answers": "Answer if you condition on a post-selection variable: Associations between X and Outcome (replication success) can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable, it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-135",
        "bucket": "BucketLarge-J",
        "title": "More Break-ins After Installing Cameras?",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of number of break-in reports.\nThey point to a larger number of events in one group or after installing cameras.\nBut the groups have very different base sizes or exposure levels (neighborhood size differs), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (installing cameras)",
            "Y = Event count (number of break-in reports)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-135",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Criminology",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (number of break-in reports) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (number of break-in reports): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (number of break-in reports) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (number of break-in reports) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-136",
        "bucket": "BucketLarge-J",
        "title": "Side Effects After New Medication Rollout",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of side-effect reports.\nThey point to a larger number of events in one group or after new medication rollout.\nBut the groups have very different base sizes or exposure levels (exposure population differs), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (new medication rollout)",
            "Y = Event count (side-effect reports)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-136",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Health Policy",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (side-effect reports) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (side-effect reports): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (side-effect reports) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (side-effect reports) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-137",
        "bucket": "BucketLarge-J",
        "title": "Discipline Disparities After Rule Change",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of discipline referrals.\nThey point to a larger number of events in one group or after new conduct rule.\nBut the groups have very different base sizes or exposure levels (group enrollment differs), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (new conduct rule)",
            "Y = Event count (discipline referrals)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-137",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Education Policy",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (discipline referrals) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (discipline referrals): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (discipline referrals) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (discipline referrals) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-138",
        "bucket": "BucketLarge-J",
        "title": "Fraud Alerts in Two Stores",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of fraud alerts.\nThey point to a larger number of events in one group or after fraud filter in Store A vs B.\nBut the groups have very different base sizes or exposure levels (transaction volume differs), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (fraud filter in Store A vs B)",
            "Y = Event count (fraud alerts)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-138",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (fraud alerts) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (fraud alerts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (fraud alerts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (fraud alerts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-139",
        "bucket": "BucketLarge-J",
        "title": "Misinformation Flags Across Topics",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of flagged posts.\nThey point to a larger number of events in one group or after topic category.\nBut the groups have very different base sizes or exposure levels (posting volume differs), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (topic category)",
            "Y = Event count (flagged posts)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-139",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Digital Media",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (flagged posts) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (flagged posts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (flagged posts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (flagged posts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-140",
        "bucket": "BucketLarge-J",
        "title": "Accidents in Two Factories",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of injury counts.\nThey point to a larger number of events in one group or after safety training.\nBut the groups have very different base sizes or exposure levels (work-hours differ), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (safety training)",
            "Y = Event count (injury counts)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-140",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (injury counts) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (injury counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (injury counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (injury counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-141",
        "bucket": "BucketLarge-J",
        "title": "Defaults in Two Loan Products",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of default counts.\nThey point to a larger number of events in one group or after loan product type.\nBut the groups have very different base sizes or exposure levels (number of loans differs), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (loan product type)",
            "Y = Event count (default counts)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-141",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (default counts) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (default counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (default counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (default counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-142",
        "bucket": "BucketLarge-J",
        "title": "Hospitalizations Among Vaccinated vs Unvaccinated",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of hospitalized counts.\nThey point to a larger number of events in one group or after vaccination status.\nBut the groups have very different base sizes or exposure levels (base rate of vaccination differs), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (vaccination status)",
            "Y = Event count (hospitalized counts)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-142",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Public Policy",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (hospitalized counts) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (hospitalized counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (hospitalized counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (hospitalized counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-143",
        "bucket": "BucketLarge-J",
        "title": "Complaints After Service Plan Launch",
        "scenario": "A stakeholder argues that an intervention or group is riskier based on raw counts of complaint counts.\nThey point to a larger number of events in one group or after service plan launch.\nBut the groups have very different base sizes or exposure levels (customer base changed), so comparing counts without denominators is misleading.",
        "variables": [
            "X = Group/intervention status (service plan launch)",
            "Y = Event count (complaint counts)",
            "N = population size or exposure (denominator)",
            "R = event rate = Y / N"
        ],
        "annotations": {
            "Case ID": "J2-143",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Easy",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "Counts Y depend on both rate R and exposure N; N differs across X, so Y alone does not identify R.",
            "Key Insight": "A larger count can correspond to a lower rate when the base population is larger.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to conclude the intervention/group increases risk from counts alone? What should be compared instead?\nWhat additional information is needed to answer causally whether X changes the event rate?",
        "expected_analysis": "Diagnose base-rate neglect: counts confound the event rate with the size of the exposed population.\nCompute/compare rates (per capita, per transaction, per hour) and ensure comparable exposure windows.\nConclusion: The count-based causal claim is INVALID; it becomes valid only after normalizing by N and considering confounders.",
        "Hidden Timestamp": "What is the relevant denominator at the time Event count (complaint counts) was counted, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Event count (complaint counts): You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Event count (complaint counts) with the correct denominator: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Event count (complaint counts) are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the relevant denominator (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-144",
        "bucket": "BucketLarge-J",
        "title": "Would I Have Been Admitted If I Retook the Test?",
        "scenario": "Applicant retakes standardized test after a rejection.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Retake decision",
            "Y = Admission outcome",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-144",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Retake decision was (or could have been) chosen, what information was available, and when was Admission outcome realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Retake decision is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-145",
        "bucket": "BucketLarge-J",
        "title": "Turning Down a Job Offer and Later Regret",
        "scenario": "A candidate declines a job offer and later experiences a downturn in their current job.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Accept offer",
            "Y = Income stability after 1 year",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-145",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Preemption",
            "Trap Subtype": "Late Preemption",
            "Difficulty": "Medium",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Accept offer was (or could have been) chosen, what information was available, and when was Income stability after 1 year realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Accept offer is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-146",
        "bucket": "BucketLarge-J",
        "title": "Parole Decision and Future Crime",
        "scenario": "A parole board denies release; later debates whether release would have increased reoffending.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Release on parole",
            "Y = Reoffense within 2 years",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-146",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Preemption",
            "Trap Subtype": "Late Preemption",
            "Difficulty": "Medium",
            "Subdomain": "Criminology",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Release on parole was (or could have been) chosen, what information was available, and when was Reoffense within 2 years realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Release on parole is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-147",
        "bucket": "BucketLarge-J",
        "title": "Medical Treatment Refusal",
        "scenario": "A patient refuses a recommended procedure and later recovers slowly.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Receive procedure",
            "Y = Recovery time",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-147",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Medium",
            "Subdomain": "Health Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Receive procedure was (or could have been) chosen, what information was available, and when was Recovery time realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Receive procedure is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-148",
        "bucket": "BucketLarge-J",
        "title": "Choosing a Major Switch",
        "scenario": "A student stays in a difficult major and wonders if switching would have improved GPA.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Switch major",
            "Y = GPA after two semesters",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-148",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Feedback Loops",
            "Trap Subtype": "Dynamic World Divergence",
            "Difficulty": "Medium",
            "Subdomain": "Education Sociology",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Switch major was (or could have been) chosen, what information was available, and when was GPA after two semesters realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Switch major is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-149",
        "bucket": "BucketLarge-J",
        "title": "Scholarship Cutoff Appeal",
        "scenario": "A student is just below a scholarship cutoff; committee debates if scholarship would change graduation probability.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Receive scholarship",
            "Y = Graduation",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-149",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Receive scholarship was (or could have been) chosen, what information was available, and when was Graduation realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Receive scholarship is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-150",
        "bucket": "BucketLarge-J",
        "title": "Minimum Wage Increase and Employment",
        "scenario": "A city raises minimum wage; policymakers ask what employment would have been without the change.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Minimum wage increase",
            "Y = Employment rate",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-150",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounder\u2013Mediator Error",
            "Trap Subtype": "Mediator Fixing Error",
            "Difficulty": "Medium",
            "Subdomain": "Public Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Minimum wage increase was (or could have been) chosen, what information was available, and when was Employment rate realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Minimum wage increase is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-151",
        "bucket": "BucketLarge-J",
        "title": "Congestion Pricing Rollout",
        "scenario": "Congestion pricing is introduced; the city asks what traffic would have been otherwise.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Introduce congestion pricing",
            "Y = Average travel time",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-151",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Feedback Loops",
            "Trap Subtype": "Dynamic World Divergence",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Introduce congestion pricing was (or could have been) chosen, what information was available, and when was Average travel time realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Introduce congestion pricing is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-152",
        "bucket": "BucketLarge-J",
        "title": "School Funding Reform",
        "scenario": "A district increases funding to low-income schools; asks what scores would have been absent reform.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Funding reform",
            "Y = Standardized test scores",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-152",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Preemption",
            "Trap Subtype": "Early Preemption",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Funding reform was (or could have been) chosen, what information was available, and when was Standardized test scores realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Funding reform is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-153",
        "bucket": "BucketLarge-J",
        "title": "Housing Construction Moratorium",
        "scenario": "A construction ban is enacted; residents debate what rents would have been without the ban.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Construction moratorium",
            "Y = Median rent",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-153",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Construction moratorium was (or could have been) chosen, what information was available, and when was Median rent realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Construction moratorium is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-154",
        "bucket": "BucketLarge-J",
        "title": "Environmental Regulation Change",
        "scenario": "A new emissions rule is enacted; regulators ask what pollution levels would have been without it.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Emissions regulation",
            "Y = Air pollutant concentration",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-154",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Public Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Emissions regulation was (or could have been) chosen, what information was available, and when was Air pollutant concentration realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Emissions regulation is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-155",
        "bucket": "BucketLarge-J",
        "title": "Public Transit Expansion",
        "scenario": "New bus routes are added; question is what car usage would have been otherwise.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Transit expansion",
            "Y = Car trips per capita",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-155",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounder\u2013Mediator Error",
            "Trap Subtype": "Mediator Fixing Error",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Transit expansion was (or could have been) chosen, what information was available, and when was Car trips per capita realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Transit expansion is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-156",
        "bucket": "BucketLarge-J",
        "title": "Algorithmic Fairness Audit: Who Was Harmed?",
        "scenario": "A recommender model is updated; auditors ask how outcomes would differ for users under the old model.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Model version",
            "Y = Exposure to harmful content",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-156",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounder\u2013Mediator Error",
            "Trap Subtype": "Mediator Fixing Error",
            "Difficulty": "Medium",
            "Subdomain": "Digital Media",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Model version was (or could have been) chosen, what information was available, and when was Exposure to harmful content realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Model version is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-157",
        "bucket": "BucketLarge-J",
        "title": "Discrimination Attribution in Hiring",
        "scenario": "A candidate is rejected; dispute whether rejection was due to bias or fit.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Candidate protected attribute",
            "Y = Hiring decision",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-157",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Preemption",
            "Trap Subtype": "Early Preemption",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Candidate protected attribute was (or could have been) chosen, what information was available, and when was Hiring decision realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Candidate protected attribute is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-158",
        "bucket": "BucketLarge-J",
        "title": "Quota Policy Evaluation",
        "scenario": "A university adds a quota; asks how composition and outcomes would look without it.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Quota policy",
            "Y = Graduation outcomes across groups",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-158",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Feedback Loops",
            "Trap Subtype": "Dynamic World Divergence",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Quota policy was (or could have been) chosen, what information was available, and when was Graduation outcomes across groups realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Quota policy is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-159",
        "bucket": "BucketLarge-J",
        "title": "Economic Stimulus Effectiveness",
        "scenario": "Stimulus checks are issued; question is what spending would have been absent stimulus.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Stimulus payment",
            "Y = Household spending",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-159",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Public Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Stimulus payment was (or could have been) chosen, what information was available, and when was Household spending realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Stimulus payment is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-160",
        "bucket": "BucketLarge-J",
        "title": "Election Outcome Counterfactual",
        "scenario": "A close election; analysts ask what would have happened if turnout rules differed.\nYou observe the realized outcome Y\u2081 under the action/policy as implemented (or under the actual decision taken).\nThe key question is the counterfactual outcome Y\u2080: what would have happened under the alternative action/policy, holding the relevant context fixed.\nBecause only one world is observed, Y\u2080 is fundamentally unobserved for the same unit at the same time.",
        "variables": [
            "X = Turnout rule change",
            "Y = Election winner",
            "Y1 = observed outcome under actual world",
            "Y0 = counterfactual outcome under alternative world",
            "U = unobserved factors affecting both X and Y"
        ],
        "annotations": {
            "Case ID": "J2-160",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Feedback Loops",
            "Trap Subtype": "Dynamic World Divergence",
            "Difficulty": "Hard",
            "Subdomain": "Public Policy",
            "Causal Structure": "Need to compare Y1 and Y0 for the same unit; Y0 is unobserved and may differ due to U or feedback over time.",
            "Key Insight": "Counterfactual claims require assumptions (exchangeability, no unmeasured confounding, stable dynamics) and careful definition of the alternative world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "State the observed outcome (Y\u2081) and define precisely the counterfactual outcome (Y\u2080) for this case.\nWhy is Y\u2080 unobserved, and what makes it hard to estimate?\nGive one condition under which the counterfactual claim 'X caused Y' would be VALID, and one under which it would be INVALID.",
        "expected_analysis": "Explicitly articulate Y\u2081 vs Y\u2080 for the same unit/time.\nIdentify the main obstacle (unobserved U, policy endogeneity, selection into worlds, or feedback loops).\nPropose an identification strategy appropriate to the context (randomization, natural experiment, diff-in-diff, regression discontinuity, instrumental variables, or causal model with strong assumptions).\nConclude the claim is CONDITIONAL: it is valid only under stated assumptions about how the alternative world would evolve and about U.",
        "Hidden Timestamp": "At the decision time when Turnout rule change was (or could have been) chosen, what information was available, and when was Election winner realized relative to that choice?",
        "Conditional Answers": "Answer if you assume comparability (no unmeasured confounding): You may use matched controls/adjustment to estimate the counterfactual, but the claim is CONDITIONAL on the assumptions.\nAnswer if the choice of Turnout rule change is driven by unobserved factors: The individual/policy counterfactual is not identifiable from observational data; the claim is UNDETERMINED.\nAnswer if you have an experimental or quasi-experimental design (randomization, instrument, regression discontinuity): You can estimate the relevant counterfactual effect for a defined target population.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The counterfactual outcome is unobserved; without a design that makes treated and untreated comparable, the claim depends on untestable assumptions. Provide a clear identification strategy (experiment, IV, RD, or sensitivity bounds)."
    },
    {
        "id": "T3-BucketJ-161",
        "bucket": "BucketLarge-J",
        "title": "Scholarship Acceptance Rates by Prep Program",
        "scenario": "A nonprofit reports that applicants from Prep Program A are accepted into selective colleges at a higher overall rate than applicants from Prep Program B. Donors conclude Program A is more effective and recommend expanding it.\n\nA counselor points out that when acceptance rates are broken down by family income bracket (low-income vs higher-income), Program B has a higher acceptance rate in both brackets. Program A has more higher-income participants overall, while Program B serves mostly low-income participants.",
        "variables": [
            "X = Prep program (A vs. B)",
            "Y = College acceptance rate",
            "Z = Family income bracket (low vs. higher)"
        ],
        "annotations": {
            "Case ID": "J2-161",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Aggregation Bias",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z affects Y and differs in distribution across X; aggregating across Z reverses the subgroup acceptance-rate comparison.",
            "Key Insight": "Aggregate success rates can reverse within-group trends when group composition differs.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the higher overall acceptance rate for Program A imply it is more effective than Program B?\nWhat happens to the comparison if you condition on family income bracket?",
        "expected_analysis": "Identify Simpson\u2019s Paradox: the aggregate difference is driven by different income composition across programs.\nIncome bracket (Z) predicts acceptance (Y) and differs across programs (X), so the overall rate is not a fair comparison.\nWithin each income bracket, Program B performs better, suggesting Program A\u2019s advantage is compositional.\nConclusion: The claim \u201cProgram A is more effective\u201d is INVALID based on the aggregate statistic alone.",
        "Hidden Timestamp": "Was family income bracket known before students enrolled in Prep Program A vs. B, making it a true pre-enrollment stratifier?",
        "Conditional Answers": "Answer if you only use overall acceptance rates: You may wrongly prefer Program A because it serves more higher-income students.\nAnswer if you stratify or standardize by income bracket: Compare within strata (or compute standardized rates) to evaluate program performance.\nAnswer if assignment to programs can be randomized within income strata: Then within-stratum differences can support a causal claim about program effectiveness.",
        "Wise Refusal": "I can\u2019t determine which program is causally better without baseline comparability (prior grades, support) and a credible assignment mechanism."
    },
    {
        "id": "T3-BucketJ-162",
        "bucket": "BucketLarge-J",
        "title": "Two Banks and the Default Rate",
        "scenario": "A city report states that Bank X has a lower overall loan default rate than Bank Y, and officials argue Bank X has better underwriting practices.\n\nWhen analysts stratify borrowers by credit-risk tier (prime vs subprime), Bank Y has a lower default rate in both tiers. The reversal occurs because Bank X approves a larger share of prime borrowers, while Bank Y serves more subprime borrowers overall.",
        "variables": [
            "X = Bank (X vs. Y)",
            "Y = Loan default rate",
            "Z = Credit-risk tier (prime vs. subprime)"
        ],
        "annotations": {
            "Case ID": "J2-162",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Imbalanced Group Composition",
            "Difficulty": "Medium",
            "Subdomain": "Finance",
            "Causal Structure": "Z strongly affects Y and the distribution of Z differs by X; aggregate defaults reverse the within-tier comparison.",
            "Key Insight": "Comparing institutions using aggregate outcomes is misleading when they serve different mixes of clients.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does Bank X\u2019s lower overall default rate prove it underwrites better than Bank Y?\nWhy can the conclusion reverse after stratifying by credit-risk tier?",
        "expected_analysis": "This is Simpson\u2019s Paradox caused by imbalanced risk composition.\nRisk tier (Z) drives default (Y). Banks differ in borrower mix, so overall default rates conflate underwriting with portfolio composition.\nWithin both tiers, Bank Y performs better.\nConclusion: The claim \u201cBank X has better underwriting\u201d is INVALID from the aggregate comparison alone.",
        "Hidden Timestamp": "Is credit-risk tier assessed before borrowers choose a bank, making Z pre-treatment with respect to X?",
        "Conditional Answers": "Answer if you compare only overall defaults: You may incorrectly attribute Bank X\u2019s lower defaults to better underwriting.\nAnswer if you compare within risk tiers or compute standardized defaults: Prefer the bank with lower within-tier defaults (and report a standardized overall rate).\nAnswer if approval decisions themselves change the portfolio mix: Separate \u201cmix effects\u201d from \u201cwithin-tier performance.\u201d",
        "Wise Refusal": "I can\u2019t judge underwriting quality without consistent risk-tier measurement and a standardized comparison across the same risk distribution."
    },
    {
        "id": "T3-BucketJ-163",
        "bucket": "BucketLarge-J",
        "title": "Clinic Readmissions by Age Mix",
        "scenario": "A hospital network claims Clinic A provides better follow-up care because its overall 30-day readmission rate is lower than Clinic B\u2019s.\n\nWhen readmissions are broken down by patient age group (under 65 vs 65+), Clinic B has lower readmission rates in both groups. Clinic A treats a much larger share of younger patients, while Clinic B treats more older patients overall.",
        "variables": [
            "X = Clinic (A vs. B)",
            "Y = 30-day readmission rate",
            "Z = Age group (under 65 vs. 65+)"
        ],
        "annotations": {
            "Case ID": "J2-163",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Aggregation Bias",
            "Difficulty": "Medium",
            "Subdomain": "Healthcare Administration",
            "Causal Structure": "Z affects Y and differs by clinic; aggregating across Z reverses the subgroup comparison.",
            "Key Insight": "Overall metrics can hide within-group differences driven by demographic mix.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does Clinic A\u2019s lower overall readmission rate imply better follow-up care?\nWhat does the comparison show within each age group?",
        "expected_analysis": "Simpson\u2019s Paradox: age group (Z) strongly affects readmission (Y) and is uneven across clinics (X).\nClinic A\u2019s overall advantage can be explained by treating more younger patients.\nWithin both age groups, Clinic B performs better.\nConclusion: The claim \u201cClinic A has better follow-up care\u201d is INVALID from the overall rate alone.",
        "Hidden Timestamp": "Was age determined before clinic selection (a true pre-treatment stratifier), or does clinic choice affect which ages are seen?",
        "Conditional Answers": "Answer if you only look at overall readmissions: You might reward Clinic A incorrectly.\nAnswer if you age-standardize or compare within age strata: The within-stratum comparison is more informative for care quality.\nAnswer if clinics differ in referral patterns: Adjust for case mix and referral selection before drawing conclusions.",
        "Wise Refusal": "I can\u2019t attribute readmission differences to care quality without case-mix adjustment and information on referral/triage processes."
    },
    {
        "id": "T3-BucketJ-164",
        "bucket": "BucketLarge-J",
        "title": "The Employee Satisfaction Survey",
        "scenario": "A company emails an anonymous satisfaction survey and reports that \u201c85% of employees are satisfied.\u201d Leadership concludes morale is high.\n\nOnly 30% of employees responded. Dissatisfied employees may be less likely to respond because they think nothing will change or fear being identified.",
        "variables": [
            "X = Survey response (respond vs. not)",
            "Y = Reported satisfaction",
            "Z = Dissatisfaction / fear of retaliation"
        ],
        "annotations": {
            "Case ID": "J2-164",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Easy",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Z affects response and satisfaction; conditioning on responders yields biased satisfaction estimate.",
            "Key Insight": "Survey results can be unrepresentative when response is selective.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does 85% satisfaction among respondents represent satisfaction among all employees?\nWhat selection mechanism could bias the observed satisfaction rate?",
        "expected_analysis": "Selection bias via sampling-on-the-outcome: only responders are observed.\nIf dissatisfied employees respond less, the observed satisfaction rate overestimates true satisfaction.\nConclusion: The claim \u201cmorale is high company-wide\u201d is INVALID from respondent-only data.",
        "Hidden Timestamp": "Did dissatisfaction exist before employees chose whether to respond, or did the survey context change willingness to respond?",
        "Conditional Answers": "Answer if response is random: Then the respondent rate estimates overall morale.\nAnswer if dissatisfied employees avoid responding: Then results are biased upward; improve sampling/response incentives.\nAnswer if response differs by team or manager: Use stratified follow-ups to assess representation.",
        "Wise Refusal": "I can\u2019t infer company-wide morale without response-rate patterns and information about non-respondents."
    },
    {
        "id": "T3-BucketJ-165",
        "bucket": "BucketLarge-J",
        "title": "Studying Only Award-Winning Films",
        "scenario": "A blog analyzes \u201cwhat makes a film profitable\u201d by looking only at movies that won major awards. The author finds that award-winning films have high budgets and concludes that increasing budgets causes higher profits.\n\nThe dataset excludes many high-budget films that were not nominated or that failed commercially. Award status is a filter for inclusion.",
        "variables": [
            "X = Production budget",
            "Y = Profit",
            "Z = Award nomination/win status"
        ],
        "annotations": {
            "Case ID": "J2-165",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Medium",
            "Subdomain": "Media Economics",
            "Causal Structure": "Conditioning on Z (award inclusion) selects a non-representative subset; X\u2013Y relationship differs from full population.",
            "Key Insight": "Analyzing only visible successes distorts relationships between inputs and outcomes.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the budget\u2013profit pattern among award winners imply bigger budgets cause profits?\nWhy does restricting to award winners bias the analysis?",
        "expected_analysis": "Selection bias: conditioning on award status creates a selected sample.\nAwards correlate with many factors and exclude failures, so the remaining sample is not representative.\nConclusion: The claim \u201cbigger budgets cause higher profits\u201d is INVALID from award-winner-only data.",
        "Hidden Timestamp": "Is award status determined before profits are realized, or is the dataset effectively filtered after outcomes are known?",
        "Conditional Answers": "Answer if awards are unrelated to profit (unlikely): Restriction would be less problematic.\nAnswer if awards filter on quality/visibility correlated with profit: Include all films to avoid selection bias.\nAnswer if you can model nomination probability: Any correction remains assumption-dependent.",
        "Wise Refusal": "I can\u2019t draw a causal conclusion without data on non-award films and a clear sampling frame that includes both successes and failures."
    },
    {
        "id": "T3-BucketJ-166",
        "bucket": "BucketLarge-J",
        "title": "Dropout Bias in a Workforce Program",
        "scenario": "A city runs a six-month workforce training program and reports that participants\u2019 average wages increased by 20%. Officials conclude the program boosted earnings.\n\nHowever, 40% of participants dropped out and are missing from the final wage measurement. The report analyzes only participants who completed the program.",
        "variables": [
            "X = Training program enrollment",
            "Y = Post-program wage",
            "Z = Program completion (complete vs. dropout)"
        ],
        "annotations": {
            "Case ID": "J2-166",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Attrition Bias",
            "Difficulty": "Medium",
            "Subdomain": "Labor Policy",
            "Causal Structure": "Completion is selected post-enrollment; completers differ from dropouts in factors affecting Y.",
            "Key Insight": "Attrition can make observed improvements unrepresentative of the full enrolled group.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the 20% wage increase among completers apply to all enrollees?\nHow can attrition bias the reported outcome?",
        "expected_analysis": "Attrition bias: outcomes are observed only for completers.\nIf dropouts would have had lower gains, analyzing only completers overstates average improvement.\nConclusion: The claim \u201cparticipants gained 20%\u201d is INVALID without accounting for missing outcomes.",
        "Hidden Timestamp": "Did dropout occur after participants could have benefited (or been harmed) by the program, making completion a post-treatment selection?",
        "Conditional Answers": "Answer if dropout is random: Completer outcomes may approximate all enrollees.\nAnswer if dropout is related to constraints or low baseline readiness: Use intent-to-treat or obtain administrative wage data for dropouts.\nAnswer if dropouts can be followed up: Measuring outcomes for all reduces attrition bias.",
        "Wise Refusal": "I can\u2019t estimate overall impact without outcomes (or credible imputations) for dropouts and evidence about why participants left."
    },
    {
        "id": "T3-BucketJ-167",
        "bucket": "BucketLarge-J",
        "title": "Tutoring and Test Scores in Two Neighborhoods",
        "scenario": "A school district observes that students who attend private tutoring have higher math scores than students who do not. Administrators conclude tutoring causes higher scores.\n\nStudents who get tutoring are more likely to come from higher-income families with more educational resources at home.",
        "variables": [
            "X = Tutoring attendance",
            "Y = Math test score",
            "Z = Socioeconomic resources"
        ],
        "annotations": {
            "Case ID": "J2-167",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Socioeconomic",
            "Difficulty": "Easy",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z influences both X and Y, creating a spurious X\u2013Y association.",
            "Key Insight": "A common cause can make correlation look like causation.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the association between tutoring and scores prove tutoring causes gains?\nWhat socioeconomic confounders could explain both tutoring and higher scores?",
        "expected_analysis": "Socioeconomic confounding: resources (Z) affect tutoring access (X) and test performance (Y).\nWithout controlling for Z or using a causal design, the tutoring\u2013score gap is not a causal effect.\nConclusion: The claim \u201ctutoring causes higher scores\u201d is INVALID from the observational association alone.",
        "Hidden Timestamp": "Were family resources present before tutoring began, making them a baseline confounder?",
        "Conditional Answers": "Answer if tutoring were randomly assigned: Score differences could be attributed to tutoring.\nAnswer if tutoring is purchased mostly by higher-resource families: Adjust for baseline resources and prior scores; residual confounding may remain.\nAnswer if comparing within the same school and income bracket: Confounding is reduced but not eliminated.",
        "Wise Refusal": "I can\u2019t make a causal claim without baseline scores, resource measures, and a credible strategy to compare similar students."
    },
    {
        "id": "T3-BucketJ-168",
        "bucket": "BucketLarge-J",
        "title": "Ice Cream Sales and Park Injuries",
        "scenario": "A city analyst finds that days with higher ice cream sales in parks also have more reported minor injuries. They argue ice cream vendors create unsafe conditions.\n\nA parks manager suggests that hot weather increases both park attendance (creating more injury opportunities) and ice cream sales.",
        "variables": [
            "X = Ice cream sales",
            "Y = Park injury reports",
            "Z = Temperature / attendance"
        ],
        "annotations": {
            "Case ID": "J2-168",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Omitted Variable",
            "Difficulty": "Easy",
            "Subdomain": "Urban Policy",
            "Causal Structure": "Z increases both X and Y; omitting Z yields misleading correlation.",
            "Key Insight": "An omitted common cause can create a misleading association.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the correlation between ice cream sales and injuries imply vendors cause injuries?\nWhat omitted variable could drive both ice cream sales and injuries?",
        "expected_analysis": "Omitted-variable confounding: temperature/attendance (Z) increases both sales (X) and injuries (Y).\nCorrelation can appear even if vendors do not affect injuries.\nConclusion: The claim \u201cice cream sales cause injuries\u201d is INVALID from the raw correlation.",
        "Hidden Timestamp": "Did temperature rise before both ice cream sales and injuries, suggesting a shared upstream cause?",
        "Conditional Answers": "Answer if you control for temperature and attendance: The sales\u2013injury correlation may disappear.\nAnswer if injuries rise on high-attendance days regardless of vendors: Vendor restriction is not justified by this correlation.\nAnswer if vendor placement is randomized across similar days: Comparisons become more informative.",
        "Wise Refusal": "I can\u2019t conclude causality without controlling for weather/attendance and knowing how vendor permits are assigned."
    },
    {
        "id": "T3-BucketJ-169",
        "bucket": "BucketLarge-J",
        "title": "Stronger Painkillers and Mortality",
        "scenario": "A hospital audit reports that patients prescribed a stronger painkiller have higher 30-day mortality than patients prescribed a milder painkiller. A supervisor argues the stronger drug is dangerous.\n\nClinicians respond that the stronger painkiller is typically prescribed to patients with more severe conditions, who are already at higher risk of death.",
        "variables": [
            "X = Painkiller prescribed (strong vs. mild)",
            "Y = 30-day mortality",
            "Z = Underlying illness severity"
        ],
        "annotations": {
            "Case ID": "J2-169",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Confounding by Indication",
            "Difficulty": "Medium",
            "Subdomain": "Healthcare Administration",
            "Causal Structure": "Severity (Z) influences both treatment choice (X) and mortality (Y).",
            "Key Insight": "Sicker patients are more likely to receive stronger treatments, confounding naive comparisons.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does higher mortality among patients receiving the stronger painkiller imply the drug causes deaths?\nHow does confounding by indication distort the comparison?",
        "expected_analysis": "Confounding by indication: severity (Z) drives prescribing (X) and mortality risk (Y).\nThe observed association may reflect baseline severity rather than a harmful drug effect.\nConclusion: The claim \u201cthe stronger drug increases mortality\u201d is INVALID from this observational comparison alone.",
        "Hidden Timestamp": "Was severity assessed before the prescription decision, and is it measured well enough to adjust for it?",
        "Conditional Answers": "Answer if patients of similar severity are compared: The estimate is less confounded.\nAnswer if severity is poorly measured: Residual confounding remains even after adjustment.\nAnswer if prescribing follows a protocol unrelated to individual severity: Comparisons are closer to causal.",
        "Wise Refusal": "I can\u2019t infer a causal drug effect without detailed severity/comorbidity measures and timing of prescription relative to deterioration."
    },
    {
        "id": "T3-BucketJ-170",
        "bucket": "BucketLarge-J",
        "title": "Telemedicine and Missed Follow-ups",
        "scenario": "A health system introduced telemedicine visits in some clinics while others stayed in-person only. A summary report claims telemedicine reduced follow-up adherence because telemedicine clinics show a lower overall rate of patients completing a recommended follow-up within 30 days.\n\nThe evaluation compares 30-day follow-up completion rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline chronic-disease burden of the clinic\u2019s patient panel (high vs. low), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = 30-day follow-up completion rate",
            "Z = baseline chronic-disease burden of the clinic\u2019s patient panel (high vs. low)"
        ],
        "annotations": {
            "Case ID": "J2-170",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will 30-day follow-up completion rate improve?\nWhy does conditioning on baseline chronic-disease burden of the clinic\u2019s patient panel (high vs. low) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was baseline chronic-disease burden of the clinic\u2019s patient panel (high vs. low) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline chronic-disease burden of the clinic\u2019s patient panel (high vs. low) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in baseline chronic-disease burden of the clinic\u2019s patient panel (high vs. low) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-171",
        "bucket": "BucketLarge-J",
        "title": "After-School Tutoring App Results",
        "scenario": "A district piloted an after-school tutoring app in some middle schools. The district claims the app lowered math performance because app schools have lower average end-of-year math scores than non-app schools.\n\nThe evaluation compares average end-of-year math score in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline student achievement level (higher vs. lower prior-year scores), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = average end-of-year math score",
            "Z = baseline student achievement level (higher vs. lower prior-year scores)"
        ],
        "annotations": {
            "Case ID": "J2-171",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will average end-of-year math score improve?\nWhy does conditioning on baseline student achievement level (higher vs. lower prior-year scores) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was baseline student achievement level (higher vs. lower prior-year scores) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline student achievement level (higher vs. lower prior-year scores) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in baseline student achievement level (higher vs. lower prior-year scores) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-172",
        "bucket": "BucketLarge-J",
        "title": "Body Cameras and Citizen Complaints",
        "scenario": "A police department deployed body cameras in some precincts first. City leaders claim body cameras increased misconduct because camera precincts show higher overall citizen complaint rates than non-camera precincts during the evaluation period.\n\nThe evaluation compares citizen complaint rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline complaint environment of the precinct (historically high vs. low complaint rate), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = citizen complaint rate",
            "Z = baseline complaint environment of the precinct (historically high vs. low complaint rate)"
        ],
        "annotations": {
            "Case ID": "J2-172",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will citizen complaint rate improve?\nWhy does conditioning on baseline complaint environment of the precinct (historically high vs. low complaint rate) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was baseline complaint environment of the precinct (historically high vs. low complaint rate) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline complaint environment of the precinct (historically high vs. low complaint rate) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in baseline complaint environment of the precinct (historically high vs. low complaint rate) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-173",
        "bucket": "BucketLarge-J",
        "title": "Transit Pass Subsidy and Commute Times",
        "scenario": "A city offered subsidized monthly transit passes at certain large worksites. A memo claims the subsidy increased commute times because subsidized sites show longer average commutes than non-subsidized sites.\n\nThe evaluation compares average door-to-door commute time in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by home-to-work distance category (short vs. long), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = average door-to-door commute time",
            "Z = home-to-work distance category (short vs. long)"
        ],
        "annotations": {
            "Case ID": "J2-173",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Urban Planning",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will average door-to-door commute time improve?\nWhy does conditioning on home-to-work distance category (short vs. long) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was home-to-work distance category (short vs. long) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each home-to-work distance category (short vs. long) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in home-to-work distance category (short vs. long) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-174",
        "bucket": "BucketLarge-J",
        "title": "Chatbot Support and Customer Satisfaction",
        "scenario": "A company added an automated chatbot to handle customer support for some product lines. Executives claim the chatbot reduced satisfaction because chatbot product lines have lower overall satisfaction scores than product lines without the chatbot.\n\nThe evaluation compares customer satisfaction score in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by issue complexity level (simple vs. complex tickets), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = customer satisfaction score",
            "Z = issue complexity level (simple vs. complex tickets)"
        ],
        "annotations": {
            "Case ID": "J2-174",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Business Operations",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will customer satisfaction score improve?\nWhy does conditioning on issue complexity level (simple vs. complex tickets) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was issue complexity level (simple vs. complex tickets) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each issue complexity level (simple vs. complex tickets) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in issue complexity level (simple vs. complex tickets) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-175",
        "bucket": "BucketLarge-J",
        "title": "Remote Work Policy and Output",
        "scenario": "A firm allowed remote work in certain teams first. Leadership claims remote work reduced productivity because remote-eligible teams show lower overall weekly output than teams that remained on-site.\n\nThe evaluation compares weekly output per employee in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by role type (individual contributor vs. people manager), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = weekly output per employee",
            "Z = role type (individual contributor vs. people manager)"
        ],
        "annotations": {
            "Case ID": "J2-175",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will weekly output per employee improve?\nWhy does conditioning on role type (individual contributor vs. people manager) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was role type (individual contributor vs. people manager) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each role type (individual contributor vs. people manager) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in role type (individual contributor vs. people manager) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-176",
        "bucket": "BucketLarge-J",
        "title": "Carbon Fee Pilot and Emissions",
        "scenario": "A region piloted a carbon fee in some municipalities. A headline article claims the carbon fee increased emissions because fee municipalities show higher overall per-capita emissions than non-fee municipalities after the pilot begins.\n\nThe evaluation compares per-capita CO\u2082 emissions in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by industrial intensity of the municipality (high vs. low share of heavy industry), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = per-capita CO\u2082 emissions",
            "Z = industrial intensity of the municipality (high vs. low share of heavy industry)"
        ],
        "annotations": {
            "Case ID": "J2-176",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Environmental Policy",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will per-capita CO\u2082 emissions improve?\nWhy does conditioning on industrial intensity of the municipality (high vs. low share of heavy industry) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was industrial intensity of the municipality (high vs. low share of heavy industry) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each industrial intensity of the municipality (high vs. low share of heavy industry) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in industrial intensity of the municipality (high vs. low share of heavy industry) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-177",
        "bucket": "BucketLarge-J",
        "title": "Pass/Fail Option and Course Completion",
        "scenario": "A university introduced an optional pass/fail grading policy in some gateway courses. An internal report claims pass/fail reduced completion because pass/fail courses have a lower overall completion rate than comparable graded courses.\n\nThe evaluation compares course completion rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by course difficulty tier (hard vs. moderate), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = course completion rate",
            "Z = course difficulty tier (hard vs. moderate)"
        ],
        "annotations": {
            "Case ID": "J2-177",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Higher Education",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will course completion rate improve?\nWhy does conditioning on course difficulty tier (hard vs. moderate) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was course difficulty tier (hard vs. moderate) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each course difficulty tier (hard vs. moderate) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in course difficulty tier (hard vs. moderate) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-178",
        "bucket": "BucketLarge-J",
        "title": "Microgrants and Small Business Survival",
        "scenario": "A city offered emergency microgrants to small businesses in selected corridors. A press release claims the grants did not help because grant-recipient corridors show lower overall one-year business survival than corridors without grants.\n\nThe evaluation compares one-year business survival rate in units that adopted the intervention versus units that did not. The aggregate comparison suggests the intervention has the opposite effect from what advocates expected.\n\nWhen results are stratified by baseline business fragility (low vs. high pre-grant revenue volatility), the treated group shows the reverse pattern within *each* stratum. The discrepancy arises because the treated and untreated groups have very different mixtures of strata.",
        "variables": [
            "X = Intervention adoption (yes vs. no)",
            "Y = one-year business survival rate",
            "Z = baseline business fragility (low vs. high pre-grant revenue volatility)"
        ],
        "annotations": {
            "Case ID": "J2-178",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Local Economic Development",
            "Causal Structure": "Z \u2192 Y and rollout implies P(Z|X=1) \u2260 P(Z|X=0); aggregating across Z reverses within-stratum contrasts.",
            "Key Insight": "Stratum imbalance can make an intervention look harmful overall even if it helps in every subgroup (or vice versa).",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we expand the intervention, will one-year business survival rate improve?\nWhy does conditioning on baseline business fragility (low vs. high pre-grant revenue volatility) reverse the aggregate conclusion?",
        "expected_analysis": "This is Simpson\u2019s Paradox under intervention (Stratified Intervention Reversal).\nZ is a baseline stratifier that affects Y, and rollout created imbalance in P(Z|X).\nThe aggregate treated-vs-untreated comparison conflates treatment effect with stratum composition.\nUse within-stratum comparisons and/or standardize to a common Z distribution.\nConclusion: The aggregate causal claim is INVALID without stratification/standardization; the correct answer is CONDITIONAL on adjusting for Z.",
        "Hidden Timestamp": "Was baseline business fragility (low vs. high pre-grant revenue volatility) determined before adoption of the intervention (pre-treatment), making it legitimate to stratify on?",
        "Conditional Answers": "Answer if you use only the overall treated vs. untreated difference: You may draw the wrong causal conclusion because treated units are concentrated in harder (or easier) strata.\nAnswer if you compare within each baseline business fragility (low vs. high pre-grant revenue volatility) stratum or standardize: This addresses the imbalance and yields a more credible estimate.\nAnswer if rollout is randomized within strata: Within-stratum comparisons can identify the causal effect more directly.",
        "Wise Refusal": "I can\u2019t estimate the intervention\u2019s effect without knowing the rollout rule and verifying overlap in baseline business fragility (low vs. high pre-grant revenue volatility) between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-179",
        "bucket": "BucketLarge-J",
        "title": "Minimum Wage Policy and Restaurant Closures",
        "scenario": "A city raises the minimum wage and later observes a higher rate of restaurant closures than in neighboring cities that did not raise wages. Commentators claim the wage increase caused closures.\n\nThe city that raised wages was already experiencing rapidly rising commercial rents and a decline in foot traffic due to major construction, both of which affect closure risk and also influenced the political push for wage reform.",
        "variables": [
            "X = minimum wage increase (raised vs. not)",
            "Y = restaurant closure rate",
            "Z = commercial rent pressure and foot traffic trends"
        ],
        "annotations": {
            "Case ID": "J2-179",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Hard",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Z \u2192 X and Z \u2192 Y; without blocking Z, X\u2013Y comparison is confounded.",
            "Key Insight": "Policy adoption can be correlated with underlying economic pressures that also affect outcomes.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement minimum wage increase (raised vs. not), will restaurant closure rate change?\nWhy might units receiving minimum wage increase (raised vs. not) differ from units not receiving it due to commercial rent pressure and foot traffic trends?",
        "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was commercial rent pressure and foot traffic trends measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of minimum wage increase (raised vs. not) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if minimum wage increase (raised vs. not) is targeted to units with different baseline commercial rent pressure and foot traffic trends: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for minimum wage increase (raised vs. not) and credible measurement of commercial rent pressure and foot traffic trends (and other confounders)."
    },
    {
        "id": "T3-BucketJ-180",
        "bucket": "BucketLarge-J",
        "title": "New Reading Curriculum and Test Scores",
        "scenario": "A district adopts a new reading curriculum in schools flagged as \u201cat risk.\u201d After one year, adopting schools have lower reading scores than non-adopting schools, and critics claim the curriculum harmed learning.\n\nAdoption was prioritized for schools with declining prior scores and higher poverty rates\u2014factors that also predict future scores regardless of curriculum.",
        "variables": [
            "X = new reading curriculum adoption (yes vs. no)",
            "Y = reading test scores",
            "Z = baseline performance trend and student poverty rate"
        ],
        "annotations": {
            "Case ID": "J2-180",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z influences both adoption X and outcomes Y; treated schools start on different trajectories.",
            "Key Insight": "Targeted interventions create treated groups that differ systematically from controls.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement new reading curriculum adoption (yes vs. no), will reading test scores change?\nWhy might units receiving new reading curriculum adoption (yes vs. no) differ from units not receiving it due to baseline performance trend and student poverty rate?",
        "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was baseline performance trend and student poverty rate measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of new reading curriculum adoption (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if new reading curriculum adoption (yes vs. no) is targeted to units with different baseline baseline performance trend and student poverty rate: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for new reading curriculum adoption (yes vs. no) and credible measurement of baseline performance trend and student poverty rate (and other confounders)."
    },
    {
        "id": "T3-BucketJ-181",
        "bucket": "BucketLarge-J",
        "title": "Community Policing Grants and Crime",
        "scenario": "A federal grant funds community policing in selected neighborhoods. A year later, funded neighborhoods show higher reported crime than unfunded neighborhoods, leading to claims that the grants increased crime.\n\nGrant selection prioritized neighborhoods with historically high crime and recent upward trends, which also predict future crime.",
        "variables": [
            "X = community policing grant funding (yes vs. no)",
            "Y = reported crime rate",
            "Z = baseline crime level and pre-grant trend"
        ],
        "annotations": {
            "Case ID": "J2-181",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Z \u2192 X and Z \u2192 Y; selecting on need confounds causal interpretation.",
            "Key Insight": "Comparing funded vs. unfunded sites confounds intervention with baseline risk.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement community policing grant funding (yes vs. no), will reported crime rate change?\nWhy might units receiving community policing grant funding (yes vs. no) differ from units not receiving it due to baseline crime level and pre-grant trend?",
        "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was baseline crime level and pre-grant trend measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of community policing grant funding (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if community policing grant funding (yes vs. no) is targeted to units with different baseline baseline crime level and pre-grant trend: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for community policing grant funding (yes vs. no) and credible measurement of baseline crime level and pre-grant trend (and other confounders)."
    },
    {
        "id": "T3-BucketJ-182",
        "bucket": "BucketLarge-J",
        "title": "Nutrition Labels and Snack Purchases",
        "scenario": "A grocery chain introduces front-of-package nutrition labels in some stores. Purchases of sugary snacks are higher in labeled stores, and an executive claims labels backfired.\n\nThe chain piloted labels first in dense urban stores with distinct customer baskets and higher baseline snack purchases.",
        "variables": [
            "X = nutrition label rollout (store labeled vs. not)",
            "Y = purchases of sugary snacks",
            "Z = store neighborhood type and baseline basket patterns"
        ],
        "annotations": {
            "Case ID": "J2-182",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Medium",
            "Subdomain": "Behavioral Economics",
            "Causal Structure": "Z affects store selection and Y; naive comparison attributes Z effects to labeling.",
            "Key Insight": "Rollout choices can confound estimated behavioral impacts.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement nutrition label rollout (store labeled vs. not), will purchases of sugary snacks change?\nWhy might units receiving nutrition label rollout (store labeled vs. not) differ from units not receiving it due to store neighborhood type and baseline basket patterns?",
        "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was store neighborhood type and baseline basket patterns measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of nutrition label rollout (store labeled vs. not) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if nutrition label rollout (store labeled vs. not) is targeted to units with different baseline store neighborhood type and baseline basket patterns: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for nutrition label rollout (store labeled vs. not) and credible measurement of store neighborhood type and baseline basket patterns (and other confounders)."
    },
    {
        "id": "T3-BucketJ-183",
        "bucket": "BucketLarge-J",
        "title": "Hybrid Work Stipend and Turnover",
        "scenario": "A company offers a hybrid-work stipend to certain teams and later observes higher turnover in stipend teams. Management claims hybrid work drives attrition.\n\nThe stipend was offered first to teams undergoing reorganization and leadership turnover, which also increases attrition risk.",
        "variables": [
            "X = hybrid-work stipend offered (yes vs. no)",
            "Y = employee turnover rate",
            "Z = team reorganization/leadership instability"
        ],
        "annotations": {
            "Case ID": "J2-183",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Z \u2192 X and Z \u2192 Y; treated teams face other shocks that raise turnover.",
            "Key Insight": "Interventions often coincide with organizational changes that also affect outcomes.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement hybrid-work stipend offered (yes vs. no), will employee turnover rate change?\nWhy might units receiving hybrid-work stipend offered (yes vs. no) differ from units not receiving it due to team reorganization/leadership instability?",
        "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was team reorganization/leadership instability measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of hybrid-work stipend offered (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if hybrid-work stipend offered (yes vs. no) is targeted to units with different baseline team reorganization/leadership instability: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for hybrid-work stipend offered (yes vs. no) and credible measurement of team reorganization/leadership instability (and other confounders)."
    },
    {
        "id": "T3-BucketJ-184",
        "bucket": "BucketLarge-J",
        "title": "Bike Lane Expansion and Retail Sales",
        "scenario": "A city expands protected bike lanes on selected commercial corridors and later sees lower retail sales on those corridors than on others. Critics claim bike lanes hurt businesses.\n\nBike lanes were prioritized for corridors already facing construction disruption and declining sales trends, which also predict future sales.",
        "variables": [
            "X = protected bike lane expansion (yes vs. no)",
            "Y = retail sales",
            "Z = baseline construction disruption and pre-policy sales trend"
        ],
        "annotations": {
            "Case ID": "J2-184",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Hard",
            "Subdomain": "Urban Policy",
            "Causal Structure": "Z influences both which corridors get X and future Y.",
            "Key Insight": "Targeting infrastructure projects to struggling corridors confounds causal claims about sales impacts.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement protected bike lane expansion (yes vs. no), will retail sales change?\nWhy might units receiving protected bike lane expansion (yes vs. no) differ from units not receiving it due to baseline construction disruption and pre-policy sales trend?",
        "expected_analysis": "This is L2 Confounding (Unblocked Backdoor).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was baseline construction disruption and pre-policy sales trend measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of protected bike lane expansion (yes vs. no) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if protected bike lane expansion (yes vs. no) is targeted to units with different baseline baseline construction disruption and pre-policy sales trend: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for protected bike lane expansion (yes vs. no) and credible measurement of baseline construction disruption and pre-policy sales trend (and other confounders)."
    },
    {
        "id": "T3-BucketJ-185",
        "bucket": "BucketLarge-J",
        "title": "Adaptive Tutoring Hours and Learning Gains",
        "scenario": "A district uses an adaptive policy: students who score poorly on weekly quizzes are assigned additional tutoring hours the following week. Analysts find that students with more tutoring hours have smaller end-of-term learning gains and argue tutoring is ineffective.\n\nWeekly quiz scores (past outcomes) influence future tutoring assignment and also predict end-of-term gains.",
        "variables": [
            "X = assigned tutoring hours (time-varying)",
            "Y = end-of-term learning gains",
            "Z = weekly quiz score history (past performance)"
        ],
        "annotations": {
            "Case ID": "J2-185",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Time-varying Confounding",
            "Difficulty": "Hard",
            "Subdomain": "Education Policy",
            "Causal Structure": "Past Y \u2192 future X and past Y \u2192 future Y; time-varying assignment creates confounding.",
            "Key Insight": "When treatment responds to outcomes, naive correlations can reverse the true effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement assigned tutoring hours (time-varying), will end-of-term learning gains change?\nWhy might units receiving assigned tutoring hours (time-varying) differ from units not receiving it due to weekly quiz score history (past performance)?",
        "expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was weekly quiz score history (past performance) measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of assigned tutoring hours (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if assigned tutoring hours (time-varying) is targeted to units with different baseline weekly quiz score history (past performance): The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for assigned tutoring hours (time-varying) and credible measurement of weekly quiz score history (past performance) (and other confounders)."
    },
    {
        "id": "T3-BucketJ-186",
        "bucket": "BucketLarge-J",
        "title": "Outreach Calls and Vaccination Uptake",
        "scenario": "A clinic increases outreach call frequency for patients who have not scheduled vaccinations. Later, patients who received more calls have lower vaccination rates, and a manager claims calls deter people.\n\nCall frequency rises when patients remain unvaccinated over time (past outcome), and those patients are harder to reach and less likely to vaccinate.",
        "variables": [
            "X = outreach call intensity (time-varying)",
            "Y = vaccination uptake",
            "Z = prior vaccination status / past non-response"
        ],
        "annotations": {
            "Case ID": "J2-186",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Time-varying Confounding",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "Past Y drives future X; past Y also predicts future Y.",
            "Key Insight": "Reactive intensification of an intervention creates time-varying confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement outreach call intensity (time-varying), will vaccination uptake change?\nWhy might units receiving outreach call intensity (time-varying) differ from units not receiving it due to prior vaccination status / past non-response?",
        "expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was prior vaccination status / past non-response measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of outreach call intensity (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if outreach call intensity (time-varying) is targeted to units with different baseline prior vaccination status / past non-response: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for outreach call intensity (time-varying) and credible measurement of prior vaccination status / past non-response (and other confounders)."
    },
    {
        "id": "T3-BucketJ-187",
        "bucket": "BucketLarge-J",
        "title": "Surge Warnings and Ride Cancellations",
        "scenario": "A ride-share platform displays \u201chigh-demand pricing\u201d warnings more often on routes where cancellations are rising. Analysts observe that rides with warnings have higher cancellation rates and claim warnings cause cancellations.\n\nWarnings are triggered in response to congestion and earlier cancellation surges, which also predict future cancellations.",
        "variables": [
            "X = displaying a surge-pricing warning (time-varying)",
            "Y = ride cancellation rate",
            "Z = recent congestion and cancellation history"
        ],
        "annotations": {
            "Case ID": "J2-187",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Time-varying Confounding",
            "Difficulty": "Hard",
            "Subdomain": "Platform Policy",
            "Causal Structure": "Past demand conditions influence X and Y; time-varying confounding biases naive estimates.",
            "Key Insight": "When interventions respond to worsening conditions, effects can be misattributed.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we implement displaying a surge-pricing warning (time-varying), will ride cancellation rate change?\nWhy might units receiving displaying a surge-pricing warning (time-varying) differ from units not receiving it due to recent congestion and cancellation history?",
        "expected_analysis": "This is L2 Confounding (Time-varying Confounding).\nZ influences both intervention assignment (X) and the outcome (Y), leaving an unblocked backdoor path in the naive treated-vs-untreated comparison.\nA causal estimate requires blocking the backdoor: randomization, strong adjustment for Z and related covariates, and checking overlap/positivity.\nConclusion: The treated-vs-untreated comparison is INVALID as a causal claim unless Z is appropriately addressed.",
        "Hidden Timestamp": "Was recent congestion and cancellation history measured before the intervention decision, and did it influence both adoption and outcomes?",
        "Conditional Answers": "Answer if assignment of displaying a surge-pricing warning (time-varying) is randomized (or as-if random): Differences in Y can be interpreted causally.\nAnswer if displaying a surge-pricing warning (time-varying) is targeted to units with different baseline recent congestion and cancellation history: The naive comparison is confounded; adjust or use quasi-experimental methods.\nAnswer if Z changes over time and affects future assignment: Use time-varying causal methods rather than a single regression.",
        "Wise Refusal": "I can\u2019t infer the causal effect without a clear assignment rule for displaying a surge-pricing warning (time-varying) and credible measurement of recent congestion and cancellation history (and other confounders)."
    },
    {
        "id": "T3-BucketJ-188",
        "bucket": "BucketLarge-J",
        "title": "Wellness Program and Blood Pressure",
        "scenario": "A company offers a voluntary wellness program with weekly coaching. The HR report compares blood pressure changes only among employees who attended at least 8 of 10 sessions and concludes the program substantially lowers blood pressure.\n\nEmployees who miss sessions are excluded from the analysis, and attendance is affected by workload, baseline health, and motivation.",
        "variables": [
            "X = wellness program enrollment",
            "Y = blood pressure change",
            "S = high attendance (8+ sessions) (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-188",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Workplace Health",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does wellness program enrollment cause a change in blood pressure change?\nHow can restricting the analysis to high attendance (8+ sessions) bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into high attendance (8+ sessions) occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into high attendance (8+ sessions) is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by high attendance (8+ sessions) and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-189",
        "bucket": "BucketLarge-J",
        "title": "Scholarship Mentoring and Graduation",
        "scenario": "A university pairs scholarship recipients with mentors. The evaluation reports that mentored students graduate at higher rates, but it includes only students who met with their mentor at least once per month.\n\nStudents who miss meetings\u2014often due to jobs or family obligations\u2014are excluded from the graduation analysis.",
        "variables": [
            "X = mentorship program participation",
            "Y = graduation rate",
            "S = regular mentor-meeting compliance (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-189",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Higher Education",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does mentorship program participation cause a change in graduation rate?\nHow can restricting the analysis to regular mentor-meeting compliance bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into regular mentor-meeting compliance occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into regular mentor-meeting compliance is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by regular mentor-meeting compliance and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-190",
        "bucket": "BucketLarge-J",
        "title": "Job Placement Workshops and Employment",
        "scenario": "A workforce agency offers job placement workshops. The agency reports strong employment gains by comparing employment rates only among participants who completed the full workshop series.\n\nParticipants who drop out early are excluded, even though their employment outcomes may differ.",
        "variables": [
            "X = workshop enrollment",
            "Y = employment status after 3 months",
            "S = workshop completion (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-190",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Labor Policy",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does workshop enrollment cause a change in employment status after 3 months?\nHow can restricting the analysis to workshop completion bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into workshop completion occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into workshop completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by workshop completion and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-191",
        "bucket": "BucketLarge-J",
        "title": "Nutrition Benefit Cards and Food Security",
        "scenario": "A city issues nutrition benefit cards. The city reports improved food security by surveying only households that used the card at least once per week.\n\nHouseholds that rarely used the card (due to access barriers or stigma) are excluded from the reported outcomes.",
        "variables": [
            "X = receiving a nutrition benefit card",
            "Y = food security score",
            "S = frequent card usage (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-191",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does receiving a nutrition benefit card cause a change in food security score?\nHow can restricting the analysis to frequent card usage bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into frequent card usage occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into frequent card usage is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by frequent card usage and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-192",
        "bucket": "BucketLarge-J",
        "title": "New Onboarding Modules and Retention",
        "scenario": "A company introduces new onboarding modules. Managers claim the new onboarding reduces early attrition because employees who completed all modules had high 90-day retention.\n\nEmployees who did not complete modules are excluded from the retention calculation.",
        "variables": [
            "X = new onboarding process",
            "Y = 90-day retention",
            "S = completion of onboarding modules (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-192",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does new onboarding process cause a change in 90-day retention?\nHow can restricting the analysis to completion of onboarding modules bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into completion of onboarding modules occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into completion of onboarding modules is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by completion of onboarding modules and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-193",
        "bucket": "BucketLarge-J",
        "title": "Community Garden Events and Cohesion",
        "scenario": "A city funds community gardens. A report claims gardens increase neighborhood cohesion because survey results are positive among residents who attended at least one garden event.\n\nResidents who never attended events are excluded from the survey analysis.",
        "variables": [
            "X = community garden program",
            "Y = neighborhood cohesion index",
            "S = event attendance (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-193",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Urban Policy",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does community garden program cause a change in neighborhood cohesion index?\nHow can restricting the analysis to event attendance bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into event attendance occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into event attendance is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by event attendance and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-194",
        "bucket": "BucketLarge-J",
        "title": "Financial Literacy App and Savings",
        "scenario": "A bank rolls out a financial literacy app. The bank claims the app increases savings because users who completed all lessons increased their savings balances.\n\nThe analysis excludes users who installed the app but did not finish lessons.",
        "variables": [
            "X = financial literacy app rollout",
            "Y = savings balance change",
            "S = lesson completion (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-194",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Behavioral Economics",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does financial literacy app rollout cause a change in savings balance change?\nHow can restricting the analysis to lesson completion bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into lesson completion occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into lesson completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by lesson completion and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-195",
        "bucket": "BucketLarge-J",
        "title": "Bike Helmet Giveaway and Injuries",
        "scenario": "A city gives away free bike helmets at transit hubs. The city reports lower cyclist injuries among those who registered their helmet pickup online, concluding the giveaway reduces injuries.\n\nCyclists who took helmets but did not register are excluded from injury tracking.",
        "variables": [
            "X = helmet giveaway program",
            "Y = cyclist injury rate",
            "S = online registration of helmet pickup (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-195",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Transportation Safety",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does helmet giveaway program cause a change in cyclist injury rate?\nHow can restricting the analysis to online registration of helmet pickup bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into online registration of helmet pickup occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into online registration of helmet pickup is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by online registration of helmet pickup and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-196",
        "bucket": "BucketLarge-J",
        "title": "Teacher Training and Observation Scores",
        "scenario": "A district offers optional teacher training. The district reports improved classroom observation scores among teachers who completed the training and submitted all follow-up reflections.\n\nTeachers who attended but did not submit reflections are excluded from the reported outcomes.",
        "variables": [
            "X = teacher training program",
            "Y = classroom observation score",
            "S = submission of required follow-up reflections (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-196",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does teacher training program cause a change in classroom observation score?\nHow can restricting the analysis to submission of required follow-up reflections bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into submission of required follow-up reflections occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into submission of required follow-up reflections is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by submission of required follow-up reflections and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-197",
        "bucket": "BucketLarge-J",
        "title": "Rent Assistance Documentation and Evictions",
        "scenario": "A county provides rent assistance. The county reports the program prevents evictions by analyzing only households that submitted all required documentation by the deadline.\n\nHouseholds missing paperwork are excluded from eviction outcome statistics.",
        "variables": [
            "X = rent assistance program",
            "Y = eviction rate",
            "S = documentation completion (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-197",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Housing Policy",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does rent assistance program cause a change in eviction rate?\nHow can restricting the analysis to documentation completion bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into documentation completion occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into documentation completion is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by documentation completion and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-198",
        "bucket": "BucketLarge-J",
        "title": "Fraud Hotline and Case Duration",
        "scenario": "A firm launches an internal fraud-reporting hotline. Leadership claims the hotline speeds up resolutions because hotline cases closed quickly.\n\nThe report includes only cases resolved within the quarter; unresolved cases are excluded.",
        "variables": [
            "X = fraud-reporting hotline introduction",
            "Y = case resolution time",
            "S = cases resolved within the quarter (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-198",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Compliance",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does fraud-reporting hotline introduction cause a change in case resolution time?\nHow can restricting the analysis to cases resolved within the quarter bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into cases resolved within the quarter occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into cases resolved within the quarter is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by cases resolved within the quarter and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-199",
        "bucket": "BucketLarge-J",
        "title": "After-School Sports and Attendance",
        "scenario": "A school offers an after-school sports program to improve attendance. The school reports improved attendance among students who participated in at least 75% of practices.\n\nStudents who enrolled but rarely attended practices are excluded from the attendance comparison.",
        "variables": [
            "X = sports program enrollment",
            "Y = school attendance rate",
            "S = high practice participation (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-199",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does sports program enrollment cause a change in school attendance rate?\nHow can restricting the analysis to high practice participation bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into high practice participation occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into high practice participation is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by high practice participation and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-200",
        "bucket": "BucketLarge-J",
        "title": "Medication Reminder Texts and Refills",
        "scenario": "A clinic sends medication reminder texts. The clinic reports higher refill rates among patients who clicked the confirmation link in the texts.\n\nPatients who received texts but never clicked are excluded from the refill calculation.",
        "variables": [
            "X = text reminder program",
            "Y = medication refill rate",
            "S = clicking the confirmation link (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-200",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Public Health",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does text reminder program cause a change in medication refill rate?\nHow can restricting the analysis to clicking the confirmation link bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into clicking the confirmation link occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into clicking the confirmation link is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by clicking the confirmation link and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-201",
        "bucket": "BucketLarge-J",
        "title": "Bootcamp Scholarships and Salary Outcomes",
        "scenario": "A nonprofit funds coding bootcamp scholarships. The nonprofit reports large salary gains using only scholarship recipients who completed the bootcamp and self-reported job outcomes.\n\nRecipients who did not report outcomes are excluded from salary statistics.",
        "variables": [
            "X = bootcamp scholarship funding",
            "Y = post-bootcamp salary",
            "S = completion plus outcome reporting (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-201",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Labor Economics",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does bootcamp scholarship funding cause a change in post-bootcamp salary?\nHow can restricting the analysis to completion plus outcome reporting bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into completion plus outcome reporting occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into completion plus outcome reporting is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by completion plus outcome reporting and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-202",
        "bucket": "BucketLarge-J",
        "title": "Neighborhood Watch App and Reported Crime",
        "scenario": "A city launches a neighborhood watch app. The city claims the app reduces crime because high-adoption neighborhoods show fewer incidents.\n\nThe evaluation excludes neighborhoods where adoption was low and relies heavily on app-based reporting.",
        "variables": [
            "X = neighborhood watch app rollout",
            "Y = reported crime incidents",
            "S = active app usage / high adoption (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-202",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does neighborhood watch app rollout cause a change in reported crime incidents?\nHow can restricting the analysis to active app usage / high adoption bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into active app usage / high adoption occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into active app usage / high adoption is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by active app usage / high adoption and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-203",
        "bucket": "BucketLarge-J",
        "title": "Transit Reliability Alerts and Satisfaction",
        "scenario": "A transit agency launches a reliability initiative. The agency reports higher satisfaction by surveying riders who signed up for service-alert notifications.\n\nRiders who did not sign up are excluded from the satisfaction survey sample.",
        "variables": [
            "X = reliability initiative",
            "Y = rider satisfaction",
            "S = subscription to service alerts (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-203",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Transportation Policy",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does reliability initiative cause a change in rider satisfaction?\nHow can restricting the analysis to subscription to service alerts bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into subscription to service alerts occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into subscription to service alerts is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by subscription to service alerts and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-204",
        "bucket": "BucketLarge-J",
        "title": "Microloan Coaching and Repayment",
        "scenario": "A microfinance organization offers microloans with optional coaching. The organization claims coaching improves repayment because borrowers who attended at least three sessions repaid at higher rates.\n\nBorrowers offered coaching but attending fewer sessions are excluded from the coached-group analysis.",
        "variables": [
            "X = coaching add-on to microloans",
            "Y = loan repayment rate",
            "S = attending 3+ coaching sessions (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-204",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Development Economics",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does coaching add-on to microloans cause a change in loan repayment rate?\nHow can restricting the analysis to attending 3+ coaching sessions bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into attending 3+ coaching sessions occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into attending 3+ coaching sessions is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by attending 3+ coaching sessions and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-205",
        "bucket": "BucketLarge-J",
        "title": "Multi-day Orientation and First-Year GPA",
        "scenario": "A college redesigns orientation into a multi-day program. Administrators claim the new orientation improves academic performance because students who attended all days had higher first-year GPAs.\n\nStudents who missed days due to work or travel constraints are excluded from the \u201cfull-attendance\u201d group.",
        "variables": [
            "X = new multi-day orientation",
            "Y = first-year GPA",
            "S = full orientation attendance (included vs. excluded)"
        ],
        "annotations": {
            "Case ID": "J2-205",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Higher Education",
            "Causal Structure": "X \u2192 S and (other factors) \u2192 S; S is related to Y; conditioning on S biases the X\u2192Y estimate.",
            "Key Insight": "Post-treatment inclusion rules can change who is counted and distort causal conclusions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does new multi-day orientation cause a change in first-year GPA?\nHow can restricting the analysis to full orientation attendance bias the estimated effect?",
        "expected_analysis": "This is L2 Selection Bias (Post-intervention Selection).\nThe analysis conditions on a post-intervention selection variable S (who is included), which is influenced by the intervention and is related to outcomes.\nBecause the comparison is made only within the selected subset, treated and untreated groups are no longer comparable, and the estimated effect can be biased.\nConclusion: The causal claim is INVALID unless you analyze the full intended population (intent-to-treat) or properly account for selection/missing outcomes.",
        "Hidden Timestamp": "Does selection into full orientation attendance occur after the intervention could influence it, and is selection related to outcomes?",
        "Conditional Answers": "Answer if selection into full orientation attendance is unrelated to outcomes (rare): Conditioning may not bias the estimate.\nAnswer if the intervention changes who is selected (compliance/attendance/retention): The selected-sample estimate is biased; prefer intent-to-treat and measure outcomes for everyone.\nAnswer if administrative outcomes exist for excluded individuals: Use them to reduce selection bias, noting remaining assumptions.",
        "Wise Refusal": "I can\u2019t estimate the intervention effect without outcomes for those excluded by full orientation attendance and a clear picture of why selection differs between treated and untreated units."
    },
    {
        "id": "T3-BucketJ-206",
        "bucket": "BucketLarge-J",
        "title": "Medication Co-pay Waiver Among Adherent Patients",
        "scenario": "A clinic waives medication co-pays for a subset of patients. Analysts compare blood-pressure outcomes only among patients who took at least 90% of doses (adherent patients) and find that those with the co-pay waiver have worse blood pressure control. They conclude the waiver harms outcomes.\n\nAdherence is influenced by both the waiver (making adherence easier) and patients\u2019 underlying health-management capacity and stress, which also affect blood pressure.",
        "variables": [
            "X = co-pay waiver (yes vs. no)",
            "Y = blood pressure control",
            "C = adherent (90%+ doses) (conditioned-on)",
            "U = health-management capacity/stress (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-206",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are adherent (90%+ doses), does X appear to change Y?\nWhy can conditioning on adherent (90%+ doses) create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (adherent (90%+ doses)) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become adherent (90%+ doses) and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-207",
        "bucket": "BucketLarge-J",
        "title": "Parole Support Program Among Those Who Attend Meetings",
        "scenario": "A county offers a parole support program. A report compares recidivism only among parolees who attended all required meetings and finds higher recidivism in the program group, concluding the program is ineffective.\n\nMeeting attendance depends on program assignment (some meetings are mandatory under the program) and on unobserved stability factors (transportation, housing), which also affect recidivism.",
        "variables": [
            "X = parole support program assignment",
            "Y = recidivism",
            "C = fully compliant with meetings (conditioned-on)",
            "U = baseline stability/transportation access (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-207",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are fully compliant with meetings, does X appear to change Y?\nWhy can conditioning on fully compliant with meetings create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (fully compliant with meetings) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become fully compliant with meetings and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-208",
        "bucket": "BucketLarge-J",
        "title": "Scholarship Counseling Among Students Who Show Up",
        "scenario": "A university offers academic counseling to scholarship students. Evaluators compare GPA only among students who attended at least five counseling sessions and find that counseled students have lower GPAs, concluding counseling hurts performance.\n\nSession attendance is affected by counseling availability and by unobserved academic difficulty and motivation, which also influence GPA.",
        "variables": [
            "X = academic counseling offer",
            "Y = semester GPA",
            "C = attended \u22655 sessions (conditioned-on)",
            "U = unobserved academic difficulty/motivation (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-208",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Higher Education",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are attended \u22655 sessions, does X appear to change Y?\nWhy can conditioning on attended \u22655 sessions create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (attended \u22655 sessions) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become attended \u22655 sessions and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-209",
        "bucket": "BucketLarge-J",
        "title": "Job Search Platform and Active Users",
        "scenario": "A job-search platform introduces a new \u201csmart recommendations\u201d feature. Analysts compare job-offer rates only among users who were active weekly and find that users with the feature have lower offer rates, concluding the feature is harmful.\n\nWeekly activity is affected by feature exposure and by unobserved job-seeker urgency and constraints, which also affect job offers.",
        "variables": [
            "X = smart recommendations feature exposure",
            "Y = job-offer rate",
            "C = weekly active user (conditioned-on)",
            "U = job-seeker urgency/constraints (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-209",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Labor Economics",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are weekly active user, does X appear to change Y?\nWhy can conditioning on weekly active user create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (weekly active user) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become weekly active user and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-210",
        "bucket": "BucketLarge-J",
        "title": "Home Energy Reports Among Readers",
        "scenario": "A utility sends home energy reports to some households. The evaluation compares electricity use only among households that opened the emailed report and finds that treated households used more energy, suggesting reports backfire.\n\nEmail opening is influenced by being sent the report and by unobserved engagement levels and household routines that also affect electricity use.",
        "variables": [
            "X = receiving home energy report emails",
            "Y = electricity consumption",
            "C = opened/read the report (conditioned-on)",
            "U = household engagement/routines (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-210",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Environmental Policy",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are opened/read the report, does X appear to change Y?\nWhy can conditioning on opened/read the report create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (opened/read the report) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become opened/read the report and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-211",
        "bucket": "BucketLarge-J",
        "title": "Teacher Coaching Among Participants Who Complete Surveys",
        "scenario": "A district offers teacher coaching. Analysts compare classroom observation scores only among teachers who completed all post-coaching surveys and find coaching teachers score worse, concluding coaching reduces performance.\n\nSurvey completion is influenced by coaching participation and by unobserved conscientiousness and workload, which also influence observation outcomes.",
        "variables": [
            "X = teacher coaching participation",
            "Y = classroom observation score",
            "C = completed all follow-up surveys (conditioned-on)",
            "U = conscientiousness/workload (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-211",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Education Policy",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are completed all follow-up surveys, does X appear to change Y?\nWhy can conditioning on completed all follow-up surveys create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (completed all follow-up surveys) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become completed all follow-up surveys and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-212",
        "bucket": "BucketLarge-J",
        "title": "Mental Health App Among Daily Users",
        "scenario": "A campus provides a mental health app to students. A study compares stress scores only among students who used the app daily and finds higher stress among app users, concluding the app increases stress.\n\nDaily use is influenced by app access and by unobserved baseline stress and help-seeking behavior, which also predict later stress scores.",
        "variables": [
            "X = app access/encouragement",
            "Y = reported stress score",
            "C = daily app user (conditioned-on)",
            "U = baseline stress/help-seeking propensity (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-212",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Medium",
            "Subdomain": "Psychology",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are daily app user, does X appear to change Y?\nWhy can conditioning on daily app user create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (daily app user) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become daily app user and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-213",
        "bucket": "BucketLarge-J",
        "title": "Transit Fare Capping Among Frequent Riders",
        "scenario": "A transit agency implements fare capping. Analysts compare satisfaction only among riders who took at least 20 trips per month and find lower satisfaction among capped-fare riders, concluding fare capping reduces satisfaction.\n\nHigh trip frequency is influenced by fare capping (making frequent riding cheaper) and by unobserved commuter dependence and route constraints, which also affect satisfaction.",
        "variables": [
            "X = fare-capping policy exposure",
            "Y = rider satisfaction",
            "C = frequent rider (\u226520 trips/month) (conditioned-on)",
            "U = commuter dependence/route constraints (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-213",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Transportation Policy",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are frequent rider (\u226520 trips/month), does X appear to change Y?\nWhy can conditioning on frequent rider (\u226520 trips/month) create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (frequent rider (\u226520 trips/month)) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become frequent rider (\u226520 trips/month) and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-214",
        "bucket": "BucketLarge-J",
        "title": "Community Mediation Program Among Participants Who Complete Mediation",
        "scenario": "A city offers community mediation for neighbor disputes. The evaluation compares conflict recurrence only among disputes that completed mediation sessions and finds higher recurrence when mediation was offered, concluding mediation worsens conflicts.\n\nCompletion depends on mediation offer (providing a path to completion) and on unobserved conflict intensity and willingness to compromise, which also affects recurrence.",
        "variables": [
            "X = mediation offer",
            "Y = conflict recurrence rate",
            "C = completed mediation (conditioned-on)",
            "U = conflict intensity/willingness to compromise (unobserved)"
        ],
        "annotations": {
            "Case ID": "J2-214",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Urban Policy",
            "Causal Structure": "X \u2192 C \u2190 U \u2192 Y; conditioning on C opens X \u2194 U, biasing X\u2013Y within C=1.",
            "Key Insight": "Conditioning on an intermediate participation/compliance variable can create spurious treatment\u2013outcome relationships.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Among those who are completed mediation, does X appear to change Y?\nWhy can conditioning on completed mediation create a spurious association between X and Y?",
        "expected_analysis": "This is L2 Collider bias (Conditioning on Compliance).\nCompliance/participation C is influenced by both the intervention X and other factors U that also affect outcomes Y.\nBy conditioning on C (an outcome of multiple causes), the analysis opens a non-causal path between X and U, which then biases the relationship between X and Y within the compliant subset.\nConclusion: The within-compliers comparison is INVALID as a causal claim; analyze the full assigned population or use appropriate causal methods (e.g., IV with strong assumptions).",
        "Hidden Timestamp": "Did compliance/participation (completed mediation) occur after assignment of X, making it a post-treatment variable affected by both X and U?",
        "Conditional Answers": "Answer if compliance is unaffected by unobserved U (rare): Conditioning on C would be less problematic.\nAnswer if U affects both compliance and outcomes: Conditioning on C induces collider bias; report intent-to-treat or avoid conditioning on C.\nAnswer if you can instrument for compliance (with valid assumptions): You may estimate a causal effect for compliers, but this is assumption-heavy.",
        "Wise Refusal": "I can\u2019t make a causal claim from the compliant-only analysis without modeling why people become completed mediation and whether those determinants also affect Y."
    },
    {
        "id": "T3-BucketJ-215",
        "bucket": "BucketLarge-J",
        "title": "Plagiarism Detector Flags in a Large Class",
        "scenario": "A professor uses an automated plagiarism detector that flags 2% of submissions. The professor says, \u201cIf the system flags you, you basically plagiarized,\u201d and proposes an automatic penalty policy.\n\nIn reality, confirmed plagiarism is rare in this class, and the detector can produce false positives, especially on common template phrases.",
        "variables": [
            "Signal = detector flag (positive)",
            "Event = underlying condition/event of interest",
            "\u03c0 = true plagiarism prevalence in the class"
        ],
        "annotations": {
            "Case ID": "J2-215",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (detector flag (positive)), what is the probability the underlying event is actually true?\nHow does the base rate (true plagiarism prevalence in the class) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim \u201ca positive signal almost surely means the event is true\u201d is INVALID without incorporating the base rate and error rates.",
        "Hidden Timestamp": "Were the base rate (true plagiarism prevalence in the class) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-216",
        "bucket": "BucketLarge-J",
        "title": "Terror Watchlist Alerts at an Airport",
        "scenario": "An airport runs passengers through a watchlist system and gets a small number of alerts. A security officer claims, \u201cAn alert means the passenger is almost certainly dangerous.\u201d\n\nThe true prevalence of dangerous individuals among passengers is extremely low, and the system can generate false positives due to name similarity.",
        "variables": [
            "Signal = watchlist alert (positive)",
            "Event = underlying condition/event of interest",
            "\u03c0 = prevalence of dangerous individuals among passengers"
        ],
        "annotations": {
            "Case ID": "J2-216",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Hard",
            "Subdomain": "Public Safety",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (watchlist alert (positive)), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of dangerous individuals among passengers) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim \u201ca positive signal almost surely means the event is true\u201d is INVALID without incorporating the base rate and error rates.",
        "Hidden Timestamp": "Were the base rate (prevalence of dangerous individuals among passengers) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-217",
        "bucket": "BucketLarge-J",
        "title": "Fraud Alerts in Online Payments",
        "scenario": "A payment platform\u2019s fraud model flags transactions as \u201chigh risk.\u201d A manager claims, \u201cHigh-risk flagged transactions are almost always fraud,\u201d and wants to auto-decline all flagged purchases.\n\nFraud is uncommon relative to total transactions, and the model can misclassify unusual but legitimate purchases.",
        "variables": [
            "Signal = fraud-model high-risk flag",
            "Event = underlying condition/event of interest",
            "\u03c0 = prevalence of fraud among all transactions"
        ],
        "annotations": {
            "Case ID": "J2-217",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Finance",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (fraud-model high-risk flag), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of fraud among all transactions) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim \u201ca positive signal almost surely means the event is true\u201d is INVALID without incorporating the base rate and error rates.",
        "Hidden Timestamp": "Were the base rate (prevalence of fraud among all transactions) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-218",
        "bucket": "BucketLarge-J",
        "title": "Rare Disease Screening in a Workplace",
        "scenario": "A company offers screening for a rare disease. An employee tests positive and the HR office says, \u201cA positive test means you probably have the disease,\u201d and recommends immediate treatment.\n\nThe disease is very rare in the workforce, and the test has non-zero false positives.",
        "variables": [
            "Signal = positive screening test",
            "Event = underlying condition/event of interest",
            "\u03c0 = prevalence of the disease in the screened population"
        ],
        "annotations": {
            "Case ID": "J2-218",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (positive screening test), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of the disease in the screened population) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim \u201ca positive signal almost surely means the event is true\u201d is INVALID without incorporating the base rate and error rates.",
        "Hidden Timestamp": "Were the base rate (prevalence of the disease in the screened population) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-219",
        "bucket": "BucketLarge-J",
        "title": "Defect Detection on a Production Line",
        "scenario": "A factory installs a camera system that flags items as defective. A supervisor states, \u201cIf the camera flags an item, it\u2019s defective,\u201d and increases scrap rates.\n\nTrue defects are uncommon on this stabilized line, and the camera sometimes flags harmless cosmetic variations.",
        "variables": [
            "Signal = camera defect flag",
            "Event = underlying condition/event of interest",
            "\u03c0 = baseline defect prevalence on the line"
        ],
        "annotations": {
            "Case ID": "J2-219",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Operations",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (camera defect flag), what is the probability the underlying event is actually true?\nHow does the base rate (baseline defect prevalence on the line) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Prior Ignorance).\nThe argument treats a positive signal as strong evidence without accounting for how rare the event is in the population.\nEven with a reasonably accurate test/flag, if the base rate is low, most positives can be false positives.\nConclusion: The claim \u201ca positive signal almost surely means the event is true\u201d is INVALID without incorporating the base rate and error rates.",
        "Hidden Timestamp": "Were the base rate (baseline defect prevalence on the line) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-220",
        "bucket": "BucketLarge-J",
        "title": "Interpreting a Positive Drug Test",
        "scenario": "A workplace drug test is said to be \u201c99% accurate.\u201d Management argues that if a test is positive, the employee almost certainly used drugs.\n\nThis reasoning uses the test\u2019s accuracy as if it directly gave P(Drug use | Positive), without considering the base rate of drug use in the tested workforce.",
        "variables": [
            "Signal = positive drug test result",
            "Event = underlying condition/event of interest",
            "\u03c0 = drug-use prevalence in the workforce"
        ],
        "annotations": {
            "Case ID": "J2-220",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Conditional Fallacy",
            "Difficulty": "Hard",
            "Subdomain": "Workplace Policy",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (positive drug test result), what is the probability the underlying event is actually true?\nHow does the base rate (drug-use prevalence in the workforce) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test\u2019s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes\u2019 rule.",
        "Hidden Timestamp": "Were the base rate (drug-use prevalence in the workforce) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-221",
        "bucket": "BucketLarge-J",
        "title": "Harassment Report and Guilt",
        "scenario": "A company receives an anonymous harassment report. A manager says, \u201cMost real harassers get reported, so if someone is reported they are likely guilty.\u201d\n\nThis confuses the likelihood of a report given guilt with the probability of guilt given a report, and ignores how common false or ambiguous reports are relative to true cases.",
        "variables": [
            "Signal = being reported",
            "Event = underlying condition/event of interest",
            "\u03c0 = prevalence of actual harassment among employees"
        ],
        "annotations": {
            "Case ID": "J2-221",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Conditional Fallacy",
            "Difficulty": "Hard",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (being reported), what is the probability the underlying event is actually true?\nHow does the base rate (prevalence of actual harassment among employees) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test\u2019s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes\u2019 rule.",
        "Hidden Timestamp": "Were the base rate (prevalence of actual harassment among employees) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-222",
        "bucket": "BucketLarge-J",
        "title": "Predictive Policing \u201cHigh-Risk\u201d Label",
        "scenario": "A predictive policing tool labels a neighborhood as \u201chigh risk.\u201d An official argues, \u201cHigh-risk labels are accurate because most high-crime neighborhoods get labeled high risk.\u201d\n\nThis mixes up P(labeled | high crime) with P(high crime | labeled) and ignores how many neighborhoods receive labels relative to the true high-crime base rate.",
        "variables": [
            "Signal = high-risk label",
            "Event = underlying condition/event of interest",
            "\u03c0 = base rate of truly high-crime neighborhoods"
        ],
        "annotations": {
            "Case ID": "J2-222",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Conditional Fallacy",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (high-risk label), what is the probability the underlying event is actually true?\nHow does the base rate (base rate of truly high-crime neighborhoods) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test\u2019s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes\u2019 rule.",
        "Hidden Timestamp": "Were the base rate (base rate of truly high-crime neighborhoods) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-223",
        "bucket": "BucketLarge-J",
        "title": "Spam Filter and Important Emails",
        "scenario": "An email filter catches \u201c95% of spam.\u201d A user claims that any email sent to spam is almost certainly spam and deletes the folder regularly.\n\nThis uses P(sent to spam | spam) as if it were P(spam | sent to spam) and ignores the fraction of all emails that are spam and the false positive rate.",
        "variables": [
            "Signal = email sent to spam folder",
            "Event = underlying condition/event of interest",
            "\u03c0 = base rate of spam among all incoming emails"
        ],
        "annotations": {
            "Case ID": "J2-223",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Conditional Fallacy",
            "Difficulty": "Medium",
            "Subdomain": "Information Systems",
            "Causal Structure": "Signal is an imperfect indicator of a rare event; posterior probability depends on \u03c0 and error rates.",
            "Key Insight": "A positive flag can be misleading when the underlying event is rare or when conditional probabilities are confused.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Given the signal (email sent to spam folder), what is the probability the underlying event is actually true?\nHow does the base rate (base rate of spam among all incoming emails) affect interpretation of the signal?",
        "expected_analysis": "This is Base-rate Neglect (Conditional Fallacy).\nThe reasoning confuses P(Event | Signal) with P(Signal | Event) (or otherwise mixes conditional directions).\nTo answer the question, you need the base rate and the test\u2019s sensitivity/specificity (or false-positive rate).\nConclusion: The inference drawn from the signal is INVALID as stated; compute the correct posterior using Bayes\u2019 rule.",
        "Hidden Timestamp": "Were the base rate (base rate of spam among all incoming emails) and the signal\u2019s false-positive/false-negative rates established before interpreting the current flagged cases?",
        "Conditional Answers": "Answer if the base rate is high (event common): A positive signal is more likely to be a true positive.\nAnswer if the base rate is very low (event rare): A large fraction of positives may be false positives even with good accuracy.\nAnswer if you have sensitivity and specificity: Use Bayes\u2019 rule to compute P(Event | Signal) rather than relying on intuition.",
        "Wise Refusal": "I can\u2019t give the probability without the base rate and the signal\u2019s accuracy (false positive/negative rates); ignoring them risks major misinterpretation."
    },
    {
        "id": "T3-BucketJ-224",
        "bucket": "BucketLarge-J",
        "title": "College Program Counterfactual Using Post-Program Confidence",
        "scenario": "A university asks: \u201cWould participants have graduated if they had not joined an intensive college-success program?\u201d An analyst proposes comparing graduates among participants and non-participants only within the subset of students who report high academic confidence at the end of the first year.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic end-of-year academic confidence.",
        "variables": [
            "X = program participation",
            "Y = graduation",
            "M = end-of-year academic confidence (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-224",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Higher Education",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on end-of-year academic confidence?\nWhy is end-of-year academic confidence a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does end-of-year academic confidence occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if end-of-year academic confidence is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if end-of-year academic confidence is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether end-of-year academic confidence is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-225",
        "bucket": "BucketLarge-J",
        "title": "Police Training Counterfactual Using Post-Training Attitudes",
        "scenario": "A department asks: \u201cWould officer complaints have fallen if we had not implemented a de-escalation training?\u201d A report proposes comparing trained vs. untrained officers only among those who later report high commitment to de-escalation principles.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-training de-escalation commitment.",
        "variables": [
            "X = de-escalation training",
            "Y = citizen complaints",
            "M = post-training de-escalation commitment (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-225",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on post-training de-escalation commitment?\nWhy is post-training de-escalation commitment a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does post-training de-escalation commitment occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if post-training de-escalation commitment is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-training de-escalation commitment is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether post-training de-escalation commitment is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-226",
        "bucket": "BucketLarge-J",
        "title": "Cash Transfer Counterfactual Using Later Savings",
        "scenario": "A city asks: \u201cWould households have avoided eviction if we had not provided a one-time cash transfer?\u201d An analyst suggests comparing treated and untreated households only among those who later have a high savings balance.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later savings balance.",
        "variables": [
            "X = cash transfer receipt",
            "Y = eviction occurrence",
            "M = later savings balance (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-226",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Housing Policy",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on later savings balance?\nWhy is later savings balance a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does later savings balance occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if later savings balance is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if later savings balance is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether later savings balance is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-227",
        "bucket": "BucketLarge-J",
        "title": "Remote Work Counterfactual Using Post-Policy Team Cohesion",
        "scenario": "A firm asks: \u201cWould productivity have been higher if we had not moved to remote work?\u201d A manager proposes comparing remote vs. on-site teams only among teams that later report high cohesion.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-policy team cohesion score.",
        "variables": [
            "X = remote-work policy exposure",
            "Y = team productivity",
            "M = post-policy team cohesion score (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-227",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on post-policy team cohesion score?\nWhy is post-policy team cohesion score a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does post-policy team cohesion score occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if post-policy team cohesion score is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-policy team cohesion score is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether post-policy team cohesion score is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-228",
        "bucket": "BucketLarge-J",
        "title": "Mentoring Counterfactual Using Later Network Size",
        "scenario": "A nonprofit asks: \u201cWould mentees have found jobs as quickly if they had not received mentoring?\u201d The evaluation compares mentored vs. non-mentored applicants only among those who later report having a large professional network.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later professional network size.",
        "variables": [
            "X = mentoring participation",
            "Y = time to job offer",
            "M = later professional network size (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-228",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Labor Economics",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on later professional network size?\nWhy is later professional network size a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does later professional network size occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if later professional network size is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if later professional network size is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether later professional network size is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-229",
        "bucket": "BucketLarge-J",
        "title": "Public Transit Subsidy Counterfactual Using Post-Subsidy Ridership",
        "scenario": "A city asks: \u201cWould congestion have been worse if we had not subsidized transit?\u201d An analyst compares subsidized vs. non-subsidized commuters only among those who became frequent transit riders afterward.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-subsidy ridership frequency.",
        "variables": [
            "X = transit subsidy exposure",
            "Y = traffic congestion contribution",
            "M = post-subsidy ridership frequency (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-229",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Transportation Policy",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on post-subsidy ridership frequency?\nWhy is post-subsidy ridership frequency a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does post-subsidy ridership frequency occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if post-subsidy ridership frequency is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-subsidy ridership frequency is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether post-subsidy ridership frequency is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-230",
        "bucket": "BucketLarge-J",
        "title": "Moderation Policy Counterfactual Using Later User Engagement",
        "scenario": "A platform asks: \u201cWould toxicity have been higher if we had not changed the moderation policy?\u201d A team proposes comparing communities with and without the policy only among communities that later exhibit high engagement.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic later engagement level.",
        "variables": [
            "X = moderation policy change",
            "Y = toxicity rate",
            "M = later engagement level (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-230",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Platform Policy",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on later engagement level?\nWhy is later engagement level a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does later engagement level occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if later engagement level is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if later engagement level is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether later engagement level is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-231",
        "bucket": "BucketLarge-J",
        "title": "Smoking Cessation Campaign Counterfactual Using Post-Campaign Motivation",
        "scenario": "A public health agency asks: \u201cWould smoking rates have fallen without the campaign?\u201d A report compares exposed vs. unexposed groups only among individuals who later report high motivation to quit.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-campaign motivation to quit.",
        "variables": [
            "X = campaign exposure",
            "Y = smoking cessation",
            "M = post-campaign motivation to quit (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-231",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on post-campaign motivation to quit?\nWhy is post-campaign motivation to quit a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does post-campaign motivation to quit occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if post-campaign motivation to quit is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-campaign motivation to quit is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether post-campaign motivation to quit is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-232",
        "bucket": "BucketLarge-J",
        "title": "Scholarship Counterfactual Using Post-Scholarship Course Load",
        "scenario": "A college asks: \u201cWould scholarship recipients have maintained the same GPA without the scholarship?\u201d The analysis compares scholarship and non-scholarship students only among those who later took a heavy course load.\n\nA policymaker asks a counterfactual: \u201cIf X had been different, would Y have changed?\u201d They propose answering it by comparing people who did and did not receive X, *but only among those with the same value of* a post-intervention characteristic post-receipt course load intensity.",
        "variables": [
            "X = scholarship receipt",
            "Y = semester GPA",
            "M = post-receipt course load intensity (post-intervention)"
        ],
        "annotations": {
            "Case ID": "J2-232",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Higher Education",
            "Causal Structure": "X \u2192 M \u2192 Y, and conditioning on M mixes information across counterfactual worlds (cross-world bias).",
            "Key Insight": "Conditioning on post-treatment variables to answer counterfactuals can create cross-world confounding.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we answer the counterfactual question \u201cWhat would Y have been if X were different?\u201d by conditioning on post-receipt course load intensity?\nWhy is post-receipt course load intensity a cross-world confounder in this counterfactual comparison?",
        "expected_analysis": "This is L3 Confounding (Cross-world Confounder).\nThe proposed adjustment conditions on M, a post-intervention variable affected by X that also influences Y.\nIn counterfactual terms, you cannot simultaneously fix M to the value it would take under X=1 and compare outcomes under X=0 without \u201ccross-world\u201d assumptions; conditioning on M can induce bias.\nConclusion: The proposed counterfactual answer is INVALID; you need a valid causal estimand/design (e.g., total effect without conditioning on post-treatment M, or a properly defined mediation analysis with strong assumptions).",
        "Hidden Timestamp": "Does post-receipt course load intensity occur after X is assigned, meaning X can change M before Y is realized?",
        "Conditional Answers": "Answer if post-receipt course load intensity is truly pre-treatment (not affected by X): Then conditioning could be appropriate (but verify timing).\nAnswer if post-receipt course load intensity is influenced by X and affects Y: Do not condition on it for the total effect; redefine the question or use mediation methods with explicit assumptions.\nAnswer if you have a randomized X and want the total effect: Compare Y across X groups without conditioning on post-treatment M.",
        "Wise Refusal": "I can\u2019t answer the counterfactual without clarifying whether post-receipt course load intensity is post-treatment and without specifying the target estimand (total effect vs. direct/mediated effects)."
    },
    {
        "id": "T3-BucketJ-233",
        "bucket": "BucketLarge-J",
        "title": "Housing Vouchers and Employment: Fixing Neighborhood Quality",
        "scenario": "A county asks: \u201cWould employment rise if we offered housing vouchers, but kept families\u2019 neighborhood quality fixed at what it is now?\u201d They argue this isolates the \u201cvoucher effect\u201d that is not due to moving.\n\nIn practice, vouchers often enable moves to different neighborhoods. Neighborhood quality affects access to jobs, networks, transportation, and childcare, which all influence employment outcomes.",
        "variables": [
            "X = housing voucher receipt",
            "Y = employment status",
            "M = neighborhood quality/opportunity level (mediator)"
        ],
        "annotations": {
            "Case ID": "J2-233",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounder\u2013Mediator Error",
            "Trap Subtype": "Mediator Fixing Error",
            "Difficulty": "Hard",
            "Subdomain": "Housing Policy",
            "Causal Structure": "X \u2192 M \u2192 Y; attempting to fix M while changing X creates an ill-defined counterfactual unless mediation assumptions are made explicit.",
            "Key Insight": "Holding a mediator fixed while changing the treatment can create an incoherent counterfactual world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to answer \u201cWhat would Y be if we change X but keep M fixed?\u201d when M is affected by X?\nWhat goes wrong when we try to fix the mediator to its observed value?",
        "expected_analysis": "This is Confounder\u2013Mediator Error (Mediator Fixing Error).\nM lies on the causal pathway from X to Y. The proposed counterfactual \u201cchange X while holding M fixed\u201d is generally not identifiable from standard observational data and can be logically inconsistent with how M would respond to X.\nFixing M can create a counterfactual world that is incompatible with the causal system, leading to biased or undefined effects.\nConclusion: The proposed counterfactual is INVALID unless you define a well-posed mediation estimand and justify strong assumptions (or perform a suitable intervention on M).",
        "Hidden Timestamp": "Does M occur after X and before Y, making it a mediator rather than a baseline confounder?",
        "Conditional Answers": "Answer if you want the total effect of X on Y: Do not fix M; compare Y under different X values.\nAnswer if you want a direct effect not through M: Use formal mediation analysis with explicit assumptions, or design an intervention that manipulates M.\nAnswer if M is actually pre-treatment: Then fixing M may be reasonable, but verify timing carefully.",
        "Wise Refusal": "I can\u2019t answer the \u201chold M fixed\u201d counterfactual without clarifying whether M is post-treatment and specifying the mediation estimand and assumptions."
    },
    {
        "id": "T3-BucketJ-234",
        "bucket": "BucketLarge-J",
        "title": "Overtime and Promotion: A Counterfactual Misorder (2)",
        "scenario": "A manager asks: \u201cWould Alex have been promoted if they hadn\u2019t worked so much overtime?\u201d The manager treats overtime as the cause and promotion as the effect.\n\nBut in this organization, employees are assigned overtime when leadership already expects they are promotion candidates and wants to test them under pressure. Overtime increases after early promotion signals appear.",
        "variables": [
            "X = working overtime",
            "Y = promotion decision"
        ],
        "annotations": {
            "Case ID": "J2-234",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Reverse Causation",
            "Trap Subtype": "Outcome-dependent Worlds",
            "Difficulty": "Hard",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Y (or early Y) \u2192 X; treating X as a cause leads to outcome-dependent counterfactuals.",
            "Key Insight": "Counterfactuals are not valid when the \u201ccause\u201d is actually reacting to the outcome.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is the counterfactual \u201cWhat would Y have been if X were different?\u201d well-defined here?\nHow could the outcome (or anticipation of it) influence X, making the counterfactual world outcome-dependent?",
        "expected_analysis": "This is L3 Reverse Causation (Outcome-dependent Worlds).\nThe setup suggests X is chosen or changes in response to the outcome (or strong early signals of the outcome), so imagining \u201cset X differently\u201d may implicitly change the underlying situation that produced Y.\nIf X is downstream of Y (or of early manifestations of Y), naive counterfactual comparisons can be ill-posed or misleading.\nConclusion: The proposed counterfactual claim is INVALID unless timing is clarified and a causal ordering that makes X antecedent to Y is justified.",
        "Hidden Timestamp": "Did promotion decision (or early indicators of it) occur before changes in working overtime, potentially causing X rather than being caused by it?",
        "Conditional Answers": "Answer if X clearly occurs before Y and can be intervened on: Then the counterfactual is meaningful and can be analyzed with a causal model.\nAnswer if Y (or early Y signals) drives X: Then interpret the relationship as reverse causation; redesign the study to capture pre-outcome X.\nAnswer if you can instrument X with an exogenous shock: Then you may identify a causal effect with strong assumptions.",
        "Wise Refusal": "I can\u2019t assess the counterfactual without a clear timeline showing whether working overtime precedes promotion decision and whether X can be manipulated independently of Y."
    },
    {
        "id": "T3-BucketJ-235",
        "bucket": "BucketLarge-J",
        "title": "Extra Policing and Crime: Reactive Deployment (2)",
        "scenario": "A city council asks: \u201cWould crime have been lower if we had not increased patrols in Neighborhood Q?\u201d A memo treats patrol increases as the cause of crime levels.\n\nPatrol hours are increased in response to early spikes in crime reports and calls for service, so rising crime drives deployment changes rather than the other way around.",
        "variables": [
            "X = increased patrol deployment",
            "Y = crime rate"
        ],
        "annotations": {
            "Case ID": "J2-235",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Reverse Causation",
            "Trap Subtype": "Outcome-dependent Worlds",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Y (or early Y) \u2192 X; treating X as a cause leads to outcome-dependent counterfactuals.",
            "Key Insight": "Counterfactuals are not valid when the \u201ccause\u201d is actually reacting to the outcome.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is the counterfactual \u201cWhat would Y have been if X were different?\u201d well-defined here?\nHow could the outcome (or anticipation of it) influence X, making the counterfactual world outcome-dependent?",
        "expected_analysis": "This is L3 Reverse Causation (Outcome-dependent Worlds).\nThe setup suggests X is chosen or changes in response to the outcome (or strong early signals of the outcome), so imagining \u201cset X differently\u201d may implicitly change the underlying situation that produced Y.\nIf X is downstream of Y (or of early manifestations of Y), naive counterfactual comparisons can be ill-posed or misleading.\nConclusion: The proposed counterfactual claim is INVALID unless timing is clarified and a causal ordering that makes X antecedent to Y is justified.",
        "Hidden Timestamp": "Did crime rate (or early indicators of it) occur before changes in increased patrol deployment, potentially causing X rather than being caused by it?",
        "Conditional Answers": "Answer if X clearly occurs before Y and can be intervened on: Then the counterfactual is meaningful and can be analyzed with a causal model.\nAnswer if Y (or early Y signals) drives X: Then interpret the relationship as reverse causation; redesign the study to capture pre-outcome X.\nAnswer if you can instrument X with an exogenous shock: Then you may identify a causal effect with strong assumptions.",
        "Wise Refusal": "I can\u2019t assess the counterfactual without a clear timeline showing whether increased patrol deployment precedes crime rate and whether X can be manipulated independently of Y."
    },
    {
        "id": "T3-BucketJ-236",
        "bucket": "BucketLarge-J",
        "title": "Recommendation Algorithm and Polarization Over Time",
        "scenario": "A platform asks: \u201cIf we had not changed the recommendation algorithm last year, would political polarization on the site be lower today?\u201d The team wants to extrapolate from a short A/B test run for two weeks.\n\nHowever, the algorithm shapes what content users see, which changes who stays on the platform, how users post, and what content is produced. Those shifts then change future recommendation data and future exposure patterns.",
        "variables": [
            "X = recommendation algorithm change",
            "Y = polarization on the platform",
            "F = user-content ecosystem and retention dynamics (dynamic state)"
        ],
        "annotations": {
            "Case ID": "J2-236",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Feedback Loops",
            "Trap Subtype": "Dynamic World Divergence",
            "Difficulty": "Hard",
            "Subdomain": "Platform Policy",
            "Causal Structure": "X changes system state F; F influences future exposure and outcomes; trajectories diverge across counterfactual worlds.",
            "Key Insight": "With feedback, the alternative world is a different evolving system, not a one-shot swap of X.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why can short-run estimates of changing X fail to predict long-run counterfactual Y under a different policy?\nHow does user-content ecosystem and retention dynamics create dynamic world divergence across counterfactual worlds?",
        "expected_analysis": "This is L3 Feedback Loops (Dynamic World Divergence).\nChanging X alters the system state F over time, and F then changes future behaviors and outcomes, creating different trajectories under different counterfactual policies.\nA static comparison that ignores how the environment adapts can misstate \u201cwhat would have happened\u201d under an alternative X.\nConclusion: The counterfactual claim is CONDITIONAL: you need a dynamic causal model (or simulation/longitudinal design) that accounts for feedback.",
        "Hidden Timestamp": "Does user-content ecosystem and retention dynamics evolve after X changes and then influence future Y, creating path dependence?",
        "Conditional Answers": "Answer if you only care about immediate effects before F adapts: A short-run causal estimate may be informative.\nAnswer if you care about long-run outcomes: Model the feedback dynamics explicitly; the counterfactual path under alternative X can diverge.\nAnswer if policy changes F in a way that changes who is exposed later: Then simple extrapolation from short-run data is unreliable.",
        "Wise Refusal": "I can\u2019t answer the long-run counterfactual without assumptions (or data) about how user-content ecosystem and retention dynamics evolves and how behavior adapts under different X values."
    },
    {
        "id": "T3-BucketJ-237",
        "bucket": "BucketLarge-J",
        "title": "Policing Strategy and Community Trust Trajectories",
        "scenario": "A city asks: \u201cIf we had used a less aggressive policing strategy, would community trust be higher after three years?\u201d An analyst tries to answer using a one-time comparison of neighborhoods with different patrol styles.\n\nPolicing style changes residents\u2019 willingness to report incidents and cooperate, which changes observed crime statistics and subsequent allocation decisions. The policy and the environment co-evolve over time.",
        "variables": [
            "X = policing strategy aggressiveness",
            "Y = community trust after three years",
            "F = reporting/cooperation dynamics affecting future allocation (dynamic state)"
        ],
        "annotations": {
            "Case ID": "J2-237",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Feedback Loops",
            "Trap Subtype": "Dynamic World Divergence",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "X changes system state F; F influences future exposure and outcomes; trajectories diverge across counterfactual worlds.",
            "Key Insight": "With feedback, the alternative world is a different evolving system, not a one-shot swap of X.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Why can short-run estimates of changing X fail to predict long-run counterfactual Y under a different policy?\nHow does reporting/cooperation dynamics affecting future allocation create dynamic world divergence across counterfactual worlds?",
        "expected_analysis": "This is L3 Feedback Loops (Dynamic World Divergence).\nChanging X alters the system state F over time, and F then changes future behaviors and outcomes, creating different trajectories under different counterfactual policies.\nA static comparison that ignores how the environment adapts can misstate \u201cwhat would have happened\u201d under an alternative X.\nConclusion: The counterfactual claim is CONDITIONAL: you need a dynamic causal model (or simulation/longitudinal design) that accounts for feedback.",
        "Hidden Timestamp": "Does reporting/cooperation dynamics affecting future allocation evolve after X changes and then influence future Y, creating path dependence?",
        "Conditional Answers": "Answer if you only care about immediate effects before F adapts: A short-run causal estimate may be informative.\nAnswer if you care about long-run outcomes: Model the feedback dynamics explicitly; the counterfactual path under alternative X can diverge.\nAnswer if policy changes F in a way that changes who is exposed later: Then simple extrapolation from short-run data is unreliable.",
        "Wise Refusal": "I can\u2019t answer the long-run counterfactual without assumptions (or data) about how reporting/cooperation dynamics affecting future allocation evolves and how behavior adapts under different X values."
    },
    {
        "id": "T3-BucketJ-238",
        "bucket": "BucketLarge-J",
        "title": "Early Preemption in a Protest Outcome",
        "scenario": "A city asks: \u201cIf the primary protest organizer had been arrested the night before, would the protest have happened?\u201d In the observed world, the organizer led the march and the protest occurred.\n\nMultiple groups were prepared to take over leadership. If the arrest had happened, a backup organizer who was already mobilizing might have led an alternative march that started earlier and still resulted in a protest.",
        "variables": [
            "X = arresting the primary organizer",
            "Y = a large protest occurs",
            "A = alternative sufficient cause"
        ],
        "annotations": {
            "Case ID": "J2-238",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Preemption",
            "Trap Subtype": "Early Preemption",
            "Difficulty": "Hard",
            "Subdomain": "Political Science",
            "Causal Structure": "Two sufficient causes for Y; removing one may not change Y because the other would occur.",
            "Key Insight": "Observed causation does not imply counterfactual dependence when multiple sufficient causes can produce the outcome.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we prevent arresting the primary organizer, does Y necessarily change?\nHow does early preemption complicate attributing Y to X in the counterfactual world?",
        "expected_analysis": "This is L3 Preemption (Early Preemption).\nThere are multiple sufficient causal paths to Y. Even if X occurs in the observed world, removing X in the counterfactual world may not change Y because an alternative cause would produce Y instead.\nAttribution requires modeling which cause would have fired in the absence of X (counterfactual dependence), not just observing that X happened.\nConclusion: The naive statement \u201cX caused Y\u201d is CONDITIONAL; it depends on whether Y counterfactually depends on X given competing causes.",
        "Hidden Timestamp": "If arresting the primary organizer were prevented, what alternative cause would have been most likely to produce Y, and would it occur earlier or later?",
        "Conditional Answers": "Answer if no other sufficient cause exists: Then preventing X would prevent Y, so X is a but-for cause.\nAnswer if alternative sufficient causes exist: Y may still occur without X; you need a structural model of competing causes and timing.\nAnswer if timing determines which cause preempts the other: Identify whether X was early/late relative to the alternative cause to assess counterfactual dependence.",
        "Wise Refusal": "I can\u2019t decide whether X is a but-for cause without specifying competing causes and their timing; otherwise the counterfactual is underdetermined."
    },
    {
        "id": "T3-BucketJ-239",
        "bucket": "BucketLarge-J",
        "title": "Late Preemption in a Power Outage",
        "scenario": "A utility asks: \u201cIf the first transformer had not failed, would the neighborhood still have lost power?\u201d In the observed world, transformer A failed and an outage occurred.\n\nBut transformer B was already overheating due to the same heatwave. Even if transformer A had not failed, transformer B likely would have failed later that day, still producing an outage.",
        "variables": [
            "X = failure of transformer A",
            "Y = power outage occurs",
            "A = alternative sufficient cause"
        ],
        "annotations": {
            "Case ID": "J2-239",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Preemption",
            "Trap Subtype": "Late Preemption",
            "Difficulty": "Hard",
            "Subdomain": "Infrastructure Policy",
            "Causal Structure": "Two sufficient causes for Y; removing one may not change Y because the other would occur.",
            "Key Insight": "Observed causation does not imply counterfactual dependence when multiple sufficient causes can produce the outcome.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If we prevent failure of transformer A, does Y necessarily change?\nHow does late preemption complicate attributing Y to X in the counterfactual world?",
        "expected_analysis": "This is L3 Preemption (Late Preemption).\nThere are multiple sufficient causal paths to Y. Even if X occurs in the observed world, removing X in the counterfactual world may not change Y because an alternative cause would produce Y instead.\nAttribution requires modeling which cause would have fired in the absence of X (counterfactual dependence), not just observing that X happened.\nConclusion: The naive statement \u201cX caused Y\u201d is CONDITIONAL; it depends on whether Y counterfactually depends on X given competing causes.",
        "Hidden Timestamp": "If failure of transformer A were prevented, what alternative cause would have been most likely to produce Y, and would it occur earlier or later?",
        "Conditional Answers": "Answer if no other sufficient cause exists: Then preventing X would prevent Y, so X is a but-for cause.\nAnswer if alternative sufficient causes exist: Y may still occur without X; you need a structural model of competing causes and timing.\nAnswer if timing determines which cause preempts the other: Identify whether X was early/late relative to the alternative cause to assess counterfactual dependence.",
        "Wise Refusal": "I can\u2019t decide whether X is a but-for cause without specifying competing causes and their timing; otherwise the counterfactual is underdetermined."
    },
    {
        "id": "T3-BucketJ-240",
        "bucket": "BucketLarge-J",
        "title": "Job Training, Confidence, and Employment: Fixing the Mediator",
        "scenario": "A policymaker asks: \u201cWould employment increase if we offered job training, but kept participants\u2019 confidence fixed at its current level?\u201d They argue this isolates the \u2018non-confidence\u2019 part of training.\n\nTraining often changes participants\u2019 confidence, and confidence itself affects job search intensity and employer interactions.",
        "variables": [
            "X = job training offer",
            "Y = employment probability",
            "M = self-confidence level (mediator)"
        ],
        "annotations": {
            "Case ID": "J2-240",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounder\u2013Mediator Error",
            "Trap Subtype": "Mediator Fixing Error",
            "Difficulty": "Hard",
            "Subdomain": "Labor Economics",
            "Causal Structure": "X \u2192 M \u2192 Y; attempting to fix M while changing X creates an ill-defined counterfactual unless mediation assumptions are made explicit.",
            "Key Insight": "Holding a mediator fixed while changing the treatment can create an incoherent counterfactual world.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Is it valid to answer \u201cWhat would Y be if we change X but keep M fixed?\u201d when M is affected by X?\nWhat goes wrong when we try to fix the mediator to its observed value?",
        "expected_analysis": "This is Confounder\u2013Mediator Error (Mediator Fixing Error).\nM lies on the causal pathway from X to Y. The proposed counterfactual \u201cchange X while holding M fixed\u201d is generally not identifiable from standard observational data and can be logically inconsistent with how M would respond to X.\nFixing M can create a counterfactual world that is incompatible with the causal system, leading to biased or undefined effects.\nConclusion: The proposed counterfactual is INVALID unless you define a well-posed mediation estimand and justify strong assumptions (or perform a suitable intervention on M).",
        "Hidden Timestamp": "Does M occur after X and before Y, making it a mediator rather than a baseline confounder?",
        "Conditional Answers": "Answer if you want the total effect of X on Y: Do not fix M; compare Y under different X values.\nAnswer if you want a direct effect not through M: Use formal mediation analysis with explicit assumptions, or design an intervention that manipulates M.\nAnswer if M is actually pre-treatment: Then fixing M may be reasonable, but verify timing carefully.",
        "Wise Refusal": "I can\u2019t answer the \u201chold M fixed\u201d counterfactual without clarifying whether M is post-treatment and specifying the mediation estimand and assumptions."
    },
    {
        "id": "T3-BucketJ-01",
        "bucket": "BucketLarge-J",
        "title": "The Department Graduation Rates",
        "scenario": "A university releases a report stating that Department A has a higher overall graduation rate than Department B. Administrators conclude that Department A\u2019s curriculum is more effective and consider expanding it.\nHowever, a faculty member notes that when graduation rates are broken down by student preparedness level (high vs. low incoming GPA), Department B has higher graduation rates in both groups.\nThe discrepancy arises because Department A enrolls a much larger proportion of high-preparedness students, while Department B enrolls more low-preparedness students overall.",
        "variables": [
            "X = Department (A vs. B)",
            "Y = Graduation rate",
            "Z = Student preparedness level (high / low incoming GPA)"
        ],
        "annotations": {
            "Case ID": "J2-01",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Aggregation Bias",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Student preparedness (Z) affects graduation (Y) and differs in distribution across departments (X). Aggregating across Z reverses subgroup-level trends.",
            "Key Insight": "Aggregate performance metrics can contradict subgroup-level performance due to population composition.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the higher overall graduation rate of Department A imply that its curriculum is more effective than Department B\u2019s?\nHow does conditioning on student preparedness level change the interpretation of the graduation data?",
        "expected_analysis": "This case requires associational reasoning and identification of Simpson\u2019s Paradox.\nKey reasoning step: Recognize that student preparedness (Z) is a confounding variable that strongly influences graduation outcomes and is unevenly distributed across departments.\nHidden temporal structure: Student preparedness is determined prior to department enrollment, so it cannot be caused by the department.\nSubgroup analysis: When conditioning on Z, Department B outperforms Department A for both preparedness levels.\nFailure mode: Inferring causal superiority of a curriculum from aggregate outcomes without stratification.\nCorrect conclusion:\nThe claim that Department A\u2019s curriculum is superior is INVALID. The aggregate association reflects differences in student composition rather than instructional quality.\nWise refusal:\nA valid assessment of curricular effectiveness would require controlling for preparedness or using a causal design (e.g., randomized assignment or matched cohorts).",
        "Hidden Timestamp": "Was Student preparedness level (high / low incoming GPA) determined before Department (A vs. B) was chosen, and could Student preparedness level (high / low incoming GPA) have influenced the choice of Department (A vs. B) before Graduation rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Department (A vs. B) on Graduation rate may be reversed because the mix of subgroups differs between Department (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Student preparedness level (high / low incoming GPA): Use the within-stratum differences (or a standardized effect). If Department (A vs. B) improves Graduation rate in each stratum, prefer Department (A vs. B) even if the aggregate looks worse.\nAnswer if Department (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Graduation rate by the key strata (e.g., Student preparedness level (high / low incoming GPA) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Department (A vs. B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-02",
        "bucket": "BucketLarge-J",
        "title": "The Crime Clearance Rates",
        "scenario": "A city publishes annual crime statistics showing that District East has a higher overall crime clearance rate than District West. City officials argue that policing strategies used in District East are more effective and consider expanding them citywide.\nHowever, analysts examining the data by crime severity (violent vs. non-violent offenses) find that District West has higher clearance rates for both categories.\nThe apparent contradiction arises because District East handles a much larger share of non-violent crimes, which are generally easier to solve, while District West deals disproportionately with violent crimes.",
        "variables": [
            "X = Police district (East vs. West)",
            "Y = Crime clearance rate",
            "Z = Crime severity (violent / non-violent)"
        ],
        "annotations": {
            "Case ID": "J2-02",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Imbalanced Group Composition",
            "Difficulty": "Medium",
            "Subdomain": "Criminology",
            "Causal Structure": "Crime severity (Z) affects clearance probability (Y) and differs in prevalence across districts (X). Aggregation across Z reverses subgroup-level performance.",
            "Key Insight": "Aggregate performance metrics can mask inferior performance within every relevant subgroup.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the higher overall clearance rate in District East imply more effective policing than in District West?\nWhy does stratifying by crime severity reverse the apparent ranking between the districts?",
        "expected_analysis": "This case requires associational reasoning and recognition of Simpson\u2019s Paradox.\nKey reasoning step: Crime severity (Z) strongly influences clearance rates and is unevenly distributed across districts.\nHidden temporal structure: Crime severity is determined before police investigation begins and is not caused by district-level policing.\nSubgroup analysis: When violent and non-violent crimes are analyzed separately, District West outperforms District East in both categories.\nFailure mode: Interpreting aggregate clearance rates as evidence of policing effectiveness without accounting for case mix.\nCorrect conclusion:\nThe claim that District East\u2019s policing strategy is superior is INVALID.\nWise refusal:\nValid comparisons require risk-adjusted or severity-adjusted clearance metrics rather than raw aggregates.",
        "Hidden Timestamp": "Was Crime severity (violent / non-violent) determined before Police district (East vs. West) was chosen, and could Crime severity (violent / non-violent) have influenced the choice of Police district (East vs. West) before Crime clearance rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Police district (East vs. West) on Crime clearance rate may be reversed because the mix of subgroups differs between Police district (East vs. West) arms.\nAnswer if you compare within strata after stratifying/standardizing by Crime severity (violent / non-violent): Use the within-stratum differences (or a standardized effect). If Police district (East vs. West) improves Crime clearance rate in each stratum, prefer Police district (East vs. West) even if the aggregate looks worse.\nAnswer if Police district (East vs. West) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Crime clearance rate by the key strata (e.g., Crime severity (violent / non-violent) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Police district (East vs. West) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-03",
        "bucket": "BucketLarge-J",
        "title": "The Online Course Completion Rates",
        "scenario": "An online education platform reports that Course X has a higher overall completion rate than Course Y. The platform promotes Course X as better designed and more engaging.\nHowever, when completion rates are examined by learner experience level (beginner vs. advanced), Course Y shows higher completion rates in both groups.\nThis occurs because Course X attracts a much larger proportion of advanced learners, who are more likely to complete any course regardless of design quality.",
        "variables": [
            "X = Course (X vs. Y)",
            "Y = Course completion rate",
            "Z = Learner experience level (beginner / advanced)"
        ],
        "annotations": {
            "Case ID": "J2-03",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Aggregation Bias",
            "Difficulty": "Easy",
            "Subdomain": "Education Technology",
            "Causal Structure": "Learner experience (Z) affects completion (Y) and differs across courses (X), producing a reversal when aggregated.",
            "Key Insight": "Differences in participant composition can dominate aggregate outcome measures.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does Course X\u2019s higher overall completion rate demonstrate superior instructional design?\nHow does learner experience level alter the interpretation of completion rates?",
        "expected_analysis": "This case requires associational reasoning and identification of Simpson\u2019s Paradox.\nKey reasoning step: Learner experience (Z) is a confounder that influences completion rates and is unevenly distributed across courses.\nHidden temporal structure: Learner experience is determined before course enrollment.\nSubgroup analysis: Course Y outperforms Course X among both beginners and advanced learners.\nFailure mode: Treating aggregate engagement metrics as causal indicators of quality.\nCorrect conclusion:\nThe claim that Course X is better designed is INVALID.\nWise refusal:\nCourse effectiveness should be evaluated within comparable learner groups or via randomized exposure.",
        "Hidden Timestamp": "Was Learner experience level (beginner / advanced) determined before Course (X vs. Y) was chosen, and could Learner experience level (beginner / advanced) have influenced the choice of Course (X vs. Y) before Course completion rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Course (X vs. Y) on Course completion rate may be reversed because the mix of subgroups differs between Course (X vs. Y) arms.\nAnswer if you compare within strata after stratifying/standardizing by Learner experience level (beginner / advanced): Use the within-stratum differences (or a standardized effect). If Course (X vs. Y) improves Course completion rate in each stratum, prefer Course (X vs. Y) even if the aggregate looks worse.\nAnswer if Course (X vs. Y) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Course completion rate by the key strata (e.g., Learner experience level (beginner / advanced) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Course (X vs. Y) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-04",
        "bucket": "BucketLarge-J",
        "title": "The Customer Satisfaction Survey",
        "scenario": "A company reports that 92% of customers are satisfied with its new subscription service, based on responses to a voluntary online satisfaction survey. Executives conclude that the service rollout has been a major success.\nHowever, internal data shows that only 15% of customers responded to the survey. Customer support logs indicate that dissatisfied users are more likely to cancel their subscriptions and disengage from company communications altogether.",
        "variables": [
            "X = Survey response participation",
            "Y = Reported customer satisfaction",
            "Z = Customer satisfaction status (satisfied / dissatisfied)"
        ],
        "annotations": {
            "Case ID": "J2-04",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Easy",
            "Subdomain": "Consumer Behavior",
            "Causal Structure": "Customer satisfaction (Z) affects likelihood of survey participation (X), and the outcome (Y) is measured only among respondents.",
            "Key Insight": "Observed satisfaction reflects who responds, not the true customer population.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the 92% satisfaction rate accurately represent overall customer sentiment?\nHow does voluntary participation affect the interpretation of survey results?",
        "expected_analysis": "This case requires associational reasoning and identification of selection bias.\nKey reasoning step: Survey participation (X) is not random; it depends on satisfaction status (Z).\nHidden temporal structure: Satisfaction exists before the decision to respond to the survey.\nFailure mode: Treating respondent statistics as representative of the full population.\nCorrect conclusion:\nThe claim that most customers are satisfied is INVALID.\nWise refusal:\nValid inference would require correcting for non-response or using randomized sampling methods.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Reported customer satisfaction occurred\u2014and is selection related to Customer satisfaction status (satisfied / dissatisfied) or Reported customer satisfaction?",
        "Conditional Answers": "Answer if Survey response participation is randomly assigned: A difference in Reported customer satisfaction across Survey response participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Customer satisfaction status (satisfied / dissatisfied)): The Survey response participation vs not-Survey response participation difference in Reported customer satisfaction is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Customer satisfaction status (satisfied / dissatisfied)) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Customer satisfaction status (satisfied / dissatisfied)); otherwise Survey response participation\u2013Reported customer satisfaction differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-05",
        "bucket": "BucketLarge-J",
        "title": "The Startup Accelerator Success Rate",
        "scenario": "A startup accelerator advertises that 70% of companies in its program succeed, defining success as raising a Series A funding round within two years. Aspiring founders interpret this statistic as evidence that participation dramatically increases startup success.\nHowever, the accelerator accepts only a small fraction of applicants and explicitly selects founders with strong prior experience, existing traction, and elite educational backgrounds.",
        "variables": [
            "X = Accelerator participation",
            "Y = Startup success (Series A funding)",
            "Z = Founder quality / prior advantages"
        ],
        "annotations": {
            "Case ID": "J2-05",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Medium",
            "Subdomain": "Entrepreneurship",
            "Causal Structure": "Founder quality (Z) affects both selection into the accelerator (X) and startup success (Y).",
            "Key Insight": "High success rates may reflect who is admitted rather than program effectiveness.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the reported success rate imply that the accelerator causes startups to succeed?\nWhat role does the selection process play in shaping observed outcomes?",
        "expected_analysis": "This case requires associational reasoning and identification of selection bias.\nKey reasoning step: Accelerator participation is correlated with success because both are driven by founder quality (Z).\nHidden temporal structure: Founder characteristics exist before accelerator admission.\nFailure mode: Inferring causal impact from outcomes among a selectively chosen group.\nCorrect conclusion:\nThe causal claim about accelerator effectiveness is INVALID.\nWise refusal:\nEstimating causal impact would require comparing accepted startups to comparable rejected applicants.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Startup success (Series A funding) occurred\u2014and is selection related to Founder quality / prior advantages or Startup success (Series A funding)?",
        "Conditional Answers": "Answer if Accelerator participation is randomly assigned: A difference in Startup success (Series A funding) across Accelerator participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Founder quality / prior advantages): The Accelerator participation vs not-Accelerator participation difference in Startup success (Series A funding) is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Founder quality / prior advantages) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Founder quality / prior advantages); otherwise Accelerator participation\u2013Startup success (Series A funding) differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-06",
        "bucket": "BucketLarge-J",
        "title": "The Employee Wellness Program",
        "scenario": "A company reports that employees enrolled in its voluntary wellness program take fewer sick days than employees who do not enroll. Management concludes that the wellness program improves employee health and considers expanding it.\nFurther examination reveals that employees who opt into the program tend to be healthier, more health-conscious, and more engaged with company initiatives even before the program begins.",
        "variables": [
            "X = Wellness program participation",
            "Y = Number of sick days taken",
            "Z = Baseline employee health / engagement"
        ],
        "annotations": {
            "Case ID": "J2-06",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Sampling-on-the-Outcome",
            "Difficulty": "Medium",
            "Subdomain": "Workplace Health",
            "Causal Structure": "Baseline health (Z) influences both program participation (X) and health outcomes (Y).",
            "Key Insight": "Voluntary programs often attract participants who would perform better regardless.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can the difference in sick days be attributed to the wellness program itself?\nWhy does voluntary enrollment complicate causal interpretation?",
        "expected_analysis": "This case requires associational reasoning and recognition of selection bias.\nKey reasoning step: Healthier employees are more likely to enroll, biasing comparisons.\nHidden temporal structure: Baseline health precedes enrollment.\nFailure mode: Mistaking correlation within a selected group for causal impact.\nCorrect conclusion:\nThe claim that the wellness program reduces sick days is INVALID.\nWise refusal:\nCausal evaluation would require random assignment or strong controls for baseline health.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Number of sick days taken occurred\u2014and is selection related to Baseline employee health / engagement or Number of sick days taken?",
        "Conditional Answers": "Answer if Wellness program participation is randomly assigned: A difference in Number of sick days taken across Wellness program participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Baseline employee health / engagement): The Wellness program participation vs not-Wellness program participation difference in Number of sick days taken is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Baseline employee health / engagement) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Baseline employee health / engagement); otherwise Wellness program participation\u2013Number of sick days taken differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-07",
        "bucket": "BucketLarge-J",
        "title": "The Wealthy State, Poor Voter",
        "scenario": "An analyst observes that states with higher average household income tend to vote for Party A in national elections. Based on this pattern, a commentator concludes that wealthier individuals are more likely to support Party A.\nHowever, individual-level polling data within states shows that higher-income individuals are actually more likely to support Party B, while lower-income individuals are more likely to support Party A.\nThe apparent contradiction arises because wealthier states differ from poorer states in urbanization, education levels, and industry composition.",
        "variables": [
            "X = State-level average income",
            "Y = Voting outcome (Party A vs. Party B)",
            "Z = Individual income level"
        ],
        "annotations": {
            "Case ID": "J2-07",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Socioeconomic",
            "Difficulty": "Medium",
            "Subdomain": "Political Science",
            "Causal Structure": "State-level income aggregates over heterogeneous individuals; group-level correlations do not reflect individual-level behavior.",
            "Key Insight": "Correlations observed at the group level cannot be assumed to hold at the individual level.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the voting pattern of wealthy states imply that wealthy individuals support Party A?\nWhy can state-level correlations mislead conclusions about individual behavior?",
        "expected_analysis": "This case requires associational reasoning and identification of the ecological fallacy.\nKey reasoning step: State-level income is an aggregate statistic that obscures within-state heterogeneity.\nHidden temporal structure: Individual income precedes voting decisions; aggregation occurs afterward.\nFailure mode: Inferring individual preferences from group-level data.\nCorrect conclusion:\nThe claim that wealthy individuals support Party A is INVALID.\nWise refusal:\nIndividual-level data is required to infer individual voting behavior; state averages are insufficient.",
        "Hidden Timestamp": "Is State-level average income measured at an aggregate level while Voting outcome (Party A vs. Party B) is an individual claim, and when/where does aggregation into Individual income level happen relative to measuring Voting outcome (Party A vs. Party B)?",
        "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how State-level average income relates to Voting outcome (Party A vs. Party B) for a person."
    },
    {
        "id": "T3-BucketJ-08",
        "bucket": "BucketLarge-J",
        "title": "Neighborhood Education and Student Achievement",
        "scenario": "A city education report shows that neighborhoods with higher average adult education levels have higher student test scores. Policymakers infer that students living in highly educated neighborhoods perform better academically.\nHowever, individual-level data reveals that students from less-educated households can perform as well as their peers when controlling for school quality and family support, regardless of neighborhood averages.\nThe neighborhood-level correlation reflects broader structural differences rather than individual household effects.",
        "variables": [
            "X = Neighborhood average education level",
            "Y = Student test scores",
            "Z = Household-level educational support"
        ],
        "annotations": {
            "Case ID": "J2-08",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Socioeconomic",
            "Difficulty": "Easy",
            "Subdomain": "Education Sociology",
            "Causal Structure": "Neighborhood averages mask variation across households; individual outcomes depend on household-level factors.",
            "Key Insight": "Group characteristics do not deterministically apply to individuals within the group.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can neighborhood education averages be used to predict individual student performance?\nWhy does this inference risk ecological fallacy?",
        "expected_analysis": "This case requires associational reasoning and recognition of the ecological fallacy.\nKey reasoning step: Neighborhood-level statistics aggregate diverse households.\nHidden temporal structure: Household characteristics precede both neighborhood averages and student outcomes.\nFailure mode: Assuming individuals inherit group-level attributes.\nCorrect conclusion:\nThe inference about individual performance is INVALID.\nWise refusal:\nIndividual-level causal claims require individual-level data.",
        "Hidden Timestamp": "Is Neighborhood average education level measured at an aggregate level while Student test scores is an individual claim, and when/where does aggregation into Household-level educational support happen relative to measuring Student test scores?",
        "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Neighborhood average education level relates to Student test scores for a person."
    },
    {
        "id": "T3-BucketJ-09",
        "bucket": "BucketLarge-J",
        "title": "National Happiness and Personal Well-Being",
        "scenario": "International surveys show that Country A ranks higher than Country B on average happiness scores. Commentators argue that citizens of Country A are happier in their daily lives.\nHowever, within both countries, individual happiness varies widely, and individual-level analysis shows that factors such as income security, health, and social relationships are stronger predictors of personal well-being than nationality.\nThe national ranking reflects aggregated survey responses rather than uniform individual experiences.",
        "variables": [
            "X = Country of residence",
            "Y = Average happiness score",
            "Z = Individual-level well-being determinants"
        ],
        "annotations": {
            "Case ID": "J2-09",
            "Pearl Level": "L1 (Association)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Socioeconomic",
            "Difficulty": "Easy",
            "Subdomain": "Sociology",
            "Causal Structure": "National averages collapse heterogeneous individual experiences into a single metric.",
            "Key Insight": "Country-level statistics cannot substitute for individual-level causal explanations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does a higher national happiness score imply that every individual is happier?\nWhy is this reasoning flawed?",
        "expected_analysis": "This case requires associational reasoning and identification of the ecological fallacy.\nKey reasoning step: National happiness scores are averages, not individual guarantees.\nHidden temporal structure: Individual experiences determine survey responses; aggregation follows.\nFailure mode: Treating aggregate indicators as individual truths.\nCorrect conclusion:\nThe claim that individuals in Country A are happier is INVALID.\nWise refusal:\nIndividual well-being must be assessed at the individual level, not inferred from national averages.",
        "Hidden Timestamp": "Is Country of residence measured at an aggregate level while Average happiness score is an individual claim, and when/where does aggregation into Individual-level well-being determinants happen relative to measuring Average happiness score?",
        "Conditional Answers": "Answer if you only have aggregate correlations: You cannot infer individual-level behavior; the relationship may be confounded by group-level factors.\nAnswer if you have individual-level data within groups: Estimate the within-group association/effect and check whether it matches the aggregate pattern.\nAnswer if there is sorting/selection into groups: Treat conclusions as CONDITIONAL unless you model the sorting mechanism or use a design that breaks it.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The evidence is at an aggregate level, but the conclusion is about individuals. I would need individual-level data (or a defensible model of sorting into groups) before inferring how Country of residence relates to Average happiness score for a person."
    },
    {
        "id": "T3-BucketJ-10",
        "bucket": "BucketLarge-J",
        "title": "The New Medical Treatment Rollout",
        "scenario": "A hospital system compares two treatments for a chronic condition: Treatment A (new) and Treatment B (standard). Hospital leadership reports that Treatment A has a higher overall recovery rate and decides to adopt it system-wide.\nHowever, when outcomes are analyzed separately for mild cases and severe cases, Treatment B has a higher recovery rate in both severity groups.\nThe apparent superiority of Treatment A arises because it is used far more often for mild cases, while Treatment B is disproportionately used for severe cases.",
        "variables": [
            "X = Treatment type (A vs. B)",
            "Y = Recovery outcome",
            "Z = Disease severity (mild / severe)"
        ],
        "annotations": {
            "Case ID": "J2-10",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Health Policy",
            "Causal Structure": "Disease severity (Z) influences both treatment assignment (X) and recovery (Y). Aggregating outcomes across severity levels reverses the treatment comparison.",
            "Key Insight": "An intervention may appear effective overall while being inferior within every relevant subgroup.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "If the hospital switches all patients to Treatment A, should it expect higher recovery rates?\nWhy does stratifying by disease severity reverse the apparent treatment effect?",
        "expected_analysis": "This case requires interventional reasoning and identification of Simpson\u2019s Paradox under intervention.\nKey reasoning step: Disease severity (Z) is a pre-treatment variable that affects both treatment choice and outcomes.\nIntervention framing: The question concerns the effect of doing X = Treatment A for all patients.\nSubgroup analysis: Within both mild and severe cases, Treatment B yields better recovery outcomes.\nFailure mode: Inferring that an intervention is beneficial based on aggregate observational success rates.\nCorrect conclusion:\nAdopting Treatment A system-wide is INVALID based on the given evidence.\nWise refusal:\nA valid policy decision would require randomized assignment or severity-adjusted comparisons before intervention.",
        "Hidden Timestamp": "Was Disease severity (mild / severe) determined before Treatment type (A vs. B) was chosen, and could Disease severity (mild / severe) have influenced the choice of Treatment type (A vs. B) before Recovery outcome was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Treatment type (A vs. B) on Recovery outcome may be reversed because the mix of subgroups differs between Treatment type (A vs. B) arms.\nAnswer if you compare within strata after stratifying/standardizing by Disease severity (mild / severe): Use the within-stratum differences (or a standardized effect). If Treatment type (A vs. B) improves Recovery outcome in each stratum, prefer Treatment type (A vs. B) even if the aggregate looks worse.\nAnswer if Treatment type (A vs. B) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Recovery outcome by the key strata (e.g., Disease severity (mild / severe) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Treatment type (A vs. B) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-11",
        "bucket": "BucketLarge-J",
        "title": "The Class Size Reduction Program",
        "scenario": "A school district pilots a class size reduction program in several schools and reports that schools with smaller classes have higher average test scores. Based on these results, district officials propose expanding the program to all schools.\nHowever, when test scores are analyzed separately for high-performing schools and low-performing schools, schools without class size reductions outperform those with reductions in both categories.\nThis discrepancy arises because class size reductions were primarily implemented in already high-performing schools, while struggling schools retained larger classes.",
        "variables": [
            "X = Class size intervention (reduced vs. standard)",
            "Y = Student test scores",
            "Z = Baseline school performance (high / low)"
        ],
        "annotations": {
            "Case ID": "J2-11",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Education Policy",
            "Causal Structure": "Baseline school performance (Z) affects both likelihood of receiving the intervention (X) and student outcomes (Y), leading to aggregate reversal.",
            "Key Insight": "An intervention\u2019s apparent success may reflect where it was implemented rather than its causal effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Would expanding class size reduction to all schools likely improve test scores?\nWhy does conditioning on baseline school performance change the interpretation?",
        "expected_analysis": "This case requires interventional reasoning and recognition of Simpson\u2019s Paradox.\nKey reasoning step: Baseline performance (Z) is determined before intervention and strongly influences outcomes.\nIntervention framing: The policy question is about the effect of doing X = reducing class sizes everywhere.\nSubgroup analysis: Within both high- and low-performing schools, non-reduced classes perform better.\nFailure mode: Mistaking correlation between intervention presence and outcomes for causal effect.\nCorrect conclusion:\nThe proposal to expand the program is INVALID based on current evidence.\nWise refusal:\nRandomized rollout or matched comparisons are needed to estimate the true effect of class size reduction.",
        "Hidden Timestamp": "Was Baseline school performance (high / low) determined before Class size intervention (reduced vs. standard) was chosen, and could Baseline school performance (high / low) have influenced the choice of Class size intervention (reduced vs. standard) before Student test scores was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Class size intervention (reduced vs. standard) on Student test scores may be reversed because the mix of subgroups differs between Class size intervention (reduced vs. standard) arms.\nAnswer if you compare within strata after stratifying/standardizing by Baseline school performance (high / low): Use the within-stratum differences (or a standardized effect). If Class size intervention (reduced vs. standard) improves Student test scores in each stratum, prefer Class size intervention (reduced vs. standard) even if the aggregate looks worse.\nAnswer if Class size intervention (reduced vs. standard) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Student test scores by the key strata (e.g., Baseline school performance (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Class size intervention (reduced vs. standard) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-12",
        "bucket": "BucketLarge-J",
        "title": "The Productivity Training Initiative",
        "scenario": "A corporation pilots a productivity training program across several departments. Management reports that employees who received the training show higher average productivity scores than those who did not, and proposes mandatory rollout.\nWhen productivity is analyzed separately for junior and senior employees, however, untrained employees outperform trained employees in both groups.\nThe discrepancy arises because the training was offered primarily to senior employees, who are more productive on average regardless of training.",
        "variables": [
            "X = Training participation (yes / no)",
            "Y = Productivity score",
            "Z = Employee seniority (junior / senior)"
        ],
        "annotations": {
            "Case ID": "J2-12",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Seniority (Z) affects both training participation (X) and productivity (Y), reversing subgroup trends when aggregated.",
            "Key Insight": "Aggregate productivity gains may reflect employee mix rather than training impact.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Should the company mandate the training program for all employees?\nWhy does separating employees by seniority reverse the apparent effect?",
        "expected_analysis": "Seniority precedes training and productivity.\nWithin both junior and senior groups, training is associated with lower productivity.\nAggregate improvement is driven by overrepresentation of senior employees in the trained group.\nConclusion: Mandatory rollout is INVALID based on current evidence.\nWise refusal: A randomized or staggered rollout is required to estimate causal impact.",
        "Hidden Timestamp": "Was Employee seniority (junior / senior) determined before Training participation (yes / no) was chosen, and could Employee seniority (junior / senior) have influenced the choice of Training participation (yes / no) before Productivity score was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Training participation (yes / no) on Productivity score may be reversed because the mix of subgroups differs between Training participation (yes / no) arms.\nAnswer if you compare within strata after stratifying/standardizing by Employee seniority (junior / senior): Use the within-stratum differences (or a standardized effect). If Training participation (yes / no) improves Productivity score in each stratum, prefer Training participation (yes / no) even if the aggregate looks worse.\nAnswer if Training participation (yes / no) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Productivity score by the key strata (e.g., Employee seniority (junior / senior) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Training participation (yes / no) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-13",
        "bucket": "BucketLarge-J",
        "title": "The Digital Policing Tool",
        "scenario": "A police department introduces a predictive analytics tool and observes that precincts using the tool report higher crime resolution rates. City leaders plan to deploy the tool across all precincts.\nHowever, when outcomes are examined separately for high-crime and low-crime precincts, precincts without the tool show higher resolution rates in both categories.\nThe tool was initially deployed in low-crime precincts where resolution rates are naturally higher.",
        "variables": [
            "X = Predictive tool deployment",
            "Y = Crime resolution rate",
            "Z = Baseline precinct crime rate (high / low)"
        ],
        "annotations": {
            "Case ID": "J2-13",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Criminology",
            "Causal Structure": "Baseline crime rate (Z) affects both deployment (X) and outcomes (Y), leading to misleading aggregates.",
            "Key Insight": "Apparent intervention success may reflect selective deployment.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Will deploying the tool citywide improve crime resolution?\nWhy does conditioning on baseline crime rate change the conclusion?",
        "expected_analysis": "Baseline crime rate is pre-intervention.\nWithin both high- and low-crime precincts, non-tool precincts perform better.\nAggregate benefit reflects deployment pattern, not tool efficacy.\nConclusion: Citywide deployment is INVALID.\nWise refusal: Proper evaluation requires randomized precinct assignment.",
        "Hidden Timestamp": "Was Baseline precinct crime rate (high / low) determined before Predictive tool deployment was chosen, and could Baseline precinct crime rate (high / low) have influenced the choice of Predictive tool deployment before Crime resolution rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Predictive tool deployment on Crime resolution rate may be reversed because the mix of subgroups differs between Predictive tool deployment arms.\nAnswer if you compare within strata after stratifying/standardizing by Baseline precinct crime rate (high / low): Use the within-stratum differences (or a standardized effect). If Predictive tool deployment improves Crime resolution rate in each stratum, prefer Predictive tool deployment even if the aggregate looks worse.\nAnswer if Predictive tool deployment can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Crime resolution rate by the key strata (e.g., Baseline precinct crime rate (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Predictive tool deployment is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-14",
        "bucket": "BucketLarge-J",
        "title": "The Marketing Campaign Conversion Rates",
        "scenario": "A company runs a new marketing campaign and reports that customers exposed to it have a higher overall conversion rate. The campaign is labeled a success and expanded nationally.\nWhen analyzed by customer purchasing power (high vs. low), however, customers not exposed to the campaign convert at higher rates in both segments.\nThe campaign was targeted primarily at high-spending customers, inflating aggregate performance.",
        "variables": [
            "X = Campaign exposure",
            "Y = Purchase conversion",
            "Z = Customer purchasing power (high / low)"
        ],
        "annotations": {
            "Case ID": "J2-14",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Marketing Analytics",
            "Causal Structure": "Purchasing power (Z) influences both exposure (X) and conversion (Y), reversing subgroup effects.",
            "Key Insight": "Targeted interventions can distort aggregate success metrics.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the campaign cause higher conversion rates?\nHow does targeting affect causal interpretation?",
        "expected_analysis": "Purchasing power precedes exposure and conversion.\nWithin each purchasing group, the campaign underperforms.\nAggregate success is driven by customer mix.\nConclusion: Expansion decision is INVALID.\nWise refusal: A/B testing across comparable customers is required.",
        "Hidden Timestamp": "Was Customer purchasing power (high / low) determined before Campaign exposure was chosen, and could Customer purchasing power (high / low) have influenced the choice of Campaign exposure before Purchase conversion was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Campaign exposure on Purchase conversion may be reversed because the mix of subgroups differs between Campaign exposure arms.\nAnswer if you compare within strata after stratifying/standardizing by Customer purchasing power (high / low): Use the within-stratum differences (or a standardized effect). If Campaign exposure improves Purchase conversion in each stratum, prefer Campaign exposure even if the aggregate looks worse.\nAnswer if Campaign exposure can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Purchase conversion by the key strata (e.g., Customer purchasing power (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Campaign exposure is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-15",
        "bucket": "BucketLarge-J",
        "title": "The School Technology Grant",
        "scenario": "A government issues technology grants to schools and finds that grant-receiving schools show higher average student performance. Officials propose expanding funding.\nWhen performance is analyzed separately for urban and rural schools, non-recipient schools outperform recipients in both categories.\nThe grants were disproportionately awarded to urban schools, which already have stronger academic performance.",
        "variables": [
            "X = Grant receipt",
            "Y = Student performance",
            "Z = School location (urban / rural)"
        ],
        "annotations": {
            "Case ID": "J2-15",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Public Policy",
            "Causal Structure": "Location (Z) affects both grant allocation (X) and outcomes (Y), producing aggregate reversal.",
            "Key Insight": "Geographic imbalance can dominate apparent policy effects.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Should the grant program be expanded nationwide?\nWhy does stratifying by school location alter the conclusion?",
        "expected_analysis": "School location is fixed prior to grant allocation.\nWithin both urban and rural strata, non-recipient schools perform better.\nAggregate effect reflects allocation bias.\nConclusion: Expansion is INVALID.\nWise refusal: Causal evaluation requires randomized or needs-based allocation.\nL2-A (Simpson\u2019s Paradox under intervention)",
        "Hidden Timestamp": "Was School location (urban / rural) determined before Grant receipt was chosen, and could School location (urban / rural) have influenced the choice of Grant receipt before Student performance was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Grant receipt on Student performance may be reversed because the mix of subgroups differs between Grant receipt arms.\nAnswer if you compare within strata after stratifying/standardizing by School location (urban / rural): Use the within-stratum differences (or a standardized effect). If Grant receipt improves Student performance in each stratum, prefer Grant receipt even if the aggregate looks worse.\nAnswer if Grant receipt can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Student performance by the key strata (e.g., School location (urban / rural) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Grant receipt is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-16",
        "bucket": "BucketLarge-J",
        "title": "The Hospital Staffing Reform",
        "scenario": "A hospital increases nurse-to-patient ratios in select wards and reports that wards with higher staffing levels have lower overall mortality rates. Administrators propose expanding the staffing reform hospital-wide.\nHowever, when mortality is examined separately for high-risk and low-risk patients, wards without the staffing increase show lower mortality in both groups.\nThe staffing reform was initially implemented in wards that treated a larger proportion of low-risk patients.",
        "variables": [
            "X = Staffing reform (higher vs. standard staffing)",
            "Y = Patient mortality",
            "Z = Patient risk level (high / low)"
        ],
        "annotations": {
            "Case ID": "J2-16",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Hard",
            "Subdomain": "Health Policy",
            "Causal Structure": "Patient risk (Z) affects both staffing assignment (X) and mortality (Y), reversing subgroup-level effects when aggregated.",
            "Key Insight": "Apparent benefits of an intervention may be driven by patient composition rather than causal impact.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Would expanding the staffing reform reduce mortality hospital-wide?\nWhy does stratifying by patient risk reverse the conclusion?",
        "expected_analysis": "Patient risk is determined prior to staffing decisions.\nWithin both risk strata, standard-staffed wards perform better.\nAggregate benefit reflects selective placement.\nConclusion: Expansion is INVALID.\nWise refusal: Randomized ward assignment or risk-adjusted analysis is required.",
        "Hidden Timestamp": "Was Patient risk level (high / low) determined before Staffing reform (higher vs. standard staffing) was chosen, and could Patient risk level (high / low) have influenced the choice of Staffing reform (higher vs. standard staffing) before Patient mortality was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Staffing reform (higher vs. standard staffing) on Patient mortality may be reversed because the mix of subgroups differs between Staffing reform (higher vs. standard staffing) arms.\nAnswer if you compare within strata after stratifying/standardizing by Patient risk level (high / low): Use the within-stratum differences (or a standardized effect). If Staffing reform (higher vs. standard staffing) improves Patient mortality in each stratum, prefer Staffing reform (higher vs. standard staffing) even if the aggregate looks worse.\nAnswer if Staffing reform (higher vs. standard staffing) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Patient mortality by the key strata (e.g., Patient risk level (high / low) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Staffing reform (higher vs. standard staffing) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-17",
        "bucket": "BucketLarge-J",
        "title": "The University Tutoring Program",
        "scenario": "A university offers an optional tutoring program and finds that participants have higher overall course pass rates. Administrators consider making tutoring mandatory.\nWhen outcomes are analyzed separately for introductory and advanced courses, non-participants outperform participants in both categories.\nTutoring was most commonly used by students enrolled in advanced courses, which already have higher pass rates.",
        "variables": [
            "X = Tutoring participation",
            "Y = Course pass rate",
            "Z = Course level (introductory / advanced)"
        ],
        "annotations": {
            "Case ID": "J2-17",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Higher Education",
            "Causal Structure": "Course level (Z) influences both tutoring use (X) and pass rates (Y), producing aggregate reversal.",
            "Key Insight": "Participation patterns can dominate aggregate intervention outcomes.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Should tutoring be made mandatory for all students?\nHow does course level affect interpretation?",
        "expected_analysis": "Course level is fixed prior to tutoring.\nWithin both course strata, tutoring underperforms.\nAggregate success reflects enrollment mix.\nConclusion: Mandatory tutoring is INVALID.\nWise refusal: Causal effect requires random assignment within courses.",
        "Hidden Timestamp": "Was Course level (introductory / advanced) determined before Tutoring participation was chosen, and could Course level (introductory / advanced) have influenced the choice of Tutoring participation before Course pass rate was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Tutoring participation on Course pass rate may be reversed because the mix of subgroups differs between Tutoring participation arms.\nAnswer if you compare within strata after stratifying/standardizing by Course level (introductory / advanced): Use the within-stratum differences (or a standardized effect). If Tutoring participation improves Course pass rate in each stratum, prefer Tutoring participation even if the aggregate looks worse.\nAnswer if Tutoring participation can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Course pass rate by the key strata (e.g., Course level (introductory / advanced) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Tutoring participation is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-18",
        "bucket": "BucketLarge-J",
        "title": "The Remote Work Policy",
        "scenario": "A firm allows employees to opt into remote work and reports that remote workers have higher average productivity. Leadership considers mandating remote work for all eligible roles.\nHowever, when productivity is examined separately for technical and non-technical roles, in-office employees outperform remote employees in both categories.\nRemote work was disproportionately adopted by technical staff, who are more productive on average.",
        "variables": [
            "X = Work arrangement (remote / in-office)",
            "Y = Productivity",
            "Z = Job role type (technical / non-technical)"
        ],
        "annotations": {
            "Case ID": "J2-18",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Simpson\u2019s Paradox",
            "Trap Subtype": "Stratified Intervention Reversal",
            "Difficulty": "Medium",
            "Subdomain": "Workplace Policy",
            "Causal Structure": "Job role (Z) affects both remote eligibility (X) and productivity (Y), reversing subgroup effects when aggregated.",
            "Key Insight": "Apparent productivity gains may reflect workforce composition.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Would mandating remote work increase productivity?\nWhy does role stratification change the conclusion?",
        "expected_analysis": "Role type precedes work arrangement.\nWithin both role categories, in-office work performs better.\nAggregate effect is misleading.\nConclusion: Mandate is INVALID.\nWise refusal: Policy evaluation requires role-stratified experimentation.\n\u2705 L2-A COMPLETE\nL2-A cases completed: 9 / 9\nSimpson\u2019s under intervention fully covered\nL2-B (Composition Effects)\n(Population changes, not treatment effects)",
        "Hidden Timestamp": "Was Job role type (technical / non-technical) determined before Work arrangement (remote / in-office) was chosen, and could Job role type (technical / non-technical) have influenced the choice of Work arrangement (remote / in-office) before Productivity was measured?",
        "Conditional Answers": "Answer if you only compare aggregates: The apparent effect of Work arrangement (remote / in-office) on Productivity may be reversed because the mix of subgroups differs between Work arrangement (remote / in-office) arms.\nAnswer if you compare within strata after stratifying/standardizing by Job role type (technical / non-technical): Use the within-stratum differences (or a standardized effect). If Work arrangement (remote / in-office) improves Productivity in each stratum, prefer Work arrangement (remote / in-office) even if the aggregate looks worse.\nAnswer if Work arrangement (remote / in-office) can be randomized within strata: Then the within-stratum comparison identifies the causal effect; report both stratum-specific and standardized estimates.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Please report Productivity by the key strata (e.g., Job role type (technical / non-technical) if that is the stratifier), and compute a standardized effect; the aggregate comparison can reverse when Work arrangement (remote / in-office) is unevenly applied across strata."
    },
    {
        "id": "T3-BucketJ-19",
        "bucket": "BucketLarge-J",
        "title": "The Gentrification Income Report",
        "scenario": "A city reports that after a neighborhood redevelopment project, average household income in the area increased by 40%. Officials conclude that redevelopment improved residents\u2019 economic well-being.\nFurther analysis shows that many original low-income residents moved out, while higher-income residents moved in.",
        "variables": [
            "X = Neighborhood redevelopment",
            "Y = Average household income",
            "Z = Resident population composition"
        ],
        "annotations": {
            "Case ID": "J2-19",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Easy",
            "Subdomain": "Urban Economics",
            "Causal Structure": "Redevelopment changes who lives in the neighborhood (Z), which alters average income (Y) without improving original residents\u2019 outcomes.",
            "Key Insight": "Aggregate improvement can reflect population turnover, not individual benefit.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Did redevelopment make original residents wealthier?\nHow does population change affect income statistics?",
        "expected_analysis": "Redevelopment affects population composition.\nAverage income rises despite no improvement for original residents.\nConclusion: Claim of resident enrichment is INVALID.\nWise refusal: Individual-level longitudinal data is required.",
        "Hidden Timestamp": "Did the intervention/change in Neighborhood redevelopment alter the composition (Resident population composition) of who is counted before Average household income was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Average household income after changing Neighborhood redevelopment can reflect a real outcome shift.\nAnswer if Neighborhood redevelopment changes who is counted via Resident population composition: The aggregate Average household income can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Average household income may be moving because the denominator/population changed after Neighborhood redevelopment via composition variable Resident population composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-20",
        "bucket": "BucketLarge-J",
        "title": "The Crime Reduction Initiative",
        "scenario": "A city introduces a crime reduction initiative in a high-crime area. One year later, the neighborhood\u2019s crime rate drops significantly. Officials claim the initiative was successful.\nSubsequent analysis reveals that many high-risk residents relocated during the same period due to rising housing costs, while lower-risk residents moved in.",
        "variables": [
            "X = Crime reduction initiative",
            "Y = Neighborhood crime rate",
            "Z = Population turnover"
        ],
        "annotations": {
            "Case ID": "J2-20",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Criminology",
            "Causal Structure": "Population change (Z) drives crime reduction (Y), independent of the intervention.",
            "Key Insight": "Crime rates can fall due to who leaves, not what policies change.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can the crime reduction be attributed to the initiative?\nWhy does population displacement complicate causal inference?",
        "expected_analysis": "Initiative coincides with population change.\nReduced crime reflects altered risk pool.\nConclusion: Attribution to the initiative is INVALID.\nWise refusal: Evaluation requires tracking crime risk among original residents.\nL2-B (Composition Effects)",
        "Hidden Timestamp": "Did the intervention/change in Crime reduction initiative alter the composition (Population turnover) of who is counted before Neighborhood crime rate was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Neighborhood crime rate after changing Crime reduction initiative can reflect a real outcome shift.\nAnswer if Crime reduction initiative changes who is counted via Population turnover: The aggregate Neighborhood crime rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Neighborhood crime rate may be moving because the denominator/population changed after Crime reduction initiative via composition variable Population turnover. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-21",
        "bucket": "BucketLarge-J",
        "title": "The School Test Score Improvement",
        "scenario": "A school district reports that after implementing a new admissions lottery, the average test scores across district schools increased. Officials conclude that the lottery policy improved academic performance.\nFurther analysis shows that the lottery led to a redistribution of students: higher-performing students concentrated in certain schools, while lower-performing students were reassigned elsewhere.",
        "variables": [
            "X = Admissions lottery policy",
            "Y = Average test scores",
            "Z = Student distribution across schools"
        ],
        "annotations": {
            "Case ID": "J2-21",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "The lottery changes student composition (Z), altering school averages (Y) without changing individual achievement.",
            "Key Insight": "Improved averages can result from reshuffling students, not learning gains.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Did the lottery policy improve individual student performance?\nHow does redistribution affect average test scores?",
        "expected_analysis": "Policy affects student allocation, not instruction.\nAverage scores rise due to compositional changes.\nConclusion: Individual performance improvement claim is INVALID.\nWise refusal: Longitudinal student-level data is required.",
        "Hidden Timestamp": "Did the intervention/change in Admissions lottery policy alter the composition (Student distribution across schools) of who is counted before Average test scores was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Average test scores after changing Admissions lottery policy can reflect a real outcome shift.\nAnswer if Admissions lottery policy changes who is counted via Student distribution across schools: The aggregate Average test scores can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Average test scores may be moving because the denominator/population changed after Admissions lottery policy via composition variable Student distribution across schools. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-22",
        "bucket": "BucketLarge-J",
        "title": "The Workforce Diversity Metric",
        "scenario": "A corporation announces that after a diversity initiative, the percentage of women in leadership roles increased. Leadership attributes this to improved promotion practices.\nFurther inspection reveals that the increase is driven largely by hiring women directly into senior roles, while promotion rates within the firm remain unchanged.",
        "variables": [
            "X = Diversity initiative",
            "Y = Share of women in leadership",
            "Z = Entry vs. promotion composition"
        ],
        "annotations": {
            "Case ID": "J2-22",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Easy",
            "Subdomain": "Labor & Organizations",
            "Causal Structure": "Leadership composition (Y) changes due to hiring mix (Z), not internal advancement.",
            "Key Insight": "Stock metrics can change without flow changes.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Did the initiative improve promotion equity?\nWhy does leadership composition change without internal progress?",
        "expected_analysis": "Initiative changes who enters leadership.\nInternal dynamics remain unchanged.\nConclusion: Promotion improvement claim is INVALID.\nWise refusal: Promotion-rate analysis is required.",
        "Hidden Timestamp": "Did the intervention/change in Diversity initiative alter the composition (Entry vs. promotion composition) of who is counted before Share of women in leadership was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Share of women in leadership after changing Diversity initiative can reflect a real outcome shift.\nAnswer if Diversity initiative changes who is counted via Entry vs. promotion composition: The aggregate Share of women in leadership can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Share of women in leadership may be moving because the denominator/population changed after Diversity initiative via composition variable Entry vs. promotion composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-23",
        "bucket": "BucketLarge-J",
        "title": "The Hospital Readmission Decline",
        "scenario": "A hospital reports a decline in 30-day readmission rates after introducing a discharge planning program. Administrators credit the program with improving patient outcomes.\nLater analysis shows that more high-risk patients were transferred to long-term care facilities rather than discharged home during the same period.",
        "variables": [
            "X = Discharge planning program",
            "Y = Readmission rate",
            "Z = Discharged patient risk profile"
        ],
        "annotations": {
            "Case ID": "J2-23",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Healthcare Management",
            "Causal Structure": "Patient mix (Z) changes the denominator for readmissions (Y).",
            "Key Insight": "Outcome metrics can improve by excluding high-risk cases.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the program reduce readmissions for comparable patients?\nHow does discharge selection affect reported rates?",
        "expected_analysis": "High-risk patients are removed from measurement pool.\nReadmission rate declines mechanically.\nConclusion: Program effectiveness claim is INVALID.\nWise refusal: Risk-adjusted readmission analysis is required.",
        "Hidden Timestamp": "Did the intervention/change in Discharge planning program alter the composition (Discharged patient risk profile) of who is counted before Readmission rate was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Readmission rate after changing Discharge planning program can reflect a real outcome shift.\nAnswer if Discharge planning program changes who is counted via Discharged patient risk profile: The aggregate Readmission rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Readmission rate may be moving because the denominator/population changed after Discharge planning program via composition variable Discharged patient risk profile. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-24",
        "bucket": "BucketLarge-J",
        "title": "The Immigration Employment Statistic",
        "scenario": "A city reports that after an influx of immigrants, the unemployment rate declined. Officials claim immigration strengthened the local labor market.\nCloser inspection reveals that immigrants were more likely to be employed upon arrival, while some unemployed residents moved away due to rising rents.",
        "variables": [
            "X = Immigration influx",
            "Y = Unemployment rate",
            "Z = Labor force composition"
        ],
        "annotations": {
            "Case ID": "J2-24",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Employment statistics change due to who enters and exits the labor force.",
            "Key Insight": "Labor metrics are sensitive to population flows.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Did immigration create jobs for existing residents?\nWhy can unemployment fall without job creation?",
        "expected_analysis": "Employment rate reflects labor pool composition.\nDecline not attributable to job growth.\nConclusion: Job creation claim is INVALID.\nWise refusal: Separate employment effects by resident status.",
        "Hidden Timestamp": "Did the intervention/change in Immigration influx alter the composition (Labor force composition) of who is counted before Unemployment rate was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Unemployment rate after changing Immigration influx can reflect a real outcome shift.\nAnswer if Immigration influx changes who is counted via Labor force composition: The aggregate Unemployment rate can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Unemployment rate may be moving because the denominator/population changed after Immigration influx via composition variable Labor force composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-25",
        "bucket": "BucketLarge-J",
        "title": "The Public Transit Ridership Surge",
        "scenario": "After expanding a public transit line, a city reports a 30% increase in ridership and claims the expansion reduced car usage.\nFurther analysis shows that many riders were former bus users whose routes were discontinued, forcing them onto the new line.",
        "variables": [
            "X = Transit line expansion",
            "Y = Ridership counts",
            "Z = Mode substitution patterns"
        ],
        "annotations": {
            "Case ID": "J2-25",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Easy",
            "Subdomain": "Transportation Policy",
            "Causal Structure": "Ridership growth reflects reclassification of existing users.",
            "Key Insight": "Usage metrics can rise without behavior change.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Did the expansion reduce car travel?\nHow does reclassification inflate ridership numbers?",
        "expected_analysis": "Riders shift between transit categories.\nNo evidence of reduced car usage.\nConclusion: Car reduction claim is INVALID.\nWise refusal: Mode-shift analysis is required.",
        "Hidden Timestamp": "Did the intervention/change in Transit line expansion alter the composition (Mode substitution patterns) of who is counted before Ridership counts was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Ridership counts after changing Transit line expansion can reflect a real outcome shift.\nAnswer if Transit line expansion changes who is counted via Mode substitution patterns: The aggregate Ridership counts can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Ridership counts may be moving because the denominator/population changed after Transit line expansion via composition variable Mode substitution patterns. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-26",
        "bucket": "BucketLarge-J",
        "title": "The University Ranking Improvement",
        "scenario": "A university rises significantly in national rankings after launching a selective honors program. Administrators claim that the program improved overall academic quality.\nFurther analysis shows that the university admitted a smaller cohort of highly qualified honors students while reducing enrollment elsewhere, without changing instructional practices for existing students.",
        "variables": [
            "X = Honors program introduction",
            "Y = University ranking metrics",
            "Z = Student intake composition"
        ],
        "annotations": {
            "Case ID": "J2-26",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Higher Education Policy",
            "Causal Structure": "Rankings improve because the student body composition (Z) changes, not because educational quality improves.",
            "Key Insight": "Institutional metrics can improve through selective enrollment rather than better outcomes.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Did the honors program improve education for existing students?\nWhy can rankings rise without instructional change?",
        "expected_analysis": "Program changes who is admitted.\nRankings reflect input quality, not value added.\nConclusion: Educational improvement claim is INVALID.\nWise refusal: Value-added measures are required.",
        "Hidden Timestamp": "Did the intervention/change in Honors program introduction alter the composition (Student intake composition) of who is counted before University ranking metrics was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in University ranking metrics after changing Honors program introduction can reflect a real outcome shift.\nAnswer if Honors program introduction changes who is counted via Student intake composition: The aggregate University ranking metrics can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for University ranking metrics may be moving because the denominator/population changed after Honors program introduction via composition variable Student intake composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-27",
        "bucket": "BucketLarge-J",
        "title": "The Environmental Emissions Drop",
        "scenario": "A city reports a decline in average per-capita carbon emissions after adopting a climate action plan. Officials credit the plan with reducing emissions.\nFurther investigation reveals that several energy-intensive factories closed during the same period, relocating to neighboring regions.",
        "variables": [
            "X = Climate action plan",
            "Y = Per-capita carbon emissions",
            "Z = Industrial activity composition"
        ],
        "annotations": {
            "Case ID": "J2-27",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Environmental Policy",
            "Causal Structure": "Emissions fall because polluting activity exits the measurement region.",
            "Key Insight": "Environmental metrics can improve via relocation, not reduction.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Did the policy reduce emissions behavior?\nWhy does displacement complicate attribution?",
        "expected_analysis": "Emissions decline reflects industrial exit.\nNo evidence of cleaner production.\nConclusion: Policy effectiveness claim is INVALID.\nWise refusal: Consumption-based emissions accounting is required.\n\u2705 L2-B COMPLETE\nL2-B cases: 9 / 9\nL2-C (Selection into Treatment)\n(People who receive the intervention differ systematically)",
        "Hidden Timestamp": "Did the intervention/change in Climate action plan alter the composition (Industrial activity composition) of who is counted before Per-capita carbon emissions was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Per-capita carbon emissions after changing Climate action plan can reflect a real outcome shift.\nAnswer if Climate action plan changes who is counted via Industrial activity composition: The aggregate Per-capita carbon emissions can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Per-capita carbon emissions may be moving because the denominator/population changed after Climate action plan via composition variable Industrial activity composition. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-28",
        "bucket": "BucketLarge-J",
        "title": "The Job Training Program",
        "scenario": "A government reports that participants in a job training program have higher post-program employment rates than non-participants. Officials conclude the program is effective.\nHowever, enrollment in the program is voluntary, and participants are more motivated and actively job-seeking than non-participants even before enrollment.",
        "variables": [
            "X = Program participation",
            "Y = Employment outcome",
            "Z = Job-seeking motivation"
        ],
        "annotations": {
            "Case ID": "J2-28",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Labor Policy",
            "Causal Structure": "Motivation (Z) affects both participation (X) and employment (Y).",
            "Key Insight": "Participants would have better outcomes regardless of treatment.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does higher employment among participants imply program effectiveness?\nWhy does voluntary enrollment bias inference?",
        "expected_analysis": "Motivation precedes treatment.\nEmployment differences reflect selection.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Random assignment is required.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Employment outcome occurred\u2014and is selection related to Job-seeking motivation or Employment outcome?",
        "Conditional Answers": "Answer if Program participation is randomly assigned: A difference in Employment outcome across Program participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Job-seeking motivation): The Program participation vs not-Program participation difference in Employment outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Job-seeking motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Job-seeking motivation); otherwise Program participation\u2013Employment outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-29",
        "bucket": "BucketLarge-J",
        "title": "The Preventive Health Screening",
        "scenario": "Patients who undergo preventive health screenings have lower mortality rates than those who do not. Health officials promote screenings as life-saving.\nHowever, individuals who choose screenings tend to be healthier, wealthier, and more health-conscious than those who decline.",
        "variables": [
            "X = Screening participation",
            "Y = Mortality",
            "Z = Baseline health behavior"
        ],
        "annotations": {
            "Case ID": "J2-29",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Medium",
            "Subdomain": "Public Health",
            "Causal Structure": "Health behavior (Z) influences both screening (X) and mortality (Y).",
            "Key Insight": "Observed benefit may reflect who chooses screening.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does screening causally reduce mortality?\nWhy are screened and unscreened populations incomparable?",
        "expected_analysis": "Health behavior precedes screening.\nMortality differences are confounded.\nConclusion: Causal claim is INVALID.\nWise refusal: Controlled trials or instrumental variables are needed.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Mortality occurred\u2014and is selection related to Baseline health behavior or Mortality?",
        "Conditional Answers": "Answer if Screening participation is randomly assigned: A difference in Mortality across Screening participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Baseline health behavior): The Screening participation vs not-Screening participation difference in Mortality is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Baseline health behavior) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Baseline health behavior); otherwise Screening participation\u2013Mortality differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-30",
        "bucket": "BucketLarge-J",
        "title": "The Leadership Development Program",
        "scenario": "Employees who attend a leadership development program are promoted at higher rates than those who do not. Management credits the program with improving leadership skills.\nHowever, attendance is limited to employees already identified as high-potential by senior managers.",
        "variables": [
            "X = Program attendance",
            "Y = Promotion outcome",
            "Z = Pre-existing leadership potential"
        ],
        "annotations": {
            "Case ID": "J2-30",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Leadership potential (Z) affects both selection into the program (X) and promotion (Y).",
            "Key Insight": "Programs targeting high performers inflate apparent effectiveness.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the program cause higher promotion rates?\nHow does pre-selection bias evaluation?",
        "expected_analysis": "Selection precedes treatment.\nPromotions reflect prior assessments.\nConclusion: Program impact claim is INVALID.\nWise refusal: Compare with matched non-selected employees.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Promotion outcome occurred\u2014and is selection related to Pre-existing leadership potential or Promotion outcome?",
        "Conditional Answers": "Answer if Program attendance is randomly assigned: A difference in Promotion outcome across Program attendance groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Pre-existing leadership potential): The Program attendance vs not-Program attendance difference in Promotion outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Pre-existing leadership potential) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Pre-existing leadership potential); otherwise Program attendance\u2013Promotion outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-31",
        "bucket": "BucketLarge-J",
        "title": "The College Scholarship Program",
        "scenario": "Students who receive a merit-based scholarship graduate at higher rates than those who do not. University administrators conclude that the scholarship improves student success.\nHowever, scholarship recipients are selected based on prior academic achievement, strong recommendations, and demonstrated motivation.",
        "variables": [
            "X = Scholarship receipt",
            "Y = Graduation outcome",
            "Z = Prior academic achievement and motivation"
        ],
        "annotations": {
            "Case ID": "J2-31",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Time-varying Confounding",
            "Difficulty": "Easy",
            "Subdomain": "Higher Education",
            "Causal Structure": "Prior achievement (Z) affects both scholarship receipt (X) and graduation (Y).",
            "Key Insight": "Scholarships may select strong students rather than create success.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does receiving the scholarship cause higher graduation rates?\nWhy does merit-based selection bias the comparison?",
        "expected_analysis": "Academic strength precedes scholarship.\nGraduation differences reflect selection.\nConclusion: Causal claim is INVALID.\nWise refusal: Randomized or need-based assignment is required.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Graduation outcome occurred\u2014and is selection related to Prior academic achievement and motivation or Graduation outcome?",
        "Conditional Answers": "Answer if Scholarship receipt is randomly assigned: A difference in Graduation outcome across Scholarship receipt groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Prior academic achievement and motivation): The Scholarship receipt vs not-Scholarship receipt difference in Graduation outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Prior academic achievement and motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Prior academic achievement and motivation); otherwise Scholarship receipt\u2013Graduation outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-32",
        "bucket": "BucketLarge-J",
        "title": "The Voluntary Online Learning Platform",
        "scenario": "Users who enroll in a voluntary online learning platform show greater skill improvement than non-users. Platform developers claim the platform is effective.\nHowever, enrollment is optional, and users are typically more self-motivated and already interested in skill development.",
        "variables": [
            "X = Platform enrollment",
            "Y = Skill improvement",
            "Z = Learner motivation"
        ],
        "annotations": {
            "Case ID": "J2-32",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Education Technology",
            "Causal Structure": "Motivation (Z) influences both enrollment (X) and learning outcomes (Y).",
            "Key Insight": "Voluntary participation inflates perceived effectiveness.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does platform usage cause skill gains?\nWhy are users and non-users incomparable?",
        "expected_analysis": "Motivation predates platform use.\nGains reflect selection bias.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Randomized access or encouragement design is needed.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Skill improvement occurred\u2014and is selection related to Learner motivation or Skill improvement?",
        "Conditional Answers": "Answer if Platform enrollment is randomly assigned: A difference in Skill improvement across Platform enrollment groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Learner motivation): The Platform enrollment vs not-Platform enrollment difference in Skill improvement is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Learner motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Learner motivation); otherwise Platform enrollment\u2013Skill improvement differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-33",
        "bucket": "BucketLarge-J",
        "title": "The Early Childhood Education Program",
        "scenario": "Children enrolled in a voluntary early childhood education program perform better academically later in life. Policymakers cite this as evidence of program success.\nHowever, parents who enroll their children tend to be more engaged, have higher educational attainment, and provide more academic support at home.",
        "variables": [
            "X = Program enrollment",
            "Y = Later academic performance",
            "Z = Parental engagement"
        ],
        "annotations": {
            "Case ID": "J2-33",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Parental engagement (Z) affects both enrollment (X) and child outcomes (Y).",
            "Key Insight": "Family background confounds program evaluation.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can better outcomes be attributed to the program itself?\nHow does parental choice bias evaluation?",
        "expected_analysis": "Engagement precedes enrollment.\nOutcomes reflect family inputs.\nConclusion: Program impact claim is INVALID.\nWise refusal: Randomized access or sibling comparisons are required.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Later academic performance occurred\u2014and is selection related to Parental engagement or Later academic performance?",
        "Conditional Answers": "Answer if Program enrollment is randomly assigned: A difference in Later academic performance across Program enrollment groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Parental engagement): The Program enrollment vs not-Program enrollment difference in Later academic performance is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Parental engagement) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Parental engagement); otherwise Program enrollment\u2013Later academic performance differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-34",
        "bucket": "BucketLarge-J",
        "title": "The Fitness App Effectiveness Claim",
        "scenario": "Users of a fitness tracking app lose more weight than non-users. The app is marketed as effective for weight loss.\nHowever, app users are typically more health-conscious and already motivated to exercise.",
        "variables": [
            "X = App usage",
            "Y = Weight loss",
            "Z = Health motivation"
        ],
        "annotations": {
            "Case ID": "J2-34",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Easy",
            "Subdomain": "Digital Health",
            "Causal Structure": "Motivation (Z) influences both app adoption (X) and outcomes (Y).",
            "Key Insight": "Technology uptake selects motivated users.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does app usage cause weight loss?\nWhy is comparing users and non-users misleading?",
        "expected_analysis": "Motivation predates app usage.\nWeight loss reflects user characteristics.\nConclusion: App effectiveness claim is INVALID.\nWise refusal: Randomized trials are needed.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Weight loss occurred\u2014and is selection related to Health motivation or Weight loss?",
        "Conditional Answers": "Answer if App usage is randomly assigned: A difference in Weight loss across App usage groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Health motivation): The App usage vs not-App usage difference in Weight loss is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Health motivation) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Health motivation); otherwise App usage\u2013Weight loss differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-35",
        "bucket": "BucketLarge-J",
        "title": "The Mentorship Program Outcomes",
        "scenario": "Employees who participate in a mentorship program receive higher performance ratings than non-participants. Managers argue that mentoring improves performance.\nHowever, mentors are assigned to employees already identified as high performers.",
        "variables": [
            "X = Mentorship participation",
            "Y = Performance rating",
            "Z = Prior performance level"
        ],
        "annotations": {
            "Case ID": "J2-35",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Unblocked Backdoor",
            "Difficulty": "Medium",
            "Subdomain": "Human Resources",
            "Causal Structure": "Prior performance (Z) influences mentorship assignment (X) and ratings (Y).",
            "Key Insight": "Programs targeting strong performers exaggerate impact.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does mentorship cause higher performance?\nHow does selection bias affect evaluation?",
        "expected_analysis": "Performance precedes mentoring.\nRatings reflect prior ability.\nConclusion: Mentorship impact claim is INVALID.\nWise refusal: Compare with matched non-selected employees.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Performance rating occurred\u2014and is selection related to Prior performance level or Performance rating?",
        "Conditional Answers": "Answer if Mentorship participation is randomly assigned: A difference in Performance rating across Mentorship participation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Prior performance level): The Mentorship participation vs not-Mentorship participation difference in Performance rating is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Prior performance level) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Prior performance level); otherwise Mentorship participation\u2013Performance rating differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-36",
        "bucket": "BucketLarge-J",
        "title": "The Therapy Program Success Rate",
        "scenario": "Patients who complete a voluntary therapy program show better mental health outcomes than those who drop out or never enroll. Providers claim the therapy is effective.\nHowever, patients who complete therapy are those who respond early or have fewer barriers to participation.",
        "variables": [
            "X = Therapy completion",
            "Y = Mental health outcome",
            "Z = Treatment adherence capacity"
        ],
        "annotations": {
            "Case ID": "J2-36",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Hard",
            "Subdomain": "Mental Health",
            "Causal Structure": "Adherence capacity (Z) influences both completion (X) and outcomes (Y).",
            "Key Insight": "Conditioning on completion induces selection bias.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does completing therapy cause better outcomes?\nWhy is completion a biased conditioning variable?",
        "expected_analysis": "Adherence differences precede outcomes.\nCompletion-based comparison is biased.\nConclusion: Effectiveness claim is INVALID.\nWise refusal: Intention-to-treat analysis is required.\nL2-D (Collider Bias) \u2014 COMPLETE (9 cases)",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Mental health outcome occurred\u2014and is selection related to Treatment adherence capacity or Mental health outcome?",
        "Conditional Answers": "Answer if Therapy completion is randomly assigned: A difference in Mental health outcome across Therapy completion groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Treatment adherence capacity): The Therapy completion vs not-Therapy completion difference in Mental health outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Treatment adherence capacity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Treatment adherence capacity); otherwise Therapy completion\u2013Mental health outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-37",
        "bucket": "BucketLarge-J",
        "title": "The Hospital Survival Paradox",
        "scenario": "A hospital studies patients admitted with a severe illness and finds that smokers have lower mortality rates than non-smokers among admitted patients. Administrators speculate that smoking may be protective.\nHowever, hospital admission occurs only for patients who become seriously ill. Smoking and other health conditions both increase the likelihood of severe illness and admission.",
        "variables": [
            "X = Smoking status",
            "Y = Mortality",
            "Z = Hospital admission (conditioning variable)"
        ],
        "annotations": {
            "Case ID": "J2-37",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "Smoking and mortality both influence hospital admission; conditioning on admission induces spurious correlation.",
            "Key Insight": "Conditioning on a collider can reverse associations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does smoking reduce mortality among hospitalized patients?\nWhy does conditioning on hospital admission distort the association?",
        "expected_analysis": "Admission is a collider influenced by smoking and health.\nConditioning induces negative correlation.\nConclusion: Protective effect claim is INVALID.\nWise refusal: Analyze population-level data without conditioning on admission.",
        "Hidden Timestamp": "Is the analysis conditioning on Hospital admission (conditioning variable) that is determined after upstream factors affecting both Smoking status and Mortality, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Hospital admission (conditioning variable)): Associations between Smoking status and Mortality can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Hospital admission (conditioning variable)), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-38",
        "bucket": "BucketLarge-J",
        "title": "The Obesity Survival Puzzle",
        "scenario": "Among patients with heart disease, overweight patients appear to have better survival rates than normal-weight patients. Some interpret this as evidence of an \u201cobesity paradox.\u201d\nHeart disease diagnosis depends on both underlying health risks and body weight, creating a selected population.",
        "variables": [
            "X = Body weight",
            "Y = Survival outcome",
            "Z = Heart disease diagnosis"
        ],
        "annotations": {
            "Case ID": "J2-38",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on disease status biases associations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Diagnosis is a collider.\nApparent benefit is spurious.\nConclusion: Causal claim is INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on Heart disease diagnosis that is determined after upstream factors affecting both Body weight and Survival outcome, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Heart disease diagnosis): Associations between Body weight and Survival outcome can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Heart disease diagnosis), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-39",
        "bucket": "BucketLarge-J",
        "title": "Elite College Admissions and Success",
        "scenario": "Among students admitted to elite colleges, students from less privileged backgrounds outperform wealthier peers academically. Commentators argue that disadvantage improves performance.\nAdmission depends on both background and academic potential.",
        "variables": [
            "X = Socioeconomic background",
            "Y = Academic performance",
            "Z = Elite college admission"
        ],
        "annotations": {
            "Case ID": "J2-39",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Education",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on selective admission distorts comparisons.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Admission is a collider.\nHigh-performing disadvantaged students are overrepresented.\nConclusion: Advantage of disadvantage claim is INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on Elite college admission that is determined after upstream factors affecting both Socioeconomic background and Academic performance, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Elite college admission): Associations between Socioeconomic background and Academic performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Elite college admission), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-40",
        "bucket": "BucketLarge-J",
        "title": "Promotion and Job Satisfaction",
        "scenario": "Among employees who are promoted, those with lower job satisfaction before promotion show higher post-promotion performance. Managers infer dissatisfaction drives improvement.\nPromotion depends on both performance and dissatisfaction signals.",
        "variables": [
            "X = Job satisfaction",
            "Y = Performance",
            "Z = Promotion status"
        ],
        "annotations": {
            "Case ID": "J2-40",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on selection outcomes creates false correlations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Promotion is a collider.\nNegative association is spurious.\nConclusion: Interpretation is INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on Promotion status that is determined after upstream factors affecting both Job satisfaction and Performance, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Promotion status): Associations between Job satisfaction and Performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Promotion status), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-41",
        "bucket": "BucketLarge-J",
        "title": "Startup Founder Traits",
        "scenario": "Among founders of successful startups, those without formal business training appear more innovative. Observers conclude training stifles creativity.\nStartup success depends on both training and innovation.",
        "variables": [
            "X = Business training",
            "Y = Innovation",
            "Z = Startup success"
        ],
        "annotations": {
            "Case ID": "J2-41",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Entrepreneurship",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on success induces tradeoff illusion.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Success is a collider.\nApparent inverse relationship is spurious.\nConclusion: Claim is INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on Startup success that is determined after upstream factors affecting both Business training and Innovation, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Startup success): Associations between Business training and Innovation can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Startup success), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-42",
        "bucket": "BucketLarge-J",
        "title": "Disaster Survivor Health",
        "scenario": "Among disaster survivors, individuals with chronic conditions appear more resilient. Analysts infer chronic illness builds resilience.\nSurvival depends on both health status and exposure.",
        "variables": [
            "X = Chronic illness",
            "Y = Resilience",
            "Z = Survival"
        ],
        "annotations": {
            "Case ID": "J2-42",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Public Health",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on survival distorts health associations.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Survival is a collider.\nResilience effect is spurious.\nConclusion: Claim is INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on Survival that is determined after upstream factors affecting both Chronic illness and Resilience, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Survival): Associations between Chronic illness and Resilience can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Survival), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-43",
        "bucket": "BucketLarge-J",
        "title": "Academic Publication Bias",
        "scenario": "Among published papers, researchers without prestigious affiliations have higher citation counts. Some argue prestige harms impact.\nPublication depends on both institutional prestige and paper quality.",
        "variables": [
            "X = Institutional prestige",
            "Y = Citation impact",
            "Z = Publication"
        ],
        "annotations": {
            "Case ID": "J2-43",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Science of Science",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on publication induces spurious tradeoffs.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Publication is a collider.\nEffect is spurious.\nConclusion: Interpretation is INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on Publication that is determined after upstream factors affecting both Institutional prestige and Citation impact, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Publication): Associations between Institutional prestige and Citation impact can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Publication), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-44",
        "bucket": "BucketLarge-J",
        "title": "Hiring from Elite Firms",
        "scenario": "Among employees hired from elite firms, those with weaker resumes perform as well as strong candidates from non-elite firms. Managers infer elite firms overvalue credentials.\nHiring depends on both resume strength and firm pedigree.",
        "variables": [
            "X = Resume strength",
            "Y = Job performance",
            "Z = Hiring decision"
        ],
        "annotations": {
            "Case ID": "J2-44",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Labor & Hiring",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on hiring distorts performance signals.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Hiring is a collider.\nComparisons are biased.\nConclusion: Claim is INVALID.",
        "Hidden Timestamp": "Is the analysis conditioning on Hiring decision that is determined after upstream factors affecting both Resume strength and Job performance, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Hiring decision): Associations between Resume strength and Job performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Hiring decision), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-45",
        "bucket": "BucketLarge-J",
        "title": "The Athlete Selection Effect",
        "scenario": "Among professional athletes, those with poorer early training outperform peers later. Analysts argue early training is overrated.\nProfessional selection depends on both training quality and innate talent.",
        "variables": [
            "X = Early training quality",
            "Y = Later performance",
            "Z = Professional selection"
        ],
        "annotations": {
            "Case ID": "J2-45",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Collider",
            "Trap Subtype": "Conditioning on Compliance",
            "Difficulty": "Hard",
            "Subdomain": "Sports Analytics",
            "Causal Structure": "X -> Z <- Y; conditioning on Z induces spurious X <-> Y",
            "Key Insight": "Conditioning on elite selection induces spurious inversions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Selection is a collider.\nApparent reversal is spurious.\nConclusion: Claim is INVALID.\nL2-E (Base-Rate Distortion) \u2014 COMPLETE (9 cases)",
        "Hidden Timestamp": "Is the analysis conditioning on Professional selection that is determined after upstream factors affecting both Early training quality and Later performance, potentially inducing a spurious association?",
        "Conditional Answers": "Answer if you condition on a post-selection variable (conditioning on Professional selection): Associations between Early training quality and Later performance can be artifacts created by conditioning; do not interpret them causally.\nAnswer if you analyze the full eligible population without conditioning: The spurious association should weaken/disappear; this is the appropriate causal estimand.\nAnswer if conditioning is unavoidable (e.g., only selected data exist): Use an explicit selection model/causal graph and treat conclusions as CONDITIONAL with sensitivity checks.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. If the analysis conditions on a post-selection variable (conditioning on Professional selection), it can induce spurious correlations. I would avoid conditioning or require a causal graph/selection model before interpreting the association."
    },
    {
        "id": "T3-BucketJ-46",
        "bucket": "BucketLarge-J",
        "title": "Vaccine Hospitalization Reports",
        "scenario": "A news report states that most hospitalized COVID patients are vaccinated, leading commentators to claim that vaccines are ineffective.\nHowever, a large majority of the population is vaccinated, while only a small fraction is unvaccinated.",
        "variables": [
            "X = Vaccination status",
            "Y = Hospitalization",
            "Z = Population base rate of vaccination"
        ],
        "annotations": {
            "Case ID": "J2-46",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Public Health",
            "Causal Structure": "Hospitalization counts reflect population proportions rather than individual risk.",
            "Key Insight": "High counts do not imply high risk without denominators.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the hospitalization statistic imply vaccines are ineffective?\nWhy are base rates necessary for interpretation?",
        "expected_analysis": "Vaccinated individuals dominate the population.\nRisk per person is lower among vaccinated.\nConclusion: Ineffectiveness claim is INVALID.\nWise refusal: Compare hospitalization rates, not counts.",
        "Hidden Timestamp": "Were the case counts for Hospitalization measured over the same time window as the base rate/denominator Population base rate of vaccination, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Hospitalization: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Hospitalization with denominator/base rate Population base rate of vaccination: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Hospitalization are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Population base rate of vaccination (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-47",
        "bucket": "BucketLarge-J",
        "title": "Crime Arrest Statistics",
        "scenario": "Police data shows that most arrests are from Neighborhood A, prompting claims that residents of Neighborhood A commit more crimes.\nHowever, Neighborhood A has twice the population of other neighborhoods.",
        "variables": [
            "X = Neighborhood",
            "Y = Arrest count",
            "Z = Neighborhood population size"
        ],
        "annotations": {
            "Case ID": "J2-47",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Counts must be normalized by population.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Higher counts reflect larger population.\nConclusion: Crime propensity claim is INVALID.\nWise refusal: Use per-capita arrest rates.",
        "Hidden Timestamp": "Were the case counts for Arrest count measured over the same time window as the base rate/denominator Neighborhood population size, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Arrest count: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Arrest count with denominator/base rate Neighborhood population size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Arrest count are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Neighborhood population size (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-48",
        "bucket": "BucketLarge-J",
        "title": "Loan Default Claims",
        "scenario": "A bank reports that most loan defaults come from low-income borrowers, concluding they are riskier.\nHowever, most borrowers in the bank\u2019s portfolio are low-income.",
        "variables": [
            "X = Income group",
            "Y = Loan default",
            "Z = Borrower distribution"
        ],
        "annotations": {
            "Case ID": "J2-48",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Finance",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Default counts track borrower mix.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Risk requires conditional default rates.\nConclusion: Risk claim is INVALID.",
        "Hidden Timestamp": "Were the case counts for Loan default measured over the same time window as the base rate/denominator Borrower distribution, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Loan default: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Loan default with denominator/base rate Borrower distribution: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Loan default are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Borrower distribution (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-49",
        "bucket": "BucketLarge-J",
        "title": "Recidivism Rate Comparison",
        "scenario": "A report states that Group A accounts for more repeat offenses than Group B, suggesting harsher sentencing is needed for Group A.\nHowever, Group A represents a much larger share of the formerly incarcerated population.",
        "variables": [
            "X = Group membership",
            "Y = Recidivism",
            "Z = Released population size"
        ],
        "annotations": {
            "Case ID": "J2-49",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Exposure determines opportunity for reoffense.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Compare rates per released individual.\nConclusion: Sentencing inference is INVALID.",
        "Hidden Timestamp": "Were the case counts for Recidivism measured over the same time window as the base rate/denominator Released population size, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Recidivism: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Recidivism with denominator/base rate Released population size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Recidivism are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Released population size (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-50",
        "bucket": "BucketLarge-J",
        "title": "Workplace Accident Reports",
        "scenario": "A company reports that most workplace accidents occur in Factory X, concluding it is unsafe.\nFactory X employs far more workers than other sites.",
        "variables": [
            "X = Factory site",
            "Y = Accident count",
            "Z = Workforce size"
        ],
        "annotations": {
            "Case ID": "J2-50",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Occupational Safety",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Larger sites generate more incidents.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Normalize by hours worked.\nConclusion: Safety claim is INVALID.",
        "Hidden Timestamp": "Were the case counts for Accident count measured over the same time window as the base rate/denominator Workforce size, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Accident count: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Accident count with denominator/base rate Workforce size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Accident count are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Workforce size (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-51",
        "bucket": "BucketLarge-J",
        "title": "Academic Misconduct Cases",
        "scenario": "A university notes that most academic misconduct cases involve first-year students, implying they cheat more.\nHowever, first-year students comprise the largest enrollment cohort.",
        "variables": [
            "X = Academic year",
            "Y = Misconduct case",
            "Z = Enrollment size"
        ],
        "annotations": {
            "Case ID": "J2-51",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Education",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Case counts reflect cohort size.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Compare misconduct rates by cohort.\nConclusion: Cheating inference is INVALID.",
        "Hidden Timestamp": "Were the case counts for Misconduct case measured over the same time window as the base rate/denominator Enrollment size, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Misconduct case: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Misconduct case with denominator/base rate Enrollment size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Misconduct case are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Enrollment size (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-52",
        "bucket": "BucketLarge-J",
        "title": "Fraud Detection Alerts",
        "scenario": "An algorithm flags more fraudulent transactions among online purchases than in-store purchases, leading to claims that online shopping is riskier.\nHowever, online transactions vastly outnumber in-store ones.",
        "variables": [
            "X = Transaction type",
            "Y = Fraud alert",
            "Z = Transaction volume"
        ],
        "annotations": {
            "Case ID": "J2-52",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Finance & Compliance",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Alert counts track transaction volume.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Compare fraud rates, not alerts.\nConclusion: Risk claim is INVALID.",
        "Hidden Timestamp": "Were the case counts for Fraud alert measured over the same time window as the base rate/denominator Transaction volume, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Fraud alert: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Fraud alert with denominator/base rate Transaction volume: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Fraud alert are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Transaction volume (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-53",
        "bucket": "BucketLarge-J",
        "title": "School Discipline Disparities",
        "scenario": "A district reports that most suspensions involve students from School A, concluding discipline problems are worse there.\nSchool A enrolls substantially more students.",
        "variables": [
            "X = School",
            "Y = Suspension",
            "Z = Enrollment size"
        ],
        "annotations": {
            "Case ID": "J2-53",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Education Policy",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Discipline counts must be normalized.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Use suspension rates per student.\nConclusion: Discipline severity claim is INVALID.",
        "Hidden Timestamp": "Were the case counts for Suspension measured over the same time window as the base rate/denominator Enrollment size, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Suspension: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Suspension with denominator/base rate Enrollment size: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Suspension are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Enrollment size (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-54",
        "bucket": "BucketLarge-J",
        "title": "Medical Side-Effect Reports",
        "scenario": "A drug safety report shows that most side-effect reports involve Drug A, raising concerns about its safety.\nDrug A is prescribed far more frequently than alternatives.",
        "variables": [
            "X = Drug type",
            "Y = Side-effect report",
            "Z = Prescription frequency"
        ],
        "annotations": {
            "Case ID": "J2-54",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Base-rate Neglect",
            "Trap Subtype": "Prior Ignorance",
            "Difficulty": "Medium",
            "Subdomain": "Healthcare",
            "Causal Structure": "Group X changes exposure/denominator Z; comparing raw counts Y without normalizing by Z misleads",
            "Key Insight": "Report volume follows usage volume.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Compare side-effects per prescription.\nConclusion: Safety inference is INVALID.\nL2-F (Measurement / Denominator Mismatch) \u2014 COMPLETE (9 cases)",
        "Hidden Timestamp": "Were the case counts for Side-effect report measured over the same time window as the base rate/denominator Prescription frequency, and are we comparing rates rather than raw counts?",
        "Conditional Answers": "Answer if you only have raw counts of Side-effect report: You cannot infer risk or effectiveness because the base population sizes may differ.\nAnswer if you compute rates for Side-effect report with denominator/base rate Prescription frequency: Interpret the rate difference (risk per capita / per exposure) rather than the count difference.\nAnswer if the base rates change over time or differ by subgroup: Use time- and subgroup-specific denominators; otherwise the conclusion is UNDETERMINED.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. Raw counts of Side-effect report are not interpretable without exposure/population sizes. Please provide rates per capita/per exposure and the denominator/base rate Prescription frequency (and by subgroup/time) before concluding anything."
    },
    {
        "id": "T3-BucketJ-55",
        "bucket": "BucketLarge-J",
        "title": "The Class Size Paradox",
        "scenario": "A school district reports that students are in larger classes than ever before, even though the average class size per school has decreased. Officials argue that overcrowding is worsening.\nThe discrepancy arises because larger schools with many small classes enroll more students, while smaller schools with fewer large classes enroll fewer students.",
        "variables": [
            "X = School",
            "Y = Class size",
            "Z = Weighting scheme (student-weighted vs. class-weighted)"
        ],
        "annotations": {
            "Case ID": "J2-55",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education Statistics",
            "Causal Structure": "Different averaging schemes produce conflicting summaries without any underlying change.",
            "Key Insight": "\u201cAverage\u201d depends on what is being averaged.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Are students actually experiencing larger classes?\nWhy do the two averages disagree?",
        "expected_analysis": "Class-weighted and student-weighted averages differ.\nNo contradiction exists.\nConclusion: Overcrowding claim is CONDITIONAL.\nWise refusal: Specify the unit of analysis.",
        "Hidden Timestamp": "Did the intervention/change in School alter the composition (Weighting scheme (student-weighted vs. class-weighted)) of who is counted before Class size was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Class size after changing School can reflect a real outcome shift.\nAnswer if School changes who is counted via Weighting scheme (student-weighted vs. class-weighted): The aggregate Class size can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Class size may be moving because the denominator/population changed after School via composition variable Weighting scheme (student-weighted vs. class-weighted). Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-56",
        "bucket": "BucketLarge-J",
        "title": "The Average Income Misinterpretation",
        "scenario": "A city reports that average income increased, while surveys show most residents feel poorer. Officials dismiss public concern.\nIncome growth is driven by gains among a small number of very high earners.",
        "variables": [
            "X = Time period",
            "Y = Income",
            "Z = Mean vs median statistic"
        ],
        "annotations": {
            "Case ID": "J2-56",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Economics",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Means are sensitive to outliers.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Median income better reflects typical experience.\nConclusion: Prosperity claim is CONDITIONAL.",
        "Hidden Timestamp": "Did the intervention/change in Time period alter the composition (Mean vs median statistic) of who is counted before Income was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Income after changing Time period can reflect a real outcome shift.\nAnswer if Time period changes who is counted via Mean vs median statistic: The aggregate Income can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Income may be moving because the denominator/population changed after Time period via composition variable Mean vs median statistic. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-57",
        "bucket": "BucketLarge-J",
        "title": "Course Rating Inflation",
        "scenario": "A university finds that average course ratings increased, and concludes teaching quality improved.\nHowever, high-enrollment courses received lower ratings, while small seminars received higher ratings.",
        "variables": [
            "X = Course",
            "Y = Rating",
            "Z = Enrollment weighting"
        ],
        "annotations": {
            "Case ID": "J2-57",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Education",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Course-weighted averages differ from student-weighted experience.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Weighting determines interpretation.\nConclusion: Teaching improvement claim is CONDITIONAL.",
        "Hidden Timestamp": "Did the intervention/change in Course alter the composition (Enrollment weighting) of who is counted before Rating was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Rating after changing Course can reflect a real outcome shift.\nAnswer if Course changes who is counted via Enrollment weighting: The aggregate Rating can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Rating may be moving because the denominator/population changed after Course via composition variable Enrollment weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-58",
        "bucket": "BucketLarge-J",
        "title": "Hospital Wait Time Reporting",
        "scenario": "A hospital reports that average patient wait times decreased, while many patients report longer waits.\nShort visits dominate the average, masking longer waits for complex cases.",
        "variables": [
            "X = Visit type",
            "Y = Wait time",
            "Z = Visit weighting"
        ],
        "annotations": {
            "Case ID": "J2-58",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Healthcare Operations",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Aggregates hide heterogeneity.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Typical patient experience differs by visit type.\nConclusion: Efficiency claim is CONDITIONAL.",
        "Hidden Timestamp": "Did the intervention/change in Visit type alter the composition (Visit weighting) of who is counted before Wait time was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Wait time after changing Visit type can reflect a real outcome shift.\nAnswer if Visit type changes who is counted via Visit weighting: The aggregate Wait time can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Wait time may be moving because the denominator/population changed after Visit type via composition variable Visit weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-59",
        "bucket": "BucketLarge-J",
        "title": "Employee Satisfaction Scores",
        "scenario": "A firm reports rising average employee satisfaction, despite increased turnover.\nSatisfied long-tenured employees remain, while dissatisfied employees leave.",
        "variables": [
            "X = Employee tenure",
            "Y = Satisfaction score",
            "Z = Survivor weighting"
        ],
        "annotations": {
            "Case ID": "J2-59",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Organizational Behavior",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Averages reflect who remains.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Satisfaction increase reflects composition of respondents.\nConclusion: Workplace improvement claim is CONDITIONAL.",
        "Hidden Timestamp": "Did the intervention/change in Employee tenure alter the composition (Survivor weighting) of who is counted before Satisfaction score was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Satisfaction score after changing Employee tenure can reflect a real outcome shift.\nAnswer if Employee tenure changes who is counted via Survivor weighting: The aggregate Satisfaction score can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Satisfaction score may be moving because the denominator/population changed after Employee tenure via composition variable Survivor weighting. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-60",
        "bucket": "BucketLarge-J",
        "title": "Traffic Congestion Metrics",
        "scenario": "A city reports that average commute times decreased, while drivers complain of worse traffic.\nOff-peak trips increased, lowering the average.",
        "variables": [
            "X = Time of travel",
            "Y = Commute duration",
            "Z = Trip distribution"
        ],
        "annotations": {
            "Case ID": "J2-60",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Transportation",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Averages depend on trip timing.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Peak-hour experience worsened.\nConclusion: Traffic improvement claim is CONDITIONAL.",
        "Hidden Timestamp": "Did the intervention/change in Time of travel alter the composition (Trip distribution) of who is counted before Commute duration was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Commute duration after changing Time of travel can reflect a real outcome shift.\nAnswer if Time of travel changes who is counted via Trip distribution: The aggregate Commute duration can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Commute duration may be moving because the denominator/population changed after Time of travel via composition variable Trip distribution. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-61",
        "bucket": "BucketLarge-J",
        "title": "Carbon Emissions Reporting",
        "scenario": "A country reports lower per-capita emissions, claiming environmental progress.\nTotal emissions increased due to population growth.",
        "variables": [
            "X = Population size",
            "Y = Emissions metric",
            "Z = Per-capita vs total denominator"
        ],
        "annotations": {
            "Case ID": "J2-61",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Environmental Policy",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Different denominators answer different questions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Both statements can be true.\nConclusion: Progress claim is CONDITIONAL.",
        "Hidden Timestamp": "Did the intervention/change in Population size alter the composition (Per-capita vs total denominator) of who is counted before Emissions metric was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Emissions metric after changing Population size can reflect a real outcome shift.\nAnswer if Population size changes who is counted via Per-capita vs total denominator: The aggregate Emissions metric can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Emissions metric may be moving because the denominator/population changed after Population size via composition variable Per-capita vs total denominator. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-62",
        "bucket": "BucketLarge-J",
        "title": "Energy Efficiency Ratings",
        "scenario": "Appliance A is rated as more energy efficient per use, while households using it consume more total energy.\nAppliance A is used more frequently.",
        "variables": [
            "X = Appliance type",
            "Y = Energy use",
            "Z = Usage frequency"
        ],
        "annotations": {
            "Case ID": "J2-62",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Energy Policy",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Efficiency does not equal total consumption.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Usage drives totals.\nConclusion: Efficiency claim is CONDITIONAL.",
        "Hidden Timestamp": "Did the intervention/change in Appliance type alter the composition (Usage frequency) of who is counted before Energy use was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Energy use after changing Appliance type can reflect a real outcome shift.\nAnswer if Appliance type changes who is counted via Usage frequency: The aggregate Energy use can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Energy use may be moving because the denominator/population changed after Appliance type via composition variable Usage frequency. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-63",
        "bucket": "BucketLarge-J",
        "title": "Social Media Engagement Rates",
        "scenario": "A platform reports higher average engagement per post, while users report declining reach.\nHigh-engagement posts dominate the average, while most posts perform poorly.",
        "variables": [
            "X = Post type",
            "Y = Engagement metric",
            "Z = Distribution skew"
        ],
        "annotations": {
            "Case ID": "J2-63",
            "Pearl Level": "L2 (Intervention)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Selection Bias",
            "Trap Subtype": "Post-intervention Selection",
            "Difficulty": "Medium",
            "Subdomain": "Digital Media",
            "Causal Structure": "Intervention X shifts composition/weights Z; aggregate Y changes even if within-group outcomes stay similar",
            "Key Insight": "Skewed distributions distort averages.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Median engagement is more informative.\nConclusion: Platform success claim is CONDITIONAL.\nL3-A (Individual Counterfactuals) \u2014 9 cases",
        "Hidden Timestamp": "Did the intervention/change in Post type alter the composition (Distribution skew) of who is counted before Engagement metric was computed?",
        "Conditional Answers": "Answer if the population/denominator is stable: A change in Engagement metric after changing Post type can reflect a real outcome shift.\nAnswer if Post type changes who is counted via Distribution skew: The aggregate Engagement metric can move even with no within-person change; you need within-unit outcomes or fixed-denominator rates.\nAnswer if you track both composition and within-group outcomes: Separate 'composition shift' from 'within-group improvement' and report both components.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. The metric for Engagement metric may be moving because the denominator/population changed after Post type via composition variable Distribution skew. Please provide fixed-denominator rates or within-unit outcomes to separate composition from real improvement."
    },
    {
        "id": "T3-BucketJ-64",
        "bucket": "BucketLarge-J",
        "title": "The Missed Exam Retake",
        "scenario": "A student failed a qualifying exam and chose not to retake it, later leaving the program. The student claims that retaking the exam would not have changed the outcome.\nHowever, historical data shows that students with similar initial scores who retook the exam often passed.",
        "variables": [
            "X = Decision to retake the exam (yes / no)",
            "Y = Program continuation",
            "Z = Student preparation and ability"
        ],
        "annotations": {
            "Case ID": "J2-64",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Medium",
            "Subdomain": "Education Outcomes",
            "Causal Structure": "Only one action (retake or not) is observed for the student; the alternative outcome is unobserved.",
            "Key Insight": "Individual-level causal claims require counterfactual comparison.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can the student conclude that retaking the exam would not have mattered?\nWhy is this counterfactual inherently unobservable?",
        "expected_analysis": "The student\u2019s outcome under retaking (Y\u2081) is unobserved.\nSimilar students provide suggestive but imperfect evidence.\nConclusion: The student\u2019s claim is UNDETERMINED.\nWise refusal: Individual causal effects cannot be known without assumptions or experimental design.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Program continuation occurred\u2014and is selection related to Student preparation and ability or Program continuation?",
        "Conditional Answers": "Answer if Decision to retake the exam (yes / no) is randomly assigned: A difference in Program continuation across Decision to retake the exam (yes / no) groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Student preparation and ability): The Decision to retake the exam (yes / no) vs not-Decision to retake the exam (yes / no) difference in Program continuation is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Student preparation and ability) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Student preparation and ability); otherwise Decision to retake the exam (yes / no)\u2013Program continuation differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-65",
        "bucket": "BucketLarge-J",
        "title": "The Declined Job Offer",
        "scenario": "An individual declined a job offer and later experienced slower career progression. They claim that accepting the offer would not have improved their career.\nComparable candidates who accepted similar offers often advanced more quickly.",
        "variables": [
            "X = Job offer acceptance",
            "Y = Career progression",
            "Z = Skill level and career ambition"
        ],
        "annotations": {
            "Case ID": "J2-65",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Reverse Causation",
            "Trap Subtype": "Outcome-dependent Worlds",
            "Difficulty": "Hard",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Personal counterfactual outcomes are unobserved.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Only one trajectory is observed.\nPeer comparisons are imperfect substitutes.\nConclusion: Claim is UNDETERMINED.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Career progression occurred\u2014and is selection related to Skill level and career ambition or Career progression?",
        "Conditional Answers": "Answer if Job offer acceptance is randomly assigned: A difference in Career progression across Job offer acceptance groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Skill level and career ambition): The Job offer acceptance vs not-Job offer acceptance difference in Career progression is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Skill level and career ambition) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Skill level and career ambition); otherwise Job offer acceptance\u2013Career progression differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-66",
        "bucket": "BucketLarge-J",
        "title": "The Medical Treatment Refusal",
        "scenario": "A patient declined a recommended medical treatment and later recovered naturally. They conclude the treatment was unnecessary.\nClinical evidence suggests that many patients who refused treatment deteriorated.",
        "variables": [
            "X = Treatment acceptance",
            "Y = Health outcome",
            "Z = Disease severity"
        ],
        "annotations": {
            "Case ID": "J2-66",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Reverse Causation",
            "Trap Subtype": "Outcome-dependent Worlds",
            "Difficulty": "Hard",
            "Subdomain": "Healthcare",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Recovery does not reveal what would have happened under treatment.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Natural recovery does not imply treatment uselessness.\nConclusion: Claim is INVALID.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Health outcome occurred\u2014and is selection related to Disease severity or Health outcome?",
        "Conditional Answers": "Answer if Treatment acceptance is randomly assigned: A difference in Health outcome across Treatment acceptance groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Disease severity): The Treatment acceptance vs not-Treatment acceptance difference in Health outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Disease severity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Disease severity); otherwise Treatment acceptance\u2013Health outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-67",
        "bucket": "BucketLarge-J",
        "title": "The Parole Board Decision",
        "scenario": "A parole board denies parole to an inmate, who later reoffends after release. The board claims parole denial was justified.\nThe counterfactual\u2014what would have happened if parole had been granted earlier\u2014is unobserved.",
        "variables": [
            "X = Parole decision",
            "Y = Recidivism",
            "Z = Risk profile"
        ],
        "annotations": {
            "Case ID": "J2-67",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Reverse Causation",
            "Trap Subtype": "Outcome-dependent Worlds",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Observed outcomes do not validate the decision taken.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Reoffending does not prove denial was optimal.\nConclusion: Claim is INVALID.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Recidivism occurred\u2014and is selection related to Risk profile or Recidivism?",
        "Conditional Answers": "Answer if Parole decision is randomly assigned: A difference in Recidivism across Parole decision groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Risk profile): The Parole decision vs not-Parole decision difference in Recidivism is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Risk profile) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Risk profile); otherwise Parole decision\u2013Recidivism differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-68",
        "bucket": "BucketLarge-J",
        "title": "The Scholarship Cutoff",
        "scenario": "A student narrowly missed a scholarship cutoff and later struggled financially. Administrators argue the cutoff was fair.\nWhether the student would have succeeded with the scholarship is unobserved.",
        "variables": [
            "X = Scholarship receipt",
            "Y = Academic success",
            "Z = Financial stability"
        ],
        "annotations": {
            "Case ID": "J2-68",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Small differences around cutoffs hide large causal uncertainty.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Near-threshold comparisons are suggestive but uncertain.\nConclusion: Claim is UNDETERMINED.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Academic success occurred\u2014and is selection related to Financial stability or Academic success?",
        "Conditional Answers": "Answer if Scholarship receipt is randomly assigned: A difference in Academic success across Scholarship receipt groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Financial stability): The Scholarship receipt vs not-Scholarship receipt difference in Academic success is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Financial stability) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Financial stability); otherwise Scholarship receipt\u2013Academic success differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-69",
        "bucket": "BucketLarge-J",
        "title": "The Therapy Dropout",
        "scenario": "A patient drops out of therapy early and does not improve. They claim therapy was ineffective for them.\nThe outcome had they completed therapy is unknown.",
        "variables": [
            "X = Therapy completion",
            "Y = Mental health outcome",
            "Z = Adherence capacity"
        ],
        "annotations": {
            "Case ID": "J2-69",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Psychology",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Dropout obscures treatment counterfactual.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Non-completion masks potential benefit.\nConclusion: Claim is INVALID.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Mental health outcome occurred\u2014and is selection related to Adherence capacity or Mental health outcome?",
        "Conditional Answers": "Answer if Therapy completion is randomly assigned: A difference in Mental health outcome across Therapy completion groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Adherence capacity): The Therapy completion vs not-Therapy completion difference in Mental health outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Adherence capacity) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Adherence capacity); otherwise Therapy completion\u2013Mental health outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-70",
        "bucket": "BucketLarge-J",
        "title": "The Missed Investment Opportunity",
        "scenario": "An investor chose not to invest in a startup that later succeeded. They conclude investing would have yielded large returns.\nThe outcome had they invested\u2014including dilution, exit timing, or failure\u2014is unknowable.",
        "variables": [
            "X = Investment decision",
            "Y = Financial return",
            "Z = Market volatility"
        ],
        "annotations": {
            "Case ID": "J2-70",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Reverse Causation",
            "Trap Subtype": "Outcome-dependent Worlds",
            "Difficulty": "Hard",
            "Subdomain": "Finance",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Observing success does not reveal individual counterfactual payoff.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Success path is not deterministic.\nConclusion: Claim is UNDETERMINED.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Financial return occurred\u2014and is selection related to Market volatility or Financial return?",
        "Conditional Answers": "Answer if Investment decision is randomly assigned: A difference in Financial return across Investment decision groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Market volatility): The Investment decision vs not-Investment decision difference in Financial return is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Market volatility) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Market volatility); otherwise Investment decision\u2013Financial return differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-71",
        "bucket": "BucketLarge-J",
        "title": "The Alternative School Choice",
        "scenario": "A parent chose a private school for their child, who later excelled academically. The parent claims public school would have led to worse outcomes.\nThe child\u2019s public-school trajectory is unobserved.",
        "variables": [
            "X = School choice",
            "Y = Academic outcome",
            "Z = Family support"
        ],
        "annotations": {
            "Case ID": "J2-71",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Education",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Success does not validate the chosen path.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Only one schooling path observed.\nConclusion: Claim is UNDETERMINED.\nL3-B (Policy Counterfactuals) \u2014 6 cases",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Academic outcome occurred\u2014and is selection related to Family support or Academic outcome?",
        "Conditional Answers": "Answer if School choice is randomly assigned: A difference in Academic outcome across School choice groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Family support): The School choice vs not-School choice difference in Academic outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Family support) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Family support); otherwise School choice\u2013Academic outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-72",
        "bucket": "BucketLarge-J",
        "title": "The Minimum Wage Increase",
        "scenario": "A city raises the minimum wage, after which employment levels remain stable. Officials claim the policy had no negative employment effects.\nHowever, the counterfactual\u2014what employment would have been without the wage increase\u2014is unobserved. Economic conditions were improving during the same period.",
        "variables": [
            "X = Minimum wage policy",
            "Y = Employment level",
            "Z = Economic trend"
        ],
        "annotations": {
            "Case ID": "J2-72",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Labor Economics",
            "Causal Structure": "Observed employment reflects both policy and macroeconomic forces.",
            "Key Insight": "Stable outcomes do not imply zero policy effect.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Can we conclude the wage increase had no effect on employment?\nWhy is the counterfactual employment trajectory unobservable?",
        "expected_analysis": "Employment without the policy is unobserved.\nEconomic growth may mask negative effects.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires synthetic control or difference-in-differences.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Employment level occurred\u2014and is selection related to Economic trend or Employment level?",
        "Conditional Answers": "Answer if Minimum wage policy is randomly assigned: A difference in Employment level across Minimum wage policy groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Economic trend): The Minimum wage policy vs not-Minimum wage policy difference in Employment level is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Economic trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Economic trend); otherwise Minimum wage policy\u2013Employment level differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-73",
        "bucket": "BucketLarge-J",
        "title": "The Policing Strategy Shift",
        "scenario": "A city shifts to a community policing strategy and observes a decline in crime. Officials credit the strategy.\nNeighboring cities without the policy also experienced crime declines.",
        "variables": [
            "X = Policing strategy",
            "Y = Crime rate",
            "Z = Regional crime trend"
        ],
        "annotations": {
            "Case ID": "J2-73",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Criminal Justice",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Common trends obscure counterfactual outcomes.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Crime may have declined anyway.\nConclusion: Policy impact claim is UNDETERMINED.\nWise refusal: Requires comparison cities.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Crime rate occurred\u2014and is selection related to Regional crime trend or Crime rate?",
        "Conditional Answers": "Answer if Policing strategy is randomly assigned: A difference in Crime rate across Policing strategy groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Regional crime trend): The Policing strategy vs not-Policing strategy difference in Crime rate is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Regional crime trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Regional crime trend); otherwise Policing strategy\u2013Crime rate differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-74",
        "bucket": "BucketLarge-J",
        "title": "Education Funding Reform",
        "scenario": "A state increases education funding, and student outcomes improve. Legislators claim success.\nOther reforms were implemented simultaneously.",
        "variables": [
            "X = Funding reform",
            "Y = Student outcomes",
            "Z = Concurrent reforms"
        ],
        "annotations": {
            "Case ID": "J2-74",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Education Policy",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Isolating policy effects requires disentangling reforms.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Counterfactual funding-only effect is unknown.\nConclusion: Claim is UNDETERMINED.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Student outcomes occurred\u2014and is selection related to Concurrent reforms or Student outcomes?",
        "Conditional Answers": "Answer if Funding reform is randomly assigned: A difference in Student outcomes across Funding reform groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Concurrent reforms): The Funding reform vs not-Funding reform difference in Student outcomes is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Concurrent reforms) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Concurrent reforms); otherwise Funding reform\u2013Student outcomes differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-75",
        "bucket": "BucketLarge-J",
        "title": "Housing Construction Ban",
        "scenario": "A city imposes a housing construction ban, after which housing prices stabilize. Officials argue the ban prevented price increases.\nPrices might have stabilized regardless due to slowing demand.",
        "variables": [
            "X = Construction ban",
            "Y = Housing prices",
            "Z = Demand trend"
        ],
        "annotations": {
            "Case ID": "J2-75",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Housing Policy",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Observed stability does not reveal prevented change.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Counterfactual price trajectory unknown.\nConclusion: Claim is UNDETERMINED.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Housing prices occurred\u2014and is selection related to Demand trend or Housing prices?",
        "Conditional Answers": "Answer if Construction ban is randomly assigned: A difference in Housing prices across Construction ban groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Demand trend): The Construction ban vs not-Construction ban difference in Housing prices is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Demand trend) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Demand trend); otherwise Construction ban\u2013Housing prices differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-76",
        "bucket": "BucketLarge-J",
        "title": "Environmental Regulation Rollout",
        "scenario": "A country introduces emissions regulations, and emissions fall. Leaders credit regulation.\nGlobal energy prices also rose sharply.",
        "variables": [
            "X = Regulation",
            "Y = Emissions",
            "Z = Energy price shocks"
        ],
        "annotations": {
            "Case ID": "J2-76",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Environmental Policy",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "External forces affect counterfactual emissions.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Cannot isolate regulation effect.\nConclusion: Claim is UNDETERMINED.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Emissions occurred\u2014and is selection related to Energy price shocks or Emissions?",
        "Conditional Answers": "Answer if Regulation is randomly assigned: A difference in Emissions across Regulation groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Energy price shocks): The Regulation vs not-Regulation difference in Emissions is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Energy price shocks) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Energy price shocks); otherwise Regulation\u2013Emissions differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-77",
        "bucket": "BucketLarge-J",
        "title": "Public Transit Expansion",
        "scenario": "A city expands public transit, after which traffic congestion eases. Officials credit the expansion.\nRemote work adoption increased during the same period.",
        "variables": [
            "X = Transit expansion",
            "Y = Congestion",
            "Z = Remote work prevalence"
        ],
        "annotations": {
            "Case ID": "J2-77",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Transportation Policy",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Multiple causes obscure the counterfactual.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Congestion might have declined anyway.\nConclusion: Claim is UNDETERMINED.\nL3-C (Fairness & Causal Attribution) \u2014 2 cases",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Congestion occurred\u2014and is selection related to Remote work prevalence or Congestion?",
        "Conditional Answers": "Answer if Transit expansion is randomly assigned: A difference in Congestion across Transit expansion groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Remote work prevalence): The Transit expansion vs not-Transit expansion difference in Congestion is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Remote work prevalence) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Remote work prevalence); otherwise Transit expansion\u2013Congestion differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-78",
        "bucket": "BucketLarge-J",
        "title": "Algorithmic Hiring Fairness",
        "scenario": "A hiring algorithm results in fewer hires from Group A. The company claims the algorithm is discriminatory.\nHowever, the counterfactual\u2014how many from Group A would have been hired without the algorithm\u2014is unobserved.",
        "variables": [
            "X = Algorithmic hiring",
            "Y = Hiring outcome",
            "Z = Applicant qualification distribution"
        ],
        "annotations": {
            "Case ID": "J2-78",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "N/A",
            "Causal Structure": "Disparity does not imply discrimination without a counterfactual baseline.",
            "Key Insight": "Fairness claims require causal comparisons.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "Does the hiring disparity prove discrimination?\nWhat counterfactual is missing?",
        "expected_analysis": "Need hiring outcomes under alternative process.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires causal fairness evaluation.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Hiring outcome occurred\u2014and is selection related to Applicant qualification distribution or Hiring outcome?",
        "Conditional Answers": "Answer if Algorithmic hiring is randomly assigned: A difference in Hiring outcome across Algorithmic hiring groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Applicant qualification distribution): The Algorithmic hiring vs not-Algorithmic hiring difference in Hiring outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Applicant qualification distribution) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Applicant qualification distribution); otherwise Algorithmic hiring\u2013Hiring outcome differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-79",
        "bucket": "BucketLarge-J",
        "title": "Discrimination Attribution Fraction",
        "scenario": "A wage gap is observed between two demographic groups. Analysts claim a specific percentage is due to discrimination.\nThe fraction attributable to discrimination depends on the counterfactual wage distribution absent discrimination.",
        "variables": [
            "X = Group membership",
            "Y = Wages",
            "Z = Job characteristics"
        ],
        "annotations": {
            "Case ID": "J2-79",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Sociology & Law",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Attribution requires assumptions about counterfactual worlds.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Multiple valid counterfactuals exist.\nConclusion: Attribution percentage is UNDETERMINED.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Wages occurred\u2014and is selection related to Job characteristics or Wages?",
        "Conditional Answers": "Answer if Group membership is randomly assigned: A difference in Wages across Group membership groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Job characteristics): The Group membership vs not-Group membership difference in Wages is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Job characteristics) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Job characteristics); otherwise Group membership\u2013Wages differences may reflect selection rather than effect."
    },
    {
        "id": "T3-BucketJ-80",
        "bucket": "BucketLarge-J",
        "title": "Election Outcome Counterfactual",
        "scenario": "An election was decided by a narrow margin. Commentators claim a specific policy decision \u201ccaused\u201d the loss.\nThe election outcome absent that policy is unknowable.",
        "variables": [
            "X = Policy decision",
            "Y = Election outcome",
            "Z = Voter preferences"
        ],
        "annotations": {
            "Case ID": "J2-80",
            "Pearl Level": "L3 (Counterfactual)",
            "Domain": "D10 (Social Science)",
            "Trap Type": "Confounding",
            "Trap Subtype": "Cross-world Confounder",
            "Difficulty": "Hard",
            "Subdomain": "Political Science",
            "Causal Structure": "Z -> X and Z -> Y (selection/confounding); observed difference mixes baseline risk with treatment effect",
            "Key Insight": "Singular historical events lack observable counterfactuals.",
            "author": "Sreya Vangara",
            "num_annotators": 2
        },
        "questions": "",
        "expected_analysis": "Many plausible counterfactual worlds.\nConclusion: Claim is UNDETERMINED.\nWise refusal: Requires formal causal modeling assumptions.",
        "Hidden Timestamp": "At what point were units selected into the observed sample\u2014before or after Election outcome occurred\u2014and is selection related to Voter preferences or Election outcome?",
        "Conditional Answers": "Answer if Policy decision is randomly assigned: A difference in Election outcome across Policy decision groups can be interpreted causally.\nAnswer if participation/exposure is voluntary or selected (e.g., Voter preferences): The Policy decision vs not-Policy decision difference in Election outcome is biased by who ends up observed/treated.\nAnswer if you can measure the selection drivers (e.g., Voter preferences) and adjust (matching/weighting/IV): The conclusion becomes CONDITIONAL on those assumptions and model quality.",
        "Wise Refusal": "I don\u2019t have enough information to make a definitive causal claim from the summary statistics alone. We need to know how units enter the treated/observed group and what pre-existing differences drive selection (especially Voter preferences); otherwise Policy decision\u2013Election outcome differences may reflect selection rather than effect."
    }
]